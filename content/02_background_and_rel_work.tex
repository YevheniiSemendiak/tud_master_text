\chapter{Background and Related Work Analysis}\label{bg}
In this chapter we provide reader with the base knowledge in field of Optimization Problems and the process of their solving.
The reader who is an expert in field of Optimization and Search Problems could find this chapter as an obvious discussion of well-known facts. If the notions of \textit{Parameter Tuning} and \textit{Parameter Control} seems like two different names for one thing, we encourage you to read this chapter carefully.
We highly recommend for everyone to refresh the knowledge of sections topics and examine the examples of Hyper-Heuristics in \ref{bg: hh examples} and Systems for parameter tuning in \ref{bg: parameter tuning expamples} since we use them later in concept implementation.

\paragraph{In this chapter we...} 


\section{Optimization, Search Problems and their Solvers}\label{bg:opt problems and solvers}


\subsection{Definition of the Optimization Problem}
%https://www.solver.com/problem-types
% need to add some kind of catchy intro, here or in previous parts of this section.
While the Search Problem (SP) defines the process of finding a possible Solution for the Computation Problem, an Optimization Problem (OP) is the special case of the SP, focused on the process of finding the "best possible" Solution for Computation Problem~\cite{goldreich2010p}. 


In this thesis we focus on the Optimization Problems -- a special case of the Search Problems.


A lot of conducted studies in this field have tried to formalize the concept of OP, but the underlying notion such a vast that it is almost impossible to exclude the application domain from the definition. However, in \cite{biegler2004retrospective,figueira2014hybrid,amaran2016simulation} authors distinguished OP characteristics that overlap through each of these works and we would like to mention them here.


First of all, let us define the subject of the Optimization. In general, it could be imagined as the Target System (TS) displayed on picture \ref{bg:pic:Target System}. Analytically it could be represented as the function $Y = f(X)$. Informally it accepts the information with it's \textit{inputs} \textbf{X} sometimes also called variables or parameters, performs a \textit{Task} and produces the result on it's \textit{outputs} \textbf{Y}.

\svgpath{{graphics/Background/}}
\begin{figure}
	\centering
	\includesvg[width=0.5\textwidth]{TargetSystem}
	\caption{Target System}
	\label{bg:pic:Target System}
\end{figure}

Pair of $X$ and respective $Y$ form a Solution for Computation Problem. 
The Solution could also be characterized by the \textit{objective} value(s) - a quantitative measure of TS performance that we minimize or maximize. 
We could obtain those value(s) directly by reading the $Y$, or indirectly for instance, noting the time TS took to produce the output $Y$ for given $X$. 
The Solution objective value(s) form object(s) of Optimization. 
For the sake of simplicity we here use $Y$, \textit{outputs}, \textit{objectives} and $X$, variables, \textbf{parameters(?)}% will decide later
 interchangeably.

\paragraph{}
Next, let us highlight the Target System characteristics. 
Among mentioned in \cite{biegler2004retrospective,figueira2014hybrid,amaran2016simulation} we found those the most important:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Input data types} of $X$ is a crucial characteristic. The variables could be either \textit{discrete} where representatives are binary strings, integer-ordered or categorical data, % One could apply mixed integer linear (nonlinear) programming here (MILP, MINLP) \cite{biegler2004retrospective}.
	\textit{continuous} where variables are usually a range of real numbers, or \textit{mixed} as the mixture of previous two cases.

	\item \textbf{Constrains} are functional dependencies that describe the relationships among inputs and defile the allowable values for them.

	\item \textbf{Amount of knowledge} TS exposes about the dependencies between $X \rightarrow Y$ or objective values. With respect to this knowledge, the Optimization could be \textit{White Box} - the TS exposes it internals fully, so it is even possible to derive the algebraic model of TS.
	%\textit{Gray Box} - the amount of exposed knowledge is significant, but not enough to build the algebraic model.
	\textit{Black Box} - the exposed knowledge is mostly negligible. As an example, imagine the 
	%In this case the Derivative Free Optimization approaches (such as Surrogate Optimization, different Meta-|Hybrid-|Hyper-Heuristics)  are applicable.

	%\paragraph{Dependency types} could be  The inputs to outputs dependencies the of Target System could also be distinguished form perspective of linearity \cite{biegler2004retrospective,figueira2014hybrid}.
	%\textit{Linear dependencies} reveal the Linear Programming Optimization approaches, while with \textbf{Nonlinear dependencies} one should consider Nonlinear Programming.

	\item \textbf{Dependencies randomness} One of possible challenges, while obtaining the knowledge about TS is uncertainty of output. Ideal case is the \textit{deterministic} dependency between $X$ and $Y$, however in most of real-world challenges engineers tackle with the \textit{stochastic} systems whose output is affected by random processes. 

	\item \textbf{Cost of evaluation} is the amount of resources (computational, time, money, etc.) TS will spend to obtain the result for particular input. It vary from very cheap if the TS is a simple algebraic formula and Task is to evaluate it, to very expensive if the TS is a complex Neuron Network and the Task is to train it on data.

	\item \textbf{Number of objectives} could be either \textit{Single}, or \textit{Multiple}. According to the number of objectives, the result of optimization will be either single Solution, or set of non-dominated (Pareto-optimal) Solutions \cite{deb2014multi}.

\end{itemize}


\subsection{Classes of Solvers}
Some (but not only) literature: \cite{bergstra2011algorithms}

The characteristics of Target System could restrict and sometimes strictly define the possible approach to perform an Optimization. 
As instance, for Black Box TS one will not be able to apply any Mixed Integer Programming Optimization approaches, however Heuristics still will work.
In opposite case, if the TS exposes algebraic dependencies between input variables and output objective, the MIP optimization, in particular MILP or MINLP approaches could be used to derive mathematically proved optimal Solution \cite{biegler2004retrospective}. 


\subsubsection{Exact Solvers}
\subsubsection{Approximate Solvers}
\subsubsection{Motivation of Approximate Solvers}
Pros and cons of both \cite{hromkovivc2013algorithmics}


\section{Approximate Solvers for Optimization Problems}
TSP as the running example. I guess, I will introduce it as an example of perturbation problems in previous section.\ref{sec:opt problms, solvrs}

\subsection{Heuristics}
\subsubsection{Definition}
\subsubsection{Examples}

\subsection{Meta-Heuristics}
\subsubsection{Definition}
\subsubsection{Classification}
\subsubsection{Examples}
We distinguish following examples among all existing meta-heuristics, since later we use them as the LLH in developed hyper-heuristic.
\paragraph{GA}
\paragraph{SA}
\paragraph{ES}

\subsection{Hybrid-Heuristics}
\subsubsection{Definition}
\subsubsection{Examples}
\paragraph{Guided Loca Search (GLS) + Fast Local Search} \cite{tsang1997fast}
\paragraph{Direct Global + Local search} \cite{syrjakow1999efficient}
\paragraph{Simulated Annealing + Local Search} \cite{martin1996combining}

\subsubsection{No-Free-Lunch Theorem}
NFL is the problem of heuristics\cite{wolpert1997no}
\subsubsection{Exploration-Exploitation Balance}
\subsubsection{Conclusion} 
Proper assignment of hyper-parameters has great impact on exploration-exploitation balance and those on (meta)~-heuristic performance. 

\subsection{Hyper-Heuristics}
\subsubsection{Definition}
\subsubsection{Classification}
\paragraph{Search Space:} heuristic selection, heuristic generation
\paragraph{Learning time:} on-line learning hyper-heuristics, off-line learning hyper-heuristics, no-learning hyper-heuristics
\paragraph{Other classification characteristics} from \cite{surv:kerschke2019automated}, \cite{burke2019classification}, mb smth else. For instance, hyperparameter tuning
\subsubsection{Examples}\label{bg: hh examples}% should I present it in following sections?
\cite{surv:drake2019recent} (Online algorithm selection at page 27); \cite{surv:kerschke2019automated}

\subsection{Conclusion on Approximate Solvers}
\paragraph{Pros and cons of heuristics} - Heuristics are strictly problem dependent and each time require adaptations.
\paragraph{Pros and cons of meta-heuristics} - no LLH selection, strict to one problem
\paragraph{Pros and cons of hybrid-heuristics} - no LLH selection, strict to one problem ? 
\paragraph{Pros and cons of hyper-heuristics} - no parameter control?


\section{Parameter Tuning as a Search Problem}\label{bg: parameter tuning}
The goal of section: analysis of existing systems for hyper-parameter optimization (tuning), weaknesses and strength of each of the system

\subsection{Parameter Tuning Problem Definition}
\subsection{Approaches for Parameter Tuning}
\paragraph{Grid Search}
\paragraph{Random Search}
\paragraph{Model Based Search}

\subsection{Systems for Model Based Parameter Tuning}\label{bg: parameter tuning expamples}

\subsubsection{IRACE}
\paragraph{approach} \cite{irace:lopez2016irace}
\paragraph{pros and cons}

\subsubsection{SMAC}
\paragraph{approach description}

\subsubsection{BOHB}
\paragraph{approach description}

\subsubsection{AUTO-SKLEARN}
\paragraph{CASH (Combined Algorithm Selection and Hyperparameter optimization) problem}
\paragraph{pros and cons (on-line or off-line, problems to solve, extensibility)}\cite{autosklearn:feurer2015efficient}

\subsubsection{BRISEv2}
\paragraph{approach description}
\todoy{Other systems?}


\section{Parameter control as an Optimization Problem}\label{bg: parameter control}
\subsection{Parameter Control Definition}
\subsection{Examples and Reported Impact}
impact of parameter control based on other's evaluation


\section{Conclusion}

The meta-heuristic systems designers reported positive impact of parameter control embedding. 
However, as the outcome of the no-free-lunch theorem, those systems can not tolerate broad range of problems, for instance, problem classes.
In other hand, hyper-heuristics are designed with an aim to select the low level heuristics and those propose a possible solution of problem, stated in no-free-lunch theorem, but the lack of parameter control could dramatically decrease the performance of LLH (probably, I need to find a prove of this, or rephrase).

\paragraph{Scope of thesis defined.} In this thesis we try to achieve the best of both worlds applying the best fitting LLH and tuning it's parameters while solving the problem on-line.
