\chapter{Background and Related Work Analysis}\label{bg}
In this chapter we provide reader with the base knowledge in field of Optimization Problems and the process of their solving.
The reader who is an expert in field of Optimization and Search Problems could find this chapter as an obvious discussion of well-known facts. If the notions of \textit{Parameter Tuning} and \textit{Parameter Control} seems like two different names for one thing, we encourage you to read this chapter carefully.
We highly recommend for everyone to refresh the knowledge of sections topics and examine the examples of Hyper-Heuristics in \ref{bg: hh examples} and Systems for parameter tuning in \ref{bg: parameter tuning expamples} since we use them later in concept implementation.

\paragraph{In this chapter we...} 


\section{Optimization, Search Problems and their Solvers}\label{bg:opt problems and solvers}


\subsection{Definition of the Optimization Problem}
%https://www.solver.com/problem-types
% need to add some kind of catchy intro, here or in previous parts of this section.
While the Search Problem (SP) defines the process of finding a possible Solution for the Computation Problem, an Optimization Problem (OP) is the special case of the SP, focused on the process of finding the "best possible" Solution for Computation Problem~\cite{goldreich2010p}. 


In this thesis we focus on the Optimization Problems -- a special case of the Search Problems.


A lot of conducted studies in this field have tried to formalize the concept of OP, but the underlying notion such a vast that it is almost impossible to exclude the application domain from the definition. The description of every possible Optimization Problem and all approaches for solving it are out of the scope of this thesis. However, a birds-eye view should be presented in order to make sure that reader is familiar with all notions used through this thesis. 


In \cite{biegler2004retrospective,figueira2014hybrid,amaran2016simulation} authors distinguished OP characteristics that overlap through each of these works and those we would like to start from them.


First of all, let us define the subject of the Optimization. In general, it could be imagined as the Target System (TS) displayed on picture \ref{bg:pic:Target System}. Analytically it could be represented as the function $Y = f(X)$. Informally it accepts the information with it's \textit{inputs} \textbf{X} sometimes also called variables or parameters, performs a \textit{Task} and produces the result on it's \textit{outputs} \textbf{Y}.

\svgpath{{graphics/Background/}}
\begin{figure}
	\centering
	\includesvg[width=0.5\textwidth]{TargetSystem}
	\caption{Target System}
	\label{bg:pic:Target System}
\end{figure}

Pair of $X$ and respective $Y$ form a Solution for Computation Problem. 
The Solution could also be characterized by the \textit{objective} value(s) - a quantitative measure of TS performance that we minimize or maximize. 
We could obtain those value(s) directly by reading the $Y$, or indirectly for instance, noting the time TS took to produce the output $Y$ for given $X$. 
The Solution objective value(s) form object(s) of Optimization. 
For the sake of simplicity we here use $Y$, \textit{outputs}, \textit{objectives} and $X$, variables, \textbf{parameters(?)}% will decide later
 interchangeably.

\paragraph{}
Next, let us highlight the Target System characteristics.
Among mentioned in \cite{biegler2004retrospective,figueira2014hybrid,amaran2016simulation} we found those the most important:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Input data types} of $X$ is a crucial characteristic. The variables could be either \textit{discrete} where representatives are binary strings, integer-ordered or categorical data, % One could apply mixed integer linear (nonlinear) programming here (MILP, MINLP) \cite{biegler2004retrospective}.
	\textit{continuous} where variables are usually a range of real numbers, or \textit{mixed} as the mixture of previous two cases.

	\item \textbf{Constrains} are functional dependencies that describe the relationships among inputs and defile the allowable values for them.

	\item \textbf{Amount of knowledge} TS exposes about the dependencies between $X \rightarrow Y$ or objective values. With respect to this knowledge, the Optimization could be \textit{White Box} -- the TS exposes it internals fully, so it is even possible to derive the algebraic model of TS.
	%\textit{Gray Box} - the amount of exposed knowledge is significant, but not enough to build the algebraic model.
	\textit{Black Box} -- the exposed knowledge is mostly negligible.
	%In this case the Derivative Free Optimization approaches (such as Surrogate Optimization, different Meta-|Hybrid-|Hyper-Heuristics)  are applicable.

	%\paragraph{Dependency types} could be  The inputs to outputs dependencies the of Target System could also be distinguished form perspective of linearity \cite{biegler2004retrospective,figueira2014hybrid}.
	%\textit{Linear dependencies} reveal the Linear Programming Optimization approaches, while with \textbf{Nonlinear dependencies} one should consider Nonlinear Programming.

	\item \textbf{Dependencies randomness} One of possible challenges, while obtaining the knowledge about TS is uncertainty of output. Ideal case is the \textit{deterministic} dependency between $X$ and $Y$, however in most of real-world challenges engineers tackle with the \textit{stochastic} systems whose output is affected by random processes. 

	\item \textbf{Cost of evaluation} is the amount of resources (computational, time, money, etc.) TS will spend to obtain the result for particular input. It vary from very cheap if the TS is a simple algebraic formula and Task is to evaluate it, to very expensive if the TS is a complex Neuron Network and the Task is to train it on data.

	\item \textbf{Number of objectives} could be either \textit{Single}, or \textit{Multiple}. According to the number of objectives, the result of optimization will be either single Solution, or set of non-dominated (Pareto-optimal) Solutions \cite{deb2014multi}.

\end{itemize}


Combining different characteristic, one could obtain broad range of Optimization Problem types.
As an example, let's grasp these characteristics for Traveling Salesman Problem \cite{applegate2006traveling} - one of the most studied combinatorial OP, yet still remaining one of the most challenging (here we consider deterministic, symmetric TSP).
The informal definition of TSP is as follows: "Given a set of cities and the distances between each of them, what is the shortest path to visit each city once and return to the origin city?"
The input data (path) is a vector of city indexes, and those the type is a non-negative integers \textit{0, 1, 2...}.
There are two constrains on path: it should contain only unique indexes (those, each city will be visited only once) and it should start and end from the same city. 
The TSP distance (or cost) matrix here plays role of Target System, clearly that this TS exposes all internal knowledge and those it is the white box.
Since the cost matrix is fixed and not changing, the TS is considered to be deterministic, cost for two identical paths are always the same (although there exist Dynamic TSP where the cost matrix changes while computing the path cost to reflect a real-time traffic information updates while traveling \cite{cheong2011dynamic}).
It is extremely cheap to compute a cost for given path using cost matrix, those overall Solution evaluation in this TS is cheap.
Since we are optimizing only the route distance, it is a Single objective OP.


\subsection{Classes of Solvers}
The provided by TS characteristics of Optimization Problem could restrict and sometimes strictly define the possible approach to solve it.
For instance, imagine you have white box deterministic Target System with discrete constrained input data and cheap evaluation. The Optimization Problem of this TS could be described using Integer Linear Programming \todoy{ref} approaches (but for scalability reasons not all problems could be tackled with ILP), or heuristics \todoy{ref}. If this TS turned to be a black box, the ILP approaches are not applicable and one should consider heuristics usage \cite{biegler2004retrospective}.


As the point of interest, we distinguish 2 broad families of Solvers for Optimization Problems:
\begin{itemize}
	\item \textbf{Exact Solvers} that proved to solve the OP optimally. In most cases, these Solvers are not able to produce any intermediate Solution. 
	\item \textbf{Approximate Solvers} that often start from some random Solution, but improve it quality while solving the OP.
\end{itemize}

Indeed, in real life use-cases sometimes it worse to sacrifice a bit the Solution quality in order to obtain it much faster. 
\todoy{to be continued...}
Some (but not only) literature: \cite{bergstra2011algorithms}

\subsubsection{Exact Solvers}
\subsubsection{Approximate Solvers}
\subsubsection{Motivation of Approximate Solvers}
Pros and cons of both \cite{hromkovivc2013algorithmics}


\section{Approximate Solvers for Optimization Problems}
TSP as the running example. I guess, I will introduce it as an example of perturbation problems in previous section.\ref{sec:opt problms, solvrs}

\subsection{Heuristics}
\subsubsection{Definition}
\subsubsection{Examples}

\subsection{Meta-Heuristics}
\subsubsection{Definition}
\subsubsection{Classification}
\subsubsection{Examples}
We distinguish following examples among all existing meta-heuristics, since later we use them as the LLH in developed hyper-heuristic.
\paragraph{GA}
\paragraph{SA}
\paragraph{ES}

\subsection{Hybrid-Heuristics}
\subsubsection{Definition}
\subsubsection{Examples}
\paragraph{Guided Loca Search (GLS) + Fast Local Search} \cite{tsang1997fast}
\paragraph{Direct Global + Local search} \cite{syrjakow1999efficient}
\paragraph{Simulated Annealing + Local Search} \cite{martin1996combining}

\subsubsection{No-Free-Lunch Theorem}
NFL is the problem of heuristics\cite{wolpert1997no}
\subsubsection{Exploration-Exploitation Balance}
\subsubsection{Conclusion} 
Proper assignment of hyper-parameters has great impact on exploration-exploitation balance and those on (meta)~-heuristic performance. 

\subsection{Hyper-Heuristics}
\subsubsection{Definition}
\subsubsection{Classification}
\paragraph{Search Space:} heuristic selection, heuristic generation
\paragraph{Learning time:} on-line learning hyper-heuristics, off-line learning hyper-heuristics, no-learning hyper-heuristics
\paragraph{Other classification characteristics} from \cite{surv:kerschke2019automated}, \cite{burke2019classification}, mb smth else. For instance, hyperparameter tuning
\subsubsection{Examples}\label{bg: hh examples}% should I present it in following sections?
\cite{surv:drake2019recent} (Online algorithm selection at page 27); \cite{surv:kerschke2019automated}

\subsection{Conclusion on Approximate Solvers}
\paragraph{Pros and cons of heuristics} - Heuristics are strictly problem dependent and each time require adaptations.
\paragraph{Pros and cons of meta-heuristics} - no LLH selection, strict to one problem
\paragraph{Pros and cons of hybrid-heuristics} - no LLH selection, strict to one problem ? 
\paragraph{Pros and cons of hyper-heuristics} - no parameter control?


\section{Parameter Tuning as a Search Problem}\label{bg: parameter tuning}
The goal of section: analysis of existing systems for hyper-parameter optimization (tuning), weaknesses and strength of each of the system

\subsection{Parameter Tuning Problem Definition}
\subsection{Approaches for Parameter Tuning}
\paragraph{Grid Search}
\paragraph{Random Search}
\paragraph{Model Based Search}

\subsection{Systems for Model Based Parameter Tuning}\label{bg: parameter tuning expamples}

\subsubsection{IRACE}
\paragraph{approach} \cite{irace:lopez2016irace}
\paragraph{pros and cons}

\subsubsection{SMAC}
\paragraph{approach description}

\subsubsection{BOHB}
\paragraph{approach description}

\subsubsection{AUTO-SKLEARN}
\paragraph{CASH (Combined Algorithm Selection and Hyperparameter optimization) problem}
\paragraph{pros and cons (on-line or off-line, problems to solve, extensibility)}\cite{autosklearn:feurer2015efficient}

\subsubsection{BRISEv2}
\paragraph{approach description}
\todoy{Other systems?}


\section{Parameter control as an Optimization Problem}\label{bg: parameter control}
\subsection{Parameter Control Definition}
\subsection{Examples and Reported Impact}
impact of parameter control based on other's evaluation


\section{Conclusion}

The meta-heuristic systems designers reported positive impact of parameter control embedding. 
However, as the outcome of the no-free-lunch theorem, those systems can not tolerate broad range of problems, for instance, problem classes.
In other hand, hyper-heuristics are designed with an aim to select the low level heuristics and those propose a possible solution of problem, stated in no-free-lunch theorem, but the lack of parameter control could dramatically decrease the performance of LLH (probably, I need to find a prove of this, or rephrase).

\paragraph{Scope of thesis defined.} In this thesis we try to achieve the best of both worlds applying the best fitting LLH and tuning it's parameters while solving the problem on-line.
