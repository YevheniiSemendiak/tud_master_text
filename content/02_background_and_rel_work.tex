\chapter{Background and Related Work Analysis}\label{bg}
In this chapter we provide reader with a review of the base knowledge in fields of Optimization Problems and approaches for solving them.
An experienced reader in field of Optimization and Search Problems could find this chapter as an obvious discussion of well-known facts. 
If the notions of \textit{Parameter Tuning} and \textit{Parameter Control} are not familiar to you or seems like two different names for one thing, we highly encourage you to spend some time with reading this chapter carefully.
In any case, it is worth for everyone to refresh the knowledge with coarse-grained description of sections topics and examine the examples of Hyper-Heuristics in \ref{bg: hh examples} and Systems for parameter tuning in \ref{bg: parameter tuning expamples} since we use them later in concept implementation.

% TODO: finalize intro to chapter
% add brief outline of lieterature review

\section{Optimization Problems and their Solvers}\label{bg:opt problems and solvers}
Our life is full of different hard and sometimes contradicting choices. 

But what is an Optimization? It is an art of making good decisions.

A decision in working hard or going home earlier, to buy cheaper goods or follow brands, to isolate ourselves or visit friends during the quarantine, to spend more time for planning trip or start it instantly. Each made decisions have it consequences.

Picture~\ref{bg:pic:Optimization tradeoff} outlines the trade-off between decision quality and amount of effort spent. Concretely, here you could spend a couple of days searching the interned, comparing routes to find the best suited one, or you could just go. While in former case you seem to waste a lot of time, in later, however, you could lose it in on the way.

\svgpath{{graphics/Background/}}
\begin{figure}
	\centering
	\includesvg[width=0.5\textwidth]{optimization_concept_tsp}
	\caption{Optimization trade-off.}
	\label{bg:pic:Optimization tradeoff}
\end{figure}
\todoy{refactor the image}

\subsection{Optimization Problems}\label{BG: subsection OPs}
While the Search Problem (SP) defines the process of finding a possible Solution for the Computation Problem, an Optimization Problem (OP) is the special case of the SP, focused on the process of finding the \emph{best possible} Solution for Computation Problem~\cite{goldreich2010p}.

In this thesis we focus ourselves on the Optimization Problems.

A lot of performed studies in this field have tried to formalize the concept of OP, but the underlying notion such a vast that it is hard to exclude the application domain from definition. The description of every possible Optimization Problem and all approaches for solving it are out of this thesis scope, but a coarse-grain review should be presented in order to make sure that everyone reading this thesis will be familiar with all used here terms and notions. 

In works~\cite{biegler2004retrospective,figueira2014hybrid,amaran2016simulation} dedicated to solving OPs, authors distinguished OP characteristics that overlap through each of these works and those we would like to start from them.


First, let us define the Optimization \textit{subject}. Analytically it could be represented as the function $Y = f(X)$ that accepts some input $X$ and reacts on in providing an output $Y$. Informally it could be imagined as the Target System $f$ (TS) shown on picture~\ref{bg:pic:Target System}. It accepts the input information with its \textit{inputs} $X_n$ sometimes also called variables or parameters, processes them performing some \textit{Task} and produces the result on its \textit{outputs} $Y_m$.

\begin{figure}
	\centering
	\includesvg[width=0.5\textwidth]{TargetSystem}
	\caption{Optimization Target System.}
	\label{bg:pic:Target System}
\end{figure}

Each (unique) pair of sets $X_n^i$ and respective $Y_m^i$ form $Solution^i$ for Computational Problem.
All possible inputs $X^i$, where $i=1...N$ form a \textit{Search Space} of size $N$, while all outcomes $Y^i$, where $i=1...M$ form an \textit{Objective Space} of size $M$. Note, that both $N$ and $M$ could be equal to $\infty$.

The Solution characterized by the \textit{objective} value(s) — a quantitative measure of TS performance that we want to minimize or maximize in Optimization Problems. 
We could obtain those value(s) directly by reading $Y_m$, or indirectly for instance, noting the wall clock time TS took to produce the output $Y^i$ for given $X^i$. 
The Solution objective value(s) form \textit{object} of Optimization. 
For the sake of simplicity we here use $Y_m$, \textit{outputs} or \textit{objectives} interchangeably as well as $X_n$, \textit{variables} or \textbf{parameters}.


Next, let us highlight the Target System characteristics.
Among all mentioned in \cite{biegler2004retrospective,figueira2014hybrid,deb2014multi,amaran2016simulation} we found those properties the most important:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Input data types} of $X_m$ is a crucial characteristic. The variables could be either \textit{discrete} where representatives are binary strings, integer-ordered or categorical data, \textit{continuous} where variables are usually a range of real numbers, or \textit{mixed} as the mixture of previous two cases.

	\item \textbf{Constrains} are functional dependencies that describe the relationships among inputs and defile the allowable values for them. As an example, imagine that having $X_n$ equal to $value$ implies that $X_{n + k}$ should not appear at all, or could take only some subset of all possible values.

	\item \textbf{Amount of knowledge} that TS exposes about its internals. In other words, exposed knowledge between the dependencies $X \rightarrow Y$. With respect to this, an Optimization could be: \textit{White Box} — TS exposes its internals fully, it is even possible to derive the algebraic model of TS.
	\textit{Gray Box} — the amount of exposed knowledge is significant, but not enough to build the algebraic model.
	\textit{Black Box} — the exposed knowledge is mostly negligible.
	%In this case the Derivative Free Optimization approaches (such as Surrogate Optimization, different Meta-|Hybrid-|Hyper-Heuristics)  are applicable.

	%\paragraph{Dependency types} could be  The inputs to outputs dependencies the of Target System could also be distinguished form perspective of linearity \cite{biegler2004retrospective,figueira2014hybrid}.
	%\textit{Linear dependencies} reveal the Linear Programming Optimization approaches, while with \textbf{Nonlinear dependencies} one should consider Nonlinear Programming.

	\item \textbf{Randomness} of TS is One of possible challenges, when the output is uncertain. Ideally TS is \textit{deterministic}, when for $X^i$ TS provides the same $Y^i$ all the time. However, in most real-life challenges engineers tackle \textit{stochastic} systems whose output is affected by random processes happened inside TS. 

	\item \textbf{Cost of evaluation} is the amount of resources (energy, time, money, etc.) TS should spend to produce the output for particular input. It varies from \textit{cheap} when TS could be an algebraic formula and evaluation Task is simple mathematic computation, to very expensive when the TS is a pharmaceutical company and the Task is to perform a whole bunch of tests for a new drug, which may last years. 

	\item \textbf{Number of objectives} is the size of output vector $Y_m^i$. It could be either \textit{Single} ($m=1$), or \textit{Multiple} ($m=2...M$) objective. With respect to the number of objectives, the result of optimization will be either single Solution, or set of non-dominated (Pareto-optimal) Solutions respectively.
\end{itemize}

Most Optimization Problem types could be obtained by combining different types of each characteristic listed above.

In this thesis we tackle real life combinatorial problem where most prominent incumbents are Bin Packing~\cite{martello1990bin}, Job-shop Scheduling~\cite{blazewicz1996job} or Vehicle Routing~\cite{toth2002vehicle} Optimization Problems.
All combinatorial problems are \textit{NP-Complete} meaning they are both \textit{NP} and \textit{NP-Hard}\cite{garey1979computers}.

As an example, let's grasp these characteristics for Traveling Salesman Problem (TSP)~\cite{applegate2006traveling} — an instance of vehicle routing and one of the most frequently studied combinatorial OP (here we consider deterministic and symmetric TSP).
The informal definition of TSP is as follows: ``Given a set of $N$ cities and the distances between each of them, what is the shortest path that visit each city once and return to the origin city?''
With respect to our previous definition of an Optimization Problem, Target System here is a function to evaluate the length of proposed path. The TSP distance (or cost) matrix used in this function for evaluation and it is clear that this TS exposes all internal knowledge and those it is a white box.
The input $X_n$ is a vector of city indexes those type of input data is non-negative integers. There are two constrains on path: it should contain only unique indexes (those, each city will be visited only once) and it should start and end from the same city:~$[2 \rightarrow 1 \rightarrow ... \rightarrow 2]$.
Since the cost matrix is fixed and not changing while during solving process, the TS is considered to be deterministic and cost for two identical paths are always the same (nevertheless, there exist Dynamic TSP where the cost matrix do changes in a runtime to reflect a more realistic real-time traffic updates\cite{cheong2011dynamic}).
It is extremely cheap to compute a cost for given path using cost matrix, those overall Solution evaluation in this OP is cheap, but the overall number of Solutions is $N!$. Since we are optimizing only the route distance, this is a Single objective OP.


\subsection{Optimization Problem Solvers}\label{BG: subsection OP Solvers}
Most Optimization Problems could be solved by an exhaustive search, though trying all possible combinations of input variables and choosing one providing the best objective value. This approach guarantees to find globally optimal solution to OP. But when the Search Space size significantly increase, the amount of time needed for brute-force search becomes infeasible and in many cases even relatively small problem instances could not be solved by an enumeration.

% TODO: highlight that exchaustive search provides guaranteed optimality

Here different optimization techniques come into play, and exposed by Target System characteristics could restrict and sometimes strictly define the applicable approach.
For instance, imagine you have white box deterministic TS with discrete constrained input data and cheap evaluation. The OP in this case could be solved using Integer Linear Programming, or heuristics approaches. If this TS turned out to be a black box, the ILP approaches will not be applicable anymore and one should consider using heuristics~\cite{biegler2004retrospective}.

Again, there exist a lot of different characteristics for Optimization Problem Solvers classification, but they are a subject of many surveying works for instance~\cite{biegler2004retrospective}. Here as the point of interest we decided to highlight only two of them.

\begin{itemize}[itemsep=8pt]
	\item \textbf{Final solution quality} perspective.
	\begin{enumerate}
		\item \textbf{Exact} Solvers are those algorithms that always provide an optimal Solution for OP.
		\item \textbf{Approximate} Solvers produce a sub-optimal output with guarantee in quality (some order of distance to the optimal solution).
		\item \textbf{Heuristics} Solvers do not give any worst-case guarantee for the final result quality.
	\end{enumerate}
	
	\item \textbf{Solution availability} perspective.
	\begin{enumerate}
		\item \textbf{Finalizing} algorithms, that report the results only at the end of their run.
		\item \textbf{Anytime} algorithms, designed for stepwise solution improvement, while solving the OP and those could expose intermediate results.
	\end{enumerate}
	
\end{itemize}

Each of these algorithm characteristics provide their own advantages paying sacrificing other. As instance, if solution is not available at any time, one will not be able to control the optimization process, but if it does available, the overall performance may suffer. 
If the former features are more or less self-explanatory, we explain the later ones in more details including examples.

\subsubsection{Solution Quality Classes}
\paragraph{Exact Solvers.}
As stated previously, the exact algorithms are those, which always solve an OP to guaranteed optimality.

For some OP it is possible to develop an effective algorithm that is much faster than the exhaustive search — they run in super-polynomial time instead of exponential still providing an optimal solution. As authors claimed in~\cite{woeginger2003exact}, if the common belief $P \ne NP$ is true, those super-polynomial time algorithms are the best we can hope to get when dealing with NP-complete (combinatorial) problems.

By the definition in~\cite{fomin2013exact}, the objective of an \textit{exact algorithm} is to perform significantly better (in terms of running time) than exhaustive search. In both works~\cite{woeginger2003exact,fomin2013exact} authors enumerated the main techniques for designing exact algorithms, each contributing in this `better' independently, those later they could be combined.

Here is a brief explanation of them:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Branching and bounding} techniques when applied to original problem, split the search space of all possible solutions (e.g. exhaustive enumeration) to set of smaller sub-spaces. More formally this process called \textit{branching the search tree} into sub-trees. This is done with an intent to prove that some sub-spaces never lead to an optimal solution and those could be rejected.
	
	\item \textbf{Dynamic programming across the sub-sets} techniques in some sort could be combined with the mentioned above branching techniques. After forming sub-trees, the dynamic programming attempts to derive solutions for smaller subsets and later combine them into solutions for lager subsets unless finally original search space solution obtained.
	
	\item \textbf{Problem preprocessing} could be applied as an initial phase of the solving process. This technique is dependable upon the underlying OP, but when applied properly, significantly reduces the running time. A toy example from~\cite{woeginger2003exact} elegantly illustrate this technique: imagine problem of finding a pair of two integers $x_i$ and $y_i$ that sum up to integer $S$ in $X_k$ and $Y_k$ sets of unique numbers ($k$ here denotes the size of sets). The exhaustive search will enumerate all $x-y$ pairs in $O(k^2)$ time. But one could first preprocess the data sorting it, after that use bisection search repeatedly in this sorted array and search for $k$ values $S - y_i$. The overall time complexity reduces to $O(k\log(k))$.
\end{itemize}

\paragraph{Approximate Solvers.} When the OP cannot be solved to optimal in polynomial time, people start thinking in alternative ways to tackle it and commonly decide to relax their requirements to optimization.
Approximate algorithms are representatives of the theoretical computer science field that have been created in order to tackle the \textit{computationally difficult white box OP} (not solvable in super-polynomial time). Words of Garey and Johnson (computer scientists, authors of \textit{Computers and intractability} book~\cite{garey1979computers}) could play a perfect description of such approaches, built on mathematical proofs: ``I can't find an efficient algorithm, but neither can all of these famous people.''
%If the widely believed conjecture $P \ne NP$ is true, a wide range of OPs (ILP) are $NP-hard$ and cannot be solved with exact solvers in polynomial time, those require relaxation either in efficiency or quality of optimization.

In contradistinction to exact, approximate algorithms relax the quality requirements and solve an OP effectively with the provable assurances on the result distance from an optimal solution~\cite{williamson2011design}. The worst-case results quality guarantee is crucial in design of approximation algorithms and also involves mathematical proofs. 

One may ask a reasonable question ``How do these algorithms guarantee on quality, if the optimal solution is unknown ahead?'' Certainly it sounds contradicting, since knowing the optimal solution cancels an optimization problem itself. The answer to this question highlights a key technique in the design of approximation algorithms that is out of this thesis scope.

In~\cite{williamson2011design} authors provided several techniques of an approximate solvers design. For instance, the \textbf{Linear Programming} relaxation plays a central role in approximate solvers. It is well known that solving the ILP is \textit{NP-hard} problem, however it could be relaxed to polynomial-time solvable Linear Programming. %Those, one of techniques is relaxation of ILP to LP. An optimal solution for LP will have value $S_LP \le S_IP = OPT$ (for the minimization case). Here we could derive a lower bound for the original minimization or upper bound for maximization problem. 
Later fractional solution for the LP will be rounded to obtain a feasible solution for the ILP % that is within a cerain factor $f$ the value of the LP $S_LP$. Thus, the ILP solution will cost no more than $f * OPT$.
and different rounding strategies define separate technique for approximate solvers~\cite{williamson2011design}: 
\begin{itemize}[itemsep=8pt]
	\item \textbf{Deterministic rounding} follows predefined ahead strategy, while
	\item \textbf{Randomized rounding} will perform a round-up of each fractional solution value to integer uniformly.
\end{itemize}

In contrast to rounding, another technique requires building a \textit{Dual Linear Program} (DLP) for given Linear Program. This approach utilizes a \textit{weak} and \textit{strong duality} properties of DLP to derive the distance of LP solution to an optimal solution of original ILP. Other properties of DLP form a basis for \textbf{Primal-dual} algorithms. They start with a dual feasible solution and use dual information to derive a solution (possibly infeasible) for primal linear program. If the \textit{primal} solution is infeasible, algorithm modifies dual solution to increase the values of the dual objective function. In any case, these approaches are far beyond thesis scope, but in case of interest, reader could start own investigation from~\cite{williamson2011design}. 


\paragraph{Heuristics.} In contradiction to mentioned above solvers, heuristics do not provide any guarantee on the Solution quality. They are applicable not only to white box TS, but also to black box cases. These approaches are sufficient to quickly reach immediate, short-term goal in cases, where finding an optimal solution is impossible or impractical because of the huge search space size.

As in previous approaches, here exist many facets for classification.
We will start from the largest one, namely \textit{level of generality}:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Simple heuristics} are algorithms, specifically designed to tackle concrete problem. They fully rely on the domain knowledge, obtained from Optimization Problem. Simple heuristics do not provide any mechanisms to escape a local optimum and those could be easily trapped to it~\cite{pearl1984intelligent}.
	
	\item \textbf{Meta-heuristics} are high-level heuristics that not hardly rely on domain knowledge, but also provide some level of generality to control the search. Those, they could be applied to broader range of OPs. Often they are nature-inspired and comprise mechanisms to escape a local optima, but may converge slower than simple heuristics. For more detailed explanation we refer survey~\cite{bianchi2009survey}.
	
	\item \textbf{Hybrid-heuristics} arise as combinations of two or more meta-heuristics. They could be imagined as recipes merge from the cook book, combining the best decisions to create something new and hopefully better.
	
	\item \textbf{Hyper-heuristics} is a heuristic that operates on the search space of low-level heuristics choosing or constructing them to solve the OP in hand instead of directly tackling search space of original problem~\cite{burke2003hyper}. 
\end{itemize}

In upcoming section~\ref{bg: heuristics section}, dedicated to heuristics, we discuss each of aforementioned approaches in more details including examples.


\subsubsection{The Best Suited Solver Type}
\epigraph{``Fast, Cheap or Good? Choose two.''}{\textit{Old engineering slogan.}}

Now we have reached the crossroad and should make a decision, which way to follow.

First, we have reviewed exact solvers for the Optimization Problems. As mentioned above, they always guarantee to derive an optimal solution. Today, tomorrow, maybe in next century, but eventually exact solver will find it. The only thing we need is to construct an exact algorithm for our problem in hand. This approach definitely provides the best quality of final solution, however it sacrifices simplicity in building a solver and speed in solving the problem.

Next, we have an approximate solver. It does not guarantee to find exactly optimal solution, but instead suggest provably good one. From our perspective, the required effort for constructing an algorithm and proving its preciseness remains the same as for exact solvers however, this approach beats the previous one in speed of problem solving, sacrificing a reasonably small amount of the result quality. It sounds like a good deal.

Finally, remain heuristic approaches. They are super-fast in comparison to previous two in problem solving, in addition they are much simpler to apply for your specific problem — no need to build complex mathematical models or prove theorems. However, the biggest flaw in these approaches is absence of guarantee in solution quality those, one should consider them up to own risk.


As we mentioned in~\ref{BG: subsection OPs}, in this thesis is dedicated to facing a real-life problems such as TSP. They showed to be \textit{NP-complete} that is why we are not allowed to apply exact solvers, only approximate and heuristics. In both cases we are sacrificing a solution quality, although in different quantities, but the heuristic algorithms repay in time-to develop and providing first results. Modern world is highly dynamic, in business survive those who faster and stronger. In most cases former plays the settle role for success. Great products are built iteratively, enhancing existing results step-by-step leaving unlucky decisions behind. It motivates us to stick the heuristic approach within the thesis.

In following section~\ref{bg: heuristics section} we shortly survey different types and examples of heuristics with their properties, weaknesses and ways to deal with them. As the result, we select the best suited heuristics class for solving the problem in hand.


\section{Heuristic Solvers for Optimization Problems}\label{bg: heuristics section}
We base our descriptions of heuristics and their examples on mentioned in section \ref{BG: subsection OPs} Traveling Salesman Problem (TSP) \cite{applegate2006traveling}. The input data $X$ to our heuristics will be the Target System description in form of distance matrix (or coordinates to build this matrix), while as an output $Y$ from heuristics we expect to obtain the sequence of cities, depicting the route plan.

% In general, when a heuristic applied to particular problem do not use the gradient or Hessian matrix of the objective function for optimizations \cite{boussaid2013survey}.

Most heuristic approaches imply following concepts:
\begin{itemize}[itemsep=8pt]
	\item \textbf{Neighborhood} defines the set of Solutions, which could be derived performing single step of heuristic search.
	\item \textbf{Iteration} could be defined as an action (or set of actions) performed over Solution in order to derive a new, hopefully better one.
	\item \textbf{Exploration} (diversification) is the process of discovering previously unvisited and hopefully high quality parts of the search space.
	\item \textbf{Exploitation} (intensification) is the usage of already accumulated knowledge (Solutions) to derive a new one, but similar to existing Solution.
\end{itemize}

%Naturally, there are lots of characteristics that could be used to classify heuristics starting from the use of memory, kind of neighborhood, way to derive a solution (construction or perturbation) or number of solutions carried from one iteration to following. 

Now we review each mentioned in previous section heuristic type.
\todoy{is it needed?..}


\subsection{Simple Heuristics}
As we mentioned above, the simple heuristics are domain dependent algorithms, designed to solve a particular problem.
They could be defied as rules of thumb, or strategies to utilize exposed by Target System and obtained from previously found solutions information to control a problem-solving process~\cite{pearl1984intelligent}. 

Scientists draw the inspiration for heuristics creation from all aspects of our being: starting from observations of how humans tackle daily problems using intuition to mechanisms discovered in nature.
%Since each algorithm is tightly bound to the concrete OP instance, it is hard to distinguish commonalities among simple heuristics except of high domain knowledge utilization.

Two main types of simple heuristics were outlined in~\cite{burke2019classification}: \textit{constructive} and \textit{perturbative}. 

The first type implies heuristics which construct Solutions from its parts stepwise. 
The prominent example of constructive approach is the \textit{Greedy Algorithm}, a.k.a. \textit{Best Improvement Local Search}. When applied to Traveling Salesman Problem, it tackles the path construction simply accepting the next closest city from currently discovered one. Generally, the Greedy Algorithm follows the logic of making a sequence of locally optimal decisions those ends up in a local optimum after constructing very first Solution. 

The second type, also called \textit{Local Search}, implies heuristics which operates on completely created Solutions. The simple example of Local Search is the \textit{Hill Climbing Algorithm}: evaluate entire Neighborhood, move to the best found Solution and repeat. This approach plays a central role in many high-order algorithms, however could be very inefficient, since in some cases Neighborhood could be enormously huge. 
Another instance of perturbative algorithm is \textit{First Improvement Local Search}~\cite{voudouris1999guided}. This heuristic accepts better Solution as soon as it finds it during Neighborhood evaluation. The advantage of this methodology over the vanilla Hill Climbing is the velocity of Search Space traversal.

Indeed, since the optimization result is fully defined be the starting point, the use of simple local search heuristics might not lead to a globally optimal solution, but the advantage here is the implementation simplicity~\cite{williamson2011design}.

 
\subsection{Meta-Heuristics}
The meta-heuristic is an algorithm, created to solve wider range of hard optimization problems without need to deeply adapt to each problem.

The research in meta-heuristics field arise even before 1940s, when they have been used already, but not studied formally. In the period of between 1940s and 1980s first formal studies appeared and later till 2000s the field of meta-heuristics arises in wide when numbers of MHs were proposed. The period from 2000 and up until now in~\cite{sorensen2017history} authors refer as the time of framework growth. It is now a time, when meta-heuristics widely appear as frameworks, providing a reusable core and requiring only domain specific adaptation.

The prefix \textit{meta} indicates these algorithms to be a \textit{higher level} when compared to simple problem dependent heuristics. A typical meta-heuristic structure could be imagined as $n-T--H$ (template-hook) framework variation pattern. The template part is problem independent and fixed from changes, it forms a core of an algorithm and usually exposes \textit{hyper-parameters} for tuning. The hook parts are domain dependent and those should be adapted for problem in hand. Later T operates by set of Hs to perform an optimization.
Many MHs contain stochastic components, which provide abilities to escape from local optimum. However, it also means that the output of meta-heuristic is non-deterministic and it could not provide guarantee on the result preciseness~\cite{boussaid2013survey}.

The success of meta-heuristic optimizer on a given OP depends on the \textit{exploration vs exploitation balance}. If there is a strong bias towards diversification, the solving process could naturally skip a good solution while performing huge steps over the search space, but when the intensification dominating, the process will quickly settle in local optima. Usually it is possible to decompose MH onto simple components and clarify, to which of competing processes contributes each component. Mentioned above simple heuristic approaches suffer from high exploitation dominance since they usually do not have components that contribute to exploration.

In general, the difference among existing meta-heuristics lays in a particular way how they are trying to achieve this balance, but the common characteristic is that most all of them are inspired by real-world processes — physics, biology, ethology, evolution, etc.


\subsubsection{Meta-Heuristics Classification}
The development process of novel methodologies has slowed down and the research community began to organize created algorithms and many classification facets were distinguished.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{graphics/Background/Metaheuristics_classification}
	\caption{Meta-heuristics Classification.}
	\label{BG: MH classification}
\end{figure}

As an example, authors in~\cite{birattari2001classification} classification review highlight following characteristics:
\begin{itemize}[itemsep=8pt]
	\item The \textbf{walk-through search space method} could be either trajectory based or discontinuous. The former one corresponds a closed walk on the neighborhood where such prominent examples as \textit{Iterated Local Search}~\cite{lourencco2003iterated} or \textit{Tabu Search}~\cite{glover1989tabu} do exist. The later one allows large jumps in the search space with many MH exemplars such as \textit{Variable Neighborhood Search}~\cite{hansen2003variable} and \textit{Simulated Annealing}~\cite{kirkpatrick1983optimization}.
	
	\item The \textbf{number of concurrent solutions} could be either single or multiple (population). Such approaches as Tabu Search, Simulated Annealing or Iterated Local Search are examples of former and Evolutionary Algorithms~\cite{eiben2015evolutionary}, Ant Colony Optimization~\cite{dorigo2007ant}, Particle Swarm Optimization~\cite{kennedy1995particle} are instances of later.
	
	\item From the \textbf{memory usage} perspective, we distinguish those approaches which do and do not utilize memory. The Tabu Search explicitly use memory in forms of tabu lists to guide the search, but the Simulated Annealing is memory-less.
	
	\item \textbf{Neighborhood structure} could be either static or dynamic. Most local search algorithms such as Simulated Annealing and Tabu Search are based on static neighborhood. The Variable Neighborhood Search is an opposite case, where various structures of neighborhood are defined and interchanged while solving an OP. 
\end{itemize}

There are many more classification facets, with are out of this thesis scope. Picture~\ref{BG: MH classification} illustrates the summarized classification including some skipped characteristics and well-known meta-heuristic instances~\cite{wiki_MH_classification}.


\subsubsection{Meta-Heuristics Examples}\label{BG: MH Examples}
Now we shortly describe some prominent and widely used meta-heuristics. The motivation for doing so comes from later usage of them as the LLH in Hyper-Heuristic, described in section~\ref{Impl: LLH}.

\paragraph{Evolutionary Algorithms (EAs).} Evolutionary Algorithms are directly inspired by the processes in nature, described in evolution theory. The common underlying idea in all of these methods is as follows: if we put a population of individuals (Solutions) into an environment with limited resources (population size limit), a competition processes cause natural selection, where only the best individuals survive (compared by the optimization objective)~\cite{eiben2015evolutionary}.

Tree basic actions are defined as operators of EAs: the \textit{recombination} operator that when applied to selected among available in population candidate Solutions (parents) produces new ones (offspring); \textit{mutation} operator applied to Solutions creates a new and very similar one. 
Applying both operators will create a set of new Solutions — the offspring, whose fitness evaluated on TS. After that, the \textit{selection} operator applied to all available Solutions (parents and offspring) for keeping the population size within defined boundaries. This process is repeatedly iterated until some termination criteria fulfilled, for instance maximal iterations counter reached, number of TS evaluations exceed, or Solution with required quality found. The work-flow of EA depicted on picture~\ref{bg:pic:EAs}.

\begin{figure}
	\centering
	\includesvg{EA}
	\caption{Evolutionary Algorithm Control Flow}
	\label{bg:pic:EAs}
\end{figure}

Well-known examples of EAs include \textit{Genetic Algorithm}~\cite{sastry2005genetic}, \textit{Genetic/Evolutionary Programming}~\cite{koza1992evolution}, \textit{Evolution Strategies}~\cite{beyer2002evolution}, and many other algorithms.

\textbf{Genetic Algorithm} (GA) is the first association coming into mind when you hear words `Evolutionary Algorithms'. GA traditionally has a fixed work-flow: given initial population of \underline{$\mu$} (we underline hyper-parameters here and in other examples of algorithms) usually randomly sampled individuals, the parent selection operator creates pairs of parents where probability of each solution to become a parent depends on its fitness. After that, the crossover operator is applied to each pair with probability \underline{$p_c$} to produce children. Then newly created Solutions undergo mutation operator with independent probability \underline{$p_m$} (hyper-parameter). Resulting offspring perform tournament within selection operator and \underline{$\mu$} survivals replace current population~\cite{eiben2015popular}. Distinguishable characteristics of vanilla GA are usage of following operators: bit-strings Solution representation, one-point crossover recombination, bit-flip mutation and generational (only children survive) selection.

\textbf{Evolution Strategy} (ES) algorithms, in contradiction to GA, are working in a vector space of Solution representation and distinguish \underline{$\mu$} individuals population size and \underline{$\lambda$} offspring generated in one iteration. Often, ES utilizes a very useful feature of \textit{self-adaptation}: changing the mutation step sizes in a runtime. The self-adaptive information is related only to ES, but not to OP under consideration and appended to the individual's chromosome. While the general work-flow for all EAs remains the same, some among underlying operators are changed. Here, parent selection operator take a whole population into consideration, the recombination scheme could involve more than two parents to create one child. To construct a child, \underline{recombination operator} adds alleles from parents in two possible ways: (1) with uniform probability for each parent (discrete recombination), (2) averaging the weights of alleles (intermediate recombination). There are two general \underline{selection schemes}, used in such algorithms: $(\mu,\lambda)$ — discard all parents and selecting only among offspring highly enriching exploration, and $(\mu+\lambda)$ — include predecessor Solutions into selection, a.k.a. \textit{elitist selection}~\cite{eiben2015popular}.

\paragraph{Simulated Annealing (SA).} This is the other type of meta-heuristic, inspired by the technique used in metallurgy to obtain 'well-ordered' solid state of metal. Annealing technique imposes a globally minimal internal energy state and avoids local minimums semi-stable structures. 

The search process in SA treated as the metal with a high temperature at the beginning and lowering it to minimum while approaching the end. %SA algorithm handles the optimization objective as annealing threat the material energy. 
SA starts with initial Solution $S$ creation (randomly or using some heuristic) and temperature parameter \underline{$T$} initialization. At each iteration, new solution candidate $S^*$ sampled within a neighborhood $N(S)$ of current $S$ and the selection criteria evaluated based on $S^*$ quality and $T$ value. $S^*$ replaces $S$ if (1) optimization objective $f(S^*)$ dominates over $f(S)$ or (2) with a probability that depends on quality loose and current $T$ value: $p(T, f(S^*), f(S)) = \exp(-\frac{f(S^*) - f(S)}{T})$. %for minimization OP and $p(T, f(S^*), f(S)) = \exp(-\frac{f(S) - f(S^*)}{T})$. 
At each iteration the temperature parameter $T$ value is decreased following some type of \underline{annealing schedule} also called as \textit{cooling rate}~\cite{boussaid2013survey}. The weak side here is that the quality of each annealing schedule is problem dependent and cannot be determined beforehand, however SA algorithms with parameter control do exist and address this problem by changing the Cooling Rate or temperature parameter $T$ during the search process. Later we shortly review these techniques in section~\ref{bg: parameter control: SA}.


\subsection{Hybrid-Heuristics}
The hybridization of different systems often provides a positive effect — taking the advantages of one system you merge them with odds of other system those getting the best from both of them. The same idea applicable in case of meta-heuristics. Imagine you have two algorithms, biased towards exploration and exploitation respectively. Applying them separately, the expecting results in most cases may be far away from optimal as the outcome of disrupted diversification-intensification balance. But when merging them into, say, repeater stages of hybrid heuristic, one will obtain both advantages of escaping local optima and finding good quality results. 

Most of available hybridization are done exactly with this idea — \textit{staging combination} of two heuristics, one exploration and second exploitation suited for getting outperforming hybrid.

The methods to construct hybrids are mostly defined by the underlying heuristics and those, to the best of our knowledge, could not be generalized and classified well. One common shared characteristic is usage of \textit{staging approach}, where the output of one algorithm is used as initial state of other. 

As for simple heuristics, we will introduce some examples of performed hybridization in order to give you a better understanding of possible hook parts within algorithms and influence of aforementioned balance onto the search process.

\subsubsection{Hybrid-Heuristics Examples}
\paragraph{Guided Local Search + Fast Local Search~\cite{tsang1997fast}.}
The main focus of \textit{GLS} here is on the Search Space exploration and guidance of process using incubated information. In some sort, \textit{GLS} is closely related to the \textit{Frequency-based memory} used in Tabu Search. During the runtime, \textit{GLS} modifies the cost function of the problem to include penalties and passes this modified cost function to local search procedure. These penalties form memory that characterize a local optimum and guide the process out of it. A local search procedure carried out by the \textit{FLS} algorithm, where the main advantage is quick neighborhood traversal. It is done by braking it up into a number of small sub-neighborhoods, and ignoring those without an improving moves by performing depth first search over these sub-neighborhoods. At some point of time \textit{FLS} reaches the local optimum and passes back the control in \textit{GLS} to repeat the iteration.

\paragraph{Direct Global + Local Search~\cite{syrjakow1999efficient}.}
This hybridization combines mentioned in name optimization strategies into two stages: stochastic global coarse pre-optimization refined by deterministic local fine-optimization. In first stage authors apply one of two described earlier meta-heuristics — Genetic Algorithm, or Simulated Annealing. The transition from Global to Local search happens after reaching the predefined conditions, for instance, when the number of Target System (goal function) evaluations exceeds a boundary, or when no distinguishable improvement was done in the last few iterations. Then, the Pattern Search algorithm~\cite{hooke1961direct} also known as direct/, derivative-free/, or black-box search plays role of Local Search heuristic in this combination. The hybrid-heuristic terminates when Pattern Search converges to local optima.

\paragraph{Simulated Annealing + Local Search~\cite{martin1996combining}.}
After brief explanation of previous two hybrids, an observant reader might make a guess what happens in this particular case, and he will be precisely correct!
Authors named this method `Chained Local Optimization', in other words it is yet another representative of staged hybridization. 
Iteration starts with the current Solution perturbation — 'kick', referring a dramatic change of current position within the search space. After this, some Local Search heuristic applied to intensify obtained Solution. Reaching the local optimum, Local Search pass the control flow back to the Simulated Annealing for acceptance criteria evaluation in order to accept or reject a new Solution, which finishes one iteration.

\paragraph{EMILI~\cite{pagnozzi2019automatic}.}
Easily Modifiable Iterated Local search Implementation (EMILI) is a framework-like system for automatic generation of new hybrid stochastic Local Search algorithms. \textit{EMILI} is a solver for Permutation Flow-Shop Problems (PFSP), also known as Flow Shop Scheduling problems in which performed a search of an optimal sequence of steps to create products within a workshop.
Here authors have implemented both algorithmic- and problem-specific building blocks, defined grammar-based rules for those blocks composition and used an automatic parameter tuning tool \textit{IRACE}~\cite{lopez2016irace} to find a high performing algorithm configurations. The work-flow of \textit{EMILI} could be split onto three steps: (1) adaptation of grammar rules to specific PFSP objective representation type (Make-span, Sum completion times, Total tardiness), (2) generation of all possible hybrid heuristics for each of PFSP type and (3) appliance of iterated racing in \textit{IRACE} to select the best performing hybrid for specific problem type. 

From our perspective, described approach of automatic algorithm generation is direct incumbent of construction Hyper-Heuristics strategies described in upcoming section~\ref{bg: hh}, however we are not authorized to change the system class (from hybrid- to hyper-heuristic) defined by \textit{EMILI} authors.
\todoy{or we can do it?}


\subsection{No Free Lunch Theorem}
A nature question could arise ``If we have all this fancy and well-performing heuristics, why should we put an effort in developing new algorithms, instead of using the existing?'' And the answer to this question is quite simple — the perfect algorithm suited for all OP does not exist and can not exist. 
The empirical research has shown that some meta-heuristics perform better with some types of problems, but poorly with others. In addition to that, for different instances of the same problem type, the same algorithm could result in unexpected performance metrics. Moreover, even in different stages of same problem solving process the dominance of one heuristic over another could change. 

All search algorithms perform exactly the same, when the results are averaged over all possible Optimization Problems. If an algorithm is gaining the performance in one problems class, it loses in another class. This is a consequence of so-called \textbf{No Free Lunch Theorem for Optimization} (NFLT)~\cite{wolpert1997no}.

In fact, one could not predict, how exactly will behave one or another algorithm with problem in hand. A possible and the most obvious way is to probe one algorithm and compare its performance to another one during problem solving process. In this case simple heuristics and meta-heuristics are out of competition, since once you solved the Optimization Problem you probably wouldn't optimize a second time.
Here come \textbf{Hyper-Heuristics} to intelligently pick heuristics that is suitable to problem in hand. We will proceed with their description and how they deal with the NFLT consequences in following section.


\subsection{Hyper-Heuristics}\label{bg: hh}
A lot of state-of-the-art heuristics and meta-heuristics are developed in a complex and very domain-dependent way, which causes problems in an algorithm implementation reuse. It motivated research community to raise the level of generality at which the optimization systems can operate and still provide good quality Solutions for various Optimization Problems. 

The term \textbf{Hyper-Heuristic} (HH) was defined to describe an approach of using some \textit{High-Level-Heuristics} (HLH) to select over other \textit{Low-Level-Heuristics} (LLH) and apply them to solve the \textit{class of Optimization Problems} rather than particular instance. Indeed, scientists report that the combination of different HLH produces better results than if hey were applied separately~\cite{drake2019recent} — note previously discussed hybrid-heuristics.
This behavior can be explained by the nature of search process and how it evolves in time. When you apply a heuristic, it sooner or later converge to some extreme point, hopefully global optimum. But it is `blind' to other, not visited regions in the Search Space. Changing the trajectory of investigation by (1) drastically varying the Neighborhood, (2) changing the strategy of Neighborhood exploration and exploitation could (1) bring you to those previously unreachable zones (2) in more rapid ways. However, usually it is hard to predict how one LLH will behave in every stage of the search process in comparison to another. In Hyper-Heuristics, this job was encapsulated into the HLH and performed automatically. 

In~\cite{moriarty1999evolutionary} authors made infer that Hyper-Heuristics can be viewed as a form of Reinforcement Learning, which is a logical conclusion especially if we rephrase it to \textit{Hyper-Heuristics utilize Reinforcement Learning methodologies}.

\begin{figure}
	\centering
	\includesvg{HH}
	\caption[Hyper-Heuristics]{Hyper-Heuristics.}
	\label{bg:pic:HH}
\end{figure}

The new concept which implicitly was used in Meta-Heuristics, but explicitly pointed out in Hyper-Heuristics is the \textbf{Domain Barrier}, see picture~\ref{bg:pic:HH}\footnotetext[1]{Icons from \href{https://thenounproject.com/}{thenounproject.com}}.
As we told previously, HH do not tackle an OP directly, but use LLH instead. This means, that usually HH are minimally aware of the domain details such as what are data types, relationships, etc. within a domain. This information rather encapsulated in LLHs and those HHs could be used to broader range of Optimization Problems.

With this idea, many researchers started to create not only Hyper-Heuristics to tackle a concrete optimization problem class, but also frameworks with building blocks for their creation.


\subsubsection{Classification}
Although, the research in Hyper-Heuristics field is actively ongoing, many algorithm instances were already created and some trials to organize approaches were conducted in~\cite{ryser2014review,drake2019recent,burke2019classification}.

Researchers in their surveys classify HHs by different characteristics, some of which overlap, but it also happens that important (from our perspective) features were not highlighted in all works. 

In this section we present the union of those important Hyper-Heuristics classification facets to better justify the goal of this thesis.

We begin with the two broadest classes, which differentiate HH \textbf{routine}, also called as \textbf{nature of High-Level-Heuristic Search Space}~\cite{burke2013hyper,burke2019classification,drake2019recent}.
The first class are Hyper-Heuristics to \textit{select} Low-Level-Heuristic, in other words \textit{Selection Hyper-Heuristic}. All our previous references to Hyper-Heuristics were made this concrete type. These algorithms operate in the Search Space, defined by complete and rather simple Low-Level-Heuristics that solve Optimization Problem. The task of HLH here is to pick the best suited LLH (or sequence of LLHs) based on available prior knowledge and apply it to OP underway. Note, that staging Hybrid-Heuristics could be viewed as Solutions of Selection HHs.
Hyper-Heuristics of the second class seek to \textit{construct} LLH following some predefined receipt and using the atomic components of other heuristics as Lego bricks. The other commonly used name here is \textit{Construction Hyper-Heuristics}. These approaches often lead to creation of new and unforeseen heuristics that are expected to reveal good performance while solving the problem in hand.

Next, the distinction in \textbf{nature of Low-Level-Heuristics Search Space} arises. 
In other words: ``How does the LLH derive Solutions for the OP?'' Authors in~\cite{burke2013hyper,burke2019classification,drake2019recent} distinguished \textit{construction} LLHs where Solution creation happens each time from scratch and \textit{perturbation} LLHs where new Solutions created from parts of already existing ones.

The other broadly used characteristic is the \textbf{memory usage method}. 
From this perspective we distinguish Hyper-Heuristics in which the learning happens \textit{on-line}, \textit{off-line} or learning mechanisms are \textit{not present} at all~\cite{ryser2014review,burke2019classification}.
\begin{itemize}[itemsep=8pt]
	\item In \textbf{on-line} case, the HH derives an information, used to select among LLH, while those LLH are solving the problem.

	\item In \textbf{off-line} case, the learning happens before solving concrete Optimization Problem. Here one should first train an HH solving other homogeneous problem instances by underlying LLHs (off-line learning phase). After that, the HLH will be able to choose among LLHs and those be applicable to problem in hand (on-line use phase). Note, that this approach also requires creation of meta-features extraction mechanism and its application to every Optimization Problem.

	\item There exist also \textbf{mixed} cases, where learning happens first in off-line and later also in on-line phase. Definitely it is a promising (in terms of results quality) research direction, despite it high complexity.
	
	\item In the last case \textbf{no learning} mechanisms present, therefore HLH here performs some sort of Random Search over LLH Search Space. At first sight, it may look like weak approach, but looking onto Variable Neighborhood Search Meta-Heuristic we would doubt it.
\end{itemize}


%Yet another faced of Hyper-Heuristics classification is the way of assigning \textit{hyper-parameters} (here we use parameters and hyper-parameter concepts interchangeably) for LLHs, or their components~\cite{drake2019recent}. We analyzed surveys and find out that some researchers do not explicitly differentiate approches with respect to nature of parameter settings~\cite{ryser2014review,burke2013hyper,burke2019classification}, while other do~\cite{drake2019recent}:
%\begin{itemize}
%	\item In \textbf{static} assignment, the underlying heuristics use provided beforehand (usually default) hyper-parameters and do not change them while solving the problem in hand.

%	\item The \textbf{dynamic} case uses some kind of rule for parameters changing, specified in advance.

%	\item There exist also an \textbf{adaptive} approach, in which HH assigns the parameters for LLH as the response to the learning process. In some sort, it is similar to the parameter control techniques used in Meta-heuristics.
	
%	\item And finally, a \textbf{self-adaptive} approach where underlying LLHs comprise \textit{parameter control} techniques and those search for the best solution for OP and own parameter settings simultaneously.
%\end{itemize}


For more detailed analysis, description, other classification facets and respective Hyper-Heuristic examples we encourage reader to look into resent classification and surveying researches~\cite{burke2003hyper,ryser2014review,drake2019recent,burke2019classification}.

\subsubsection{Hyper-Heuristics Instance Examples}\label{bg: hh examples}
\paragraph{Hyper-Heuristic for Integration and Test Order Problem~\cite{guizzo2015hyper}.} \textit{HITO} is an example of generational HH. LLHs in this case are presented as a composition of basic EAs operators — crossover and mutation forming multi objective evolutionary algorithms (MOEA). HH selects those components from $jMetal$ framework~\cite{durillo2011jmetal} using interchangeably Choice Function (in form of weighted linear equation) and Multi Armed Bandit based Heuristic to balance exploitation of good components and exploration of new promising ones.


\paragraph{Markov Chain Hyper-Heuristic~\cite{mcclymont2011markov}.} \textit{MCHH} is an on-line selective Hyper-Heuristic for multi-objective continuous problems. It utilizes reinforcement learning techniques and Markov Chain approximations to provide adaptive heuristic selection method. While solving an OP, \textit{MCHH} updates prior knowledge about the probability of producing Pareto dominating Solutions by each underlying LLH using Markov Chains those guiding an LLH selection process. Applying on-line reinforcement learning techniques, this HH adapts transition of weights in the Markov Chains constructed from all available LLHs those updating prior knowledge for LLH selection.


\subsubsection{Hyper-Heuristics Frameworks Examples}\label{bg: hh fw examples}
\paragraph{Hyper-Heuristics Flexible Framework~\cite{ochoa2012hyflex}.} \textit{HyFlex} is a software skeleton, built specifically to help other researchers creating Hyper-Heuristics. It provides the implementation of components for 6 problem domains (Boolean Satisfiability, Bin Packing, Personnel Scheduling, Permutation Flow Shop, Traveling Salesman and Vehicle Routing problems) such as problem and solution descriptions, evaluation functions and adaptations for set of Low-Level-Heuristics. The set benchmarks and comparison techniques to other built HHs on top of \textit{HyFlex} are included to framework as well. 

The intent of \textit{HyFlex} creators was to provide Low-Level features that enable others to focus directly on High-Level-Heuristics implementation without need to challenge other minor needs. It also brings a clear comparison among created HLH performance, since the other parts are mostly common. From the classification perspective, all derivatives from the \textit{HyFlex} framework are Selection Hyper-Heuristics, however they utilize different learning approaches. Algorithms, built on top of \textit{HyFlex} framework could be found in many reviews~\cite{misir2012intelligent,ryser2014review,drake2019recent} or on the CHeSC 2011 challenge website\footnote[1]{\href{http://www.asap.cs.nott.ac.uk/external/chesc2011/}{Cross Domain Heuristic Search Challenge website: asap.cs.nott.ac.uk/external/chesc2011/}} (dedicated to choosing the best HH built on top of \textit{HyFlex}).

Along with \textit{HyFlex}, a number of hyper-heuristic-dedicated frameworks is growing, some of them are under active development while others are abandoned:
\begin{itemize}[itemsep=8pt]
	\item \textit{Hyperion}~\cite{swan2011hyperion} is a construction hyper-heuristic framework aiming to extract information from OP search domain for identification of promising components in form of object-oriented analysis.
	
	\item \textit{hMod}~\cite{urra2013hMod} framework allows not only to rapidly prototype an algorithm using provided components, but also to construct those components using predefined abstractions (such as $IterativeHeuristic$). In current development stage, developers of \textit{hMod} are focusing on creation of development mechanisms rather than providing a set pre-built heuristics. 
	
	\item \textit{EvoHyp}~\cite{pillay2017evohyp} framework focuses on hyper-heuristics created from evolutionary algorithms and their components. Here authors enable framework users to construct both selection and generation HHs for both construction and perturbation LLHs types.
\end{itemize}


\subsection{Conclusion on Heuristic Solvers}
To conclude our review on Heuristic approaches for solving Optimization Problems, we shortly remind you pros and cons of each heuristic level.

On the basis remain Simple Heuristics with all their domain-specific knowledge usage and particular tricks for solving problems. Usually, they are created to tackle a concrete problem instance in hand applying simple algorithmic approach. The simplicity of application and usually fast runtime paid back by medium results quality.

On the next level inhabit Meta-Heuristics. They could be compared with more sophisticated Solutions hunters which could not only charge directly, but also take a step back when stuck in a dead end. This additional skill enables them to survive in new and complex environments (Optimization Problems), however some adaptations to understand a concrete problem and parameter tuning for better performance still should be performed.

Among with MHs exit Hybrid-Heuristics. There is nothing special here, they just took some survival abilities from several Meta-Heuristics hoping to outperform and still requiring adaptation and tuning. In some cases this hybridization provides it advantage, but as the time shows, they did not force out MHs. Those we can conclude that the provided balance between development effort and exposed results quality not always assure users to use them.

Finally, the chosen ones that lead the others, Hyper-Heuristics are on the upper generality level. 
Operating by the other Heuristics, HHs analyze how good former are and make use of this knowledge by solving the concrete problem using those best suited Heuristics. Imposing such great abilities, Hyper-Heuristics tackle not only the concrete optimization problem, but entire class of problems.


\section{Setting Heuristic Parameters}\label{bg: Setting Heuristic Parameters}
The most of existing learning algorithms expose some parameter set, needed to be assigned before using this algorithm. Modifying these parameters, one could change the system behavior and possible result quality.

When we are talking about the problem of settings the best parameters, following terms should be refined explicitly:
\begin{enumerate}
	\item \textbf{Target System (TS)} is the subject which parameters are undergoing changes. Simply, it could be a heuristic, machine learning algorithm or other system.
	\item \textbf{Parameter} is one of exposed by TS setting hooks. It should be described in terms of its type and possible values.
	\item \textbf{Configuration} is the unique combination of parameter values, sufficient to run TS.
	\item \textbf{Search Space} is the set of all possible Configurations for defined Parameters.
\end{enumerate}

In this thesis we use notions of \textit{Parameter} and \textit{Hyper-Parameter} (HP) interchangeably, since the approaches discussed in this section are generally applicable also in Machine Learning cases. For instance, consider Neuron Network (NN). Hyper-parameters in this case will specify the structure (number of hidden layers, units, etc.) and learning process (learning rate, regularization parameters values, etc.) of network and changing them dramatically affect performance and results.

One frequent incumbent of Optimization Problems is \textbf{Parameter Settings Problem} (PSP) — searching of hyper-parameter values that optimize some characteristic of TS. When talking about NN example, we could solve PSP in order to maximize network accuracy in given dataset (Single Objective PSP). Taking into account a number of characteristics simultaneously such as running time and prediction accuracy results in Multi-Objective PSP.

The same applies to heuristics: proper assignment of hyper-parameters has a great impact on exploration-exploitation balance and those on algorithm performance.

% the motivation of using parameter tuning system as basis for hyper-heuristics
% TODO: this block should be in conclution
We believe, that a central role of off-line selective Hyper-Heuristics, namely solving the algorithm selection problem could be represented as the parameter tuning problem, where instead of parameter selection we select entire system according to reported performance. With this idea in mind, we investigate a possibility of turning parameter tuning system into a Hyper-Heuristic.
For doing so, our next step is the review and comparison of existing parameter tuning approaches and systems. 


\subsection{Approaches for Parameter Tuning}
% TODO: rename to "Parameter Setting", split "Parameter tuning" and "Parameter control"
Nowadays there exist wast number of ways to assign parameters for system. 
One of the simplest and error-prone ways is just trusting your (or someones else) intuition and using those parameters that seems to you more or less logic for particular system and problem instance. People quickly abandoned it in favor of automatic approaches, since novel computational capacities easily provide a possibility for it.

We briefly outline existing automated approaches illustrating each with a picture~\ref{bg: fig:automated parameter tuning approaches}. Each graphic shows dependencies between parameters $X_1$ ($x$ axis), $X_2$ ($y$ axis) and optimization subject values (which we are maximizing here) along those axes.

\begin{figure}
	 \centering
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\linewidth]{graphics/Background/hyperparameter-grid-search.png}
		\caption{Grid}
		\label{bg: fig:automated parameter tuning approaches: grid}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\linewidth]{graphics/Background/hyperparameter-random-search.png}
		\caption{Random}
		\label{bg: fig:automated parameter tuning approaches: random}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\linewidth]{graphics/Background/hyperparameter-learning-search.png}
		\caption{Learning}
		\label{bg: fig:automated parameter tuning approaches: learning}
	\end{subfigure}
	\caption{Automated Parameter Tuning Approaches\protect\footnote{Graphics from~\cite{koch2017automated}}.}
	\label{bg: fig:automated parameter tuning approaches}
\end{figure}


\paragraph{Grid Search Parameter Tuning.} It is a simple approach of solving the search problem by exchanging enumeration of possible solutions (parameter sets) for relaxed problem instance. This relaxation is derived by user specifying finite number of possible values for each of hyper-parameters under consideration. After evaluating each possible configuration for system under estimation, algorithm reports the best found solution. Obviously, this approach could skip promising parts of search space as shown on picture~\ref{bg: fig:automated parameter tuning approaches: grid}.


\paragraph{Random Search Parameter Tuning.} This methodology relies on random (usually uniform) sampling of hyper-parameters and their evaluation on each iteration. At first sight, it might look unreliable to chaotically traverse the search space. But empirical studies show that with growing number of evaluations this technique starts to outperform grid search~\cite{bergstra2012random}. Compare visually the best configurations (highlighted in pink) found by grid (picture~\ref{bg: fig:automated parameter tuning approaches: grid}) and random search (picture~\ref{bg: fig:automated parameter tuning approaches: random}) techniques.

\paragraph{Model Based Search Parameter Tuning.} In most cases, the dependencies between tuned parameter values and optimization objective do exist, can be observed and utilized for hyper-parameter tuning by predicting which parameter combinations could produce a better results, those making guesses more precise. As it showed on picture~\ref{bg: fig:automated parameter tuning approaches: learning}, after accumulating more information, learning algorithm will make more precise guesses, what in contrast to previously described model-free approaches is more preferable and robust.
With this idea researchers started to build systems that (1) traverse the search space more efficiently and (2) could mimic dependencies between parameters and objective resulting in surrogate models. The later is some sort of enhancement used in combination with former enabling you to simulate evaluation of real system instead of expensive real probe.

\paragraph{Parameter control.} One prominent characteristic of the system is its adaptation ability in runtime. In particular, if we are talking about the process of problem solving, it is a well-known fact, that algorithm performing promisingly in early beginning, later could struggle hard and vise versa. This could be caused by various reasons, and it is hard to tell, which of them you are facing right now. Parameter control in such cases is the ability of system to change its parameters automatically as the response to changes in solving process in other words it is known as \textit{on-line} version of parameter tuning. The drawback of this approach lays in its generality, since every algorithm should implement its own parameter control technique that is moreover often dependent upon the underlying problem.


\subsection{Systems for Model-Based Parameter Tuning}\label{bg: parameter tuning expamples}
The parameter tuning is an obligatory task for getting the maximum system performance and should be done at design time.
Novel approaches for tuning are usually built in form of frameworks with exposed hooks for attaching system under estimation.

Naturally, the Target System evaluations here supposed to be extremely costly since they require real execution of target algorithm, that is why algorithms in those frameworks are trying to utilize every single bit of information from evaluations, building surrogate models and using Bayesian optimization approaches for making each evaluation valuable.

In this section we review some among existing open-source parameter tuning systems from following perspectives:%, that could be used as basis for Hyper-Heuristic.
%TODO: in implementation. While analyzing, we discuss each system with following perspectives, highlight characteristics that later will be used to select the best suited system as basis for Hyper-Heuristic:
\begin{itemize}
	\item \textbf{Conditional parameters support} It exposed for user ability of tuning system to describe and tackle conditional dependencies between hyper-parameters. As an example imagine parameter that can take only some specific values, for instance \textit{crossover type} in Genetic Algorithm could be \textit{Partially Mapped Crossover ($PMX$)}, \textit{Cycle Crossover ($CX$)}, etc.~\cite{larranaga1999genetic} Selecting concrete crossover type, one will also need to specify respective parameters for this crossover type. But it could turn out, that these parameters are illegal, when the other crossover type is selected. This type of dependency could be described by parent-child relationship, however other types of dependencies also exists.
	
	\item \textbf{Parameter types support} It is one of basic features required in system to be usable. Namely, Target System parameters could be not only numerical (more concretely integer or fractional), but also categorical in form of strings, boolean values, etc. Considering categorical data types, they could be either nominal or ordinal. Difference between categorical types lays in fact, that the latest depict not only possible atomic values, but also order between them. For concrete example we discuss the Genetic Algorithm again with following parameters: population size (numerical integer in range $[1...\inf)$), mutation probability (numerical fractional $[0...1]$), crossover type (categorical nominal ${PMX, CX}$). Indeed, population size could also be displayed as a set of finite values ${10, 100, 1000}$ (categorical ordinal type).
	
	\item \textbf{Extensibility} This feature is crucial in cases, if one would like to try a new promising and intriguing learning algorithm, that was not included in parameter tuning system yet. In fact, one may need not only new learning algorithm, but other features like non-trivial stopping criterion, tools for handling stochastic behaviors, or different strategies for random sampling (which are utilized while tuning system is learning before making a prediction).
	
	\item \textbf{Parallel Evaluations }The simultaneous evaluation of multiple Configurations required for utilizing available computing resources that could scale horizontally. 
	
	% TODO: implementation \item[Support for Online Problem Solving.] This is a bit complex characteristic of system that we are willing to distinguish. As it turns out, most parameter tuning systems require full evaluation of Target System for Configuration comparison. However, in case of Hyper-Heuristic, the Configuration evaluation is a trial to solve the problem in hand using particular $LLH$ (tuned parameter) Here we compare final result quality, reported by each $LLH$
	
\end{itemize}


\subsubsection{IRACE~\cite{lopez2016irace}}\label{bg: irace}
The first system under investigation is an implementation if Iterated Racing Algorithm\cite{birattari2010f} in \textbf{IRACE} package for off-line hyper-parameters tuning.
The underlying methodology consists three main steps: (1) using prior knowledge, sample new Configurations, (2) distinguish the best ones among sampled using racing algorithm and (3) update prior knowledge to bias next samplings in (1) towards better Configurations. 
Prior knowledge here represented as probability distribution for each tuned parameter independently (truncated normal and discrete distributions for numeric and categorical hyper-parameters respectively). During update step (3), the probability distributions are increased for those values, where the best Configurations were found.

Racing step (2) could be described as process of running Target System using Configuration under evaluation on a set of heterogeneous problem instances. While solving those instances, the statistically worse-performing Configurations are rejected and racing proceeds with remaining ones. This process continues until reaching required number of survivals or number of solved problem instances. 

As it was mentioned above, \textbf{IRACE} supports various data types such as numerical and categorical as well as the possibility of conditions description. While the problem of data types solved by different distributions used, the conditional relationships are determined by the dependency graphs. During sampling (1), first non-conditional and only afterwards dependent parameters are sampled, if respective conditions are satisfied. The framework highly utilize racing algorithm for evaluations and \textit{Friedman test}~\cite{conover1980practical} or alternatively \textit{paired t-test} for statistical analysis of parameters, those it is static in terms of variability and extensibility of learning mechanisms. In terms of parallel evaluations, the algorithm is doing well at the beginning of each racing step, however as the process proceeds less and less evaluations are executed in parallel those all available resources are utilized optimally at all steps of algorithm.


\subsubsection{SMACv3~\cite{hutter2011sequential}}\label{bg: smac}
Sequential Model-based Algorithm Configuration (\textbf{SMAC}) is a system for parameters optimization developed by AutoML research group (here we review \textbf{SMACv3}). 

In this research, scientists generalized the process of parameter tuning under the term \textit{Sequential Model-Based Optimization (SMBO)} as iterations between fitting models and using them to make choices of Configurations.

It is an extension introducing learning models to previously existing Random Online Aggressive Racing (\textbf{ROAR}) mechanism. Authors in their research showed that the machine learning mechanisms and regression models in particular could be applied not only for tuning parameters, but also for optimizing any expensive black-box functions in general (we believe that the last holds also for the majority of other parameter tuning systems). 

This system development was directed to tackle existing (at that point of time) limitations of all published \textbf{SMBO} approaches, namely expanding an applicability not only to numerical, but also to categorical parameters and optimizing target algorithm not only on single, but on number of problem instances (benchmark set of problem instances) for facing the variance.
A routine in Sequential Model-based Optimization is somehow similar to one implemented in \textbf{IRACE} and could be imagined as iterated application two tree steps: (1) building learning model, (2) using it for making choices which Configurations to investigate next and (3) actual evaluation of sampled Configurations.
The evaluation (3) here carried out by original ROAR mechanism in which evaluation of each new candidate Solution continues until enough data (from benchmark set of problem instances) obtained to either replace current Solution or reject candidate. In contrary to original model-less ROAR, at step (1) \textbf{SMAC} builds machine learning regression random forest~\cite{breiman2001random}. The usage of regression decision trees (which form the forest) was motivated by known fact that they are known to fit well with categorical data. Later, at step (2) an Iterative Local Search (\textbf{ILS}) heuristic applied in combination with Bayesian optimization technique evaluating \textit{Expected Improvement} (EI). \textbf{ILS} started on the best previously found Configurations and was used for sampling new promising ones while distinguish between them carried out by EI using regression model built at step (1). EI is large for those Configurations, with low predicted cost, or for those with high uncertainty those providing exploration-exploitation balance automatically~\cite{jones1998efficient}.

% TODO: try a random forest as a 1ST level of HH, SMAC paper (short version), PAGE 7, CITES 18, 19.

Exposing such a great learning capabilities and using Expected Improvement technique that guarantees to converge the search process to global optimum given enough time, the major drawback in this system is lack of flexibility to include conditional dependencies between parameters into search space description. 
One of possible solutions here could be the use of conditional-aware neighborhood definition in \textbf{ILS} (currently it just carried out by sampling in Gaussian distribution). In fact, used Search Space representation framework ConfigSpace\cite{configspace} is able to specify dependencies among hyper-parameters and verify the Configuration validity in terms of parameter conditions violation. However, to the best of our knowledge, during the Configuration sampling those conditions are not taken into account and could be broken thus resulting in illegal parameter combination. Obviously those cases are handled and broken Configurations are rejected, but we believe that in case of `sparse' Search Spaces (where significant amount of parameter combinations are restricted by conditions) this approach could lead to ineffective sampling and great struggling in system predictive abilities. Unfortunately, we did not find any officially published evaluations of such cases and could only make guesses based on own intuition and framework developers advises for SMACv3 application in such cases~\footnote[1]{Visit GitHub repository of SMACv3 for more info~\url{https://github.com/automl/SMAC3/issues/403}}.

ROAR mechanism is the derivative from \textbf{FocusedILS} algorithm (solver in \textbf{ParamILS} parameter tuning framework~\cite{hutter2009paramils}) where each evaluation of new candidate Solution on problem instance performed sequentially. Since the ROAR evaluation strategy is applied at step (3), we conclude that the utilization of, in principle, available parallel computation capabilities is another drawback of \textbf{SMACv3} framework.


\subsubsection{BOHB~\cite{falkner2018bohb}}\label{bg: bohb}
While SMAC outperforms and partially reuses decisions done in ParamILS, BOHB (Bayesian Optimization combined with HyperBand) is the parameter tuning tool that outperforms SMAC and was introduced by the same AutoML research group.

As it stated in name, the main routines here are carried out with mainly two algorithms: learning (1) and Configurations sampling (2) is done with Bayesian Optimization technique Tree Parzen Estimator (TPE) while Configurations evaluation and comparison carried out by HyperBand (HB) algorithm.

The TPE instead of na\"ive Gaussian Processes-based (GP BO) Bayesian Optimization was motivated by better dimensional scaling and internal support of both numerical and categorical data types (however, some minor transformations are still required). Unlike GP where optimization done by modeling distributions of Configuration results given it parameters, TPE builds two \textit{parameter distributions} splitting Configurations into two sets according to their `goodness'. For more detailed explanation we refer to original TPE description~\cite{bergstra2011algorithms}.

The other part of BOHB, namely HyperBand is a promising multi-armed bandit strategy for hyper-parameter optimization~\cite{li2017hyperband} in which the \textit{budget} for Configurations evaluation is defined beforehand and divided into iterations. The role of budget could play any control parameter that denotes the accuracy of Configuration evaluation by TS where the maximum budget gives you the most precise Configuration evaluation while minimum amount results in the least accurate approximation of result.
Running examples of budget could be the number of iterations in iterative algorithm or time to train the neuron network.
As the result, requirements arise for TS to expose and support budget usage as expected in algorithm.
At each iteration, HB randomly samples a number of Configurations for evaluation which, in fact, decreases for former iterations while the amount of budget remains the same. As the outcome, first iterations of HB are full of coarse-grain evaluated Configurations while later iterations produce higher number of more accurate measurements. At each iteration of HB, the Successful Halving (SH) procedure is executed to drop (usually $\frac{2}{3}$) badly performing Configurations. As one could expect, since the number of measuring Configurations in each iteration decreases, the measurements could be done more precisely since the amount of SHs execution drops too.

Binding of HyperBand and Bayesian Tree Parzen Estimator exists is several places. Firstly, the learning models are updated each time when new results are available for every budget value. Next, at each iteration of HB, TPE model is used for sampling new Configurations. Note that BOHB uses model built with results obtained with the largest budget only. This decision leads in more precise predictions in the later stages of parameter optimization procedure.

The drawback of this system lays in the way of handling conditions between hyper-parameters. BOHB actual implementation HpBandSter (a distributed HyperBand implementation on Steroids)~\footnote[1]{GitHub repository: \url{https://github.com/automl/HpBandSter}} as SMACv3 system uses ConfigSpace framework for Configuration Space definition. As we discussed in SMACv3 description (section~\ref{bg: smac}), it naturally allows to encode the dependencies and conditions among parameters within space. As authors stated, the TPE learning models accomplish this knowledge implicitly by shrinking the densities for forbidden parameters (actually those parameters are still added to models by \textit{imputation} mechanism where empty or default values are assigned). 
However, consider case of two Configurations $C_1$, $C_2$ appearance such that some parameter $P_i$ is forbidden in $C_1$, but not in $C_2$. The actual number of such parameters could vary dramatically in `sparse' Search Spaces those distributions estimated by KDEs will not reflect the reality. As a consequence, the prediction performed using such distributions will often result in `invalid' Configuration within `sparse' spaces hurting the performance and accuracy of sampling.

% TODO: it is more for concept description
% Even if fix of Configuration sampling in such cases is relatively simple task (parent-child relationship utilized in IRACE), then building proper density distributions of `good' parameters reflecting conditions is hard.


\subsubsection{Auto-sklearn~\cite{feurer2015efficient}}
From perspective of Optimization Problem solving, goals of automatic machine learning are quite similar to persecuted by Hyper-Heuristics. 
They both operate on Search Space of algorithms (or their building blocks) which later are combined and applied to problem in hand with objective to find the best performing one. In particular, based on Scikit-learn framework~\cite{scikit-learn}, Auto-sklearn system operates over number of classifiers, data and features preprocessing methods including their hyper-parameters to learn the dependencies among data and solve the classification task. %which in some sort is similar to usage of LLHs to process a Search Space.

This \textit{Combined Algorithm Selection and Hyper-parameter tuning problem} was formalized as \textit{CASH} and presented previously in Auto-WEKA~\cite{thornton2013auto} system.

The crucial decisions made in Auto-sklearn resulting into state-of-art performance and classification accuracies is the combination of off-line and on-line learning.

During the off-line phase, namely \textit{metal-learning}, researchers conducted meta-features for datasets published by OpenML community~\cite{OpenMLPython2019}. 
The entropy of results, data skewness, number of features and their classes is a sparse set of meta-features examples used to characterize a dataset (overall number is 38). After that, for each of available in meta-learning dataset, the search of best performing machine-learning pipeline was done using Bayesian Optimization technique implemented in discussed previously SMAC~\cite{hutter2011sequential} framework. The resulting combination of dataset, machine learning pipeline and its accuracy were stored and later used to seed the same Bayesian Optimization for on-line phase of pipeline search. 

During the on-line phase, the other crucial improvement was introduced. Usually, while searching the best-performing pipeline each new (except final) instance was built, trained, evaluated, results are stored, but the pipeline itself discarded. Here, the idea lays in storing previously instantiated and trained pipelines obtained while solving the CASH problem. Later they are used to form an ensemble of models and make final classification together. This mean, that the derivative of this architecture search is a set of models with different hyper-parameters and preprocessing techniques, starting from the worst performing ones (obtained at the beginning) and ending by the best suited for dataset in hand. 
Information from the meta-learning phase is used as follows: for a given new dataset they derive meta-features and select some portion of created during meta-learning pipelines that are the nearest in terms of meta-feature space. Then these pipelines were evaluated on a new dataset to form initial Configurations for Bayesian Optimization in SMAC.

However, the potential of this meta-learning approach is derived entirely from such dataset repository existence and depends on homogeneous datasets availability, while the strength of proposed on-line methodology is more universal and lays only on the applicability of underlying machine learning models and strength of optimization mechanisms.
The investigation of proposed approach universality would be rather intriguing, since Auto-sklearn was applied only to classification, but not a regression problems~\cite{feurer2015efficient,biedenkapp-ecai20}.




\subsubsection{BRISEv2~\cite{brise2spl}}
Among previously described systems we could distinguish ones that were created directly to face the parameter tuning problem and the others that are more generic optimizers but still applicable in such cases.
For concrete optimizer to be usable for searching the parameters, it should expose several features. First, it must consider optimization function evaluation to be expensive those tackle this problem explicitly, as instance we could suggest the usage of surrogate models or other Target System (TS) approximations. Next, potential tuner should be able to tackle dependencies and conditions among parameters. $\leftarrow$ it will go to the beginning of this subsection.


The great part of software potential lays not only in its ability to tackle a problem in hand, but also on the general usability and adaptivity to unforeseen tasks. 

Here we review BRISE major version 2 (more concretely, v2.2 available on September 2019\footnote[1]{GitHub: \url{github.com/dpukhkaiev/BRISE2/tree/10485473070bac01c0b3104e56e694e85fa816ad}}), since the very early BRISE versions (major 1~\cite{brise1monolite}) were more monolithic and those hard to apply for parameter tuning problem in hand.

While designing this system, authors are focused not only on particular learning mechanisms for parameter prediction, but overall system modularity.
Being a Software Product Line, BRISE was designed as a set of interchangeable interactive components (nodes) each acting according to its own specific role. 
Overall, system could be viewed from two perspectives. One is more general outline of all interacting nodes and the other is analysis of concretely \textit{main-node}.
Before reviewing each part, it is worth to justify a couple of used terms. \textit{Experiment} is a complete run of BRISE, for instance parameter tuning session. \textit{Configuration} $C$ is a combination of input Parameter values for Target System $TS$ (or algorithm) under evaluation. \textit{Search Space} is a set of all possible Configurations, while the \textit{Parameter} $P$ is a building block for Search Space and meta-description of particular Configuration part. \textit{Task} $T$ is an evaluation of $TS$ under provided $C$. Notably, the relationships here is as follows: Experiment contains one Search Space, which built-up from some Parameters. During the Experiment several Configurations appear, each compose set of Parameter values and after evaluation(s) could contain one (or several) Tasks.

From birds eye view perspective, BRISE consists of \textit{main-node} as the system backbone, several \textit{worker-nodes} as TS runners with given Configurations, \textit{worker-service} to distribute the load between main-node and set of worker-nodes, \textit{front-end-node} to represent and control optimization process on a web-page, and non-obligatory \textit{benchmark} node that but could be handy for executing and analyzing number of Experiments. 

To apply the system for problem in hand, one should (1) construct an Experiment and Search Space descriptions and (2) add definition of $TS$ evaluation in \textit{workers}, all the rest will be carried out by system.
% TODO: we deffinitely should formalize Tasks, since this entry is exposed to user (it should parse it to retrieve prameters as well as add results).

From our perspective, the advantage of BRISE over other systems comes from the design of \textit{main-node}. It is designed set of operating core entities (Experiment, Search Space and Configuration) with other non-core entities exposed to user for variability such as surrogate models, termination criteria, outliers detectors, repetition strategies, etc. Naturally, a number of implementations are provided out-of-box for all variable entities such as ridge regression with polynomial features and Tree Parzen Bayesian Optimization for models; wall-clock timeout, exceeding number of evaluated Configurations, an absence of improvement and others for termination criteria and so on. Describing different entities combinations in Experiment Description, one could obtain the system with desired behavior or even further, supply own learning model, sampling plan, termination logic, etc. 


As every other system, BRISE also has flaws. Talking about the Search Space representation, BRISE relies on ConfigSpace module~\cite{configspace} which is also used SMACv3 and BOHB frameworks, so all statements claimed previously are also valid here. The predictive models are acting pretty the same here those it is hard to apply BRISE in cases of sparse Configurations spaces.


\subsection{Parameter control}\label{bg: parameter control}
As we defined before, the parameter control is an approach of \textit{on-line} settings the parameters, built into the algorithm to react on changes in solving process on-the-fly.

The biggest disadvantage of Parameter Tuning defined by the fact, that it usually requires large number of algorithm runs (evaluations) to evaluate performance with different settings.
On opposite site, the parameter control approaches are solving this issue but with the drawback of non-universality. Meaning, the proper control strategies for one algorithm usually is not applicable for other.

Searching the examples of parameter tuning for heuristics, one definitely will find himself exhausted to enumerate all methodologies proposed for evolutionary algorithms since this idea have come from EAs~\cite{karafotias2014parameter}, however techniques for other meta-heuristics do exist too.
Since the idea of adapting parameters exclusively dependent onto concrete algorithm instance, it is hard to generally classify approaches for all algorithms somehow, but for each particular one it could be done.

The only one generic classification facet is the type of controlling mechanism, where \textit{deterministic} (following a predefined schedule), \textit{adaptive} (assign parameter values upon received learning feedback) types are distinguishable.

We provide an insight of parameter control by reviewing some examples among numbers of existing adaptation strategies in meta-heuristics.
Authors reported significant optimization performance improvements in each of mentioned later researches, but for more comprehensive review of recently published parameter control strategies we encourage reader to examine~\cite{huang2019survey}.

\subsubsection{Parameter Control in Evolutionary Algorithms} 
The motivation for huge number of performed studies tackling adaptive changes in Evolutionary Algorithms is that the performance of later greatly depends not only on (1) parameter values used in EAs which is usually found by tuning approaches but also on (2) the fact, that the best parameter values may change during the run of EAs. 
Shortly, there exists strategies for controlling each EA parameter separately or together.  Most commonly implicated parameters are: \textit{population size}, \textit{selection strategies} and \textit{variation aspects} (namely crossover and mutation operators). For more detailed explanation and analysis of each approach we suggest reader to refer recently conducted reviews and researches if field of parameter control in Evolutionary Algorithms~\cite{karafotias2014parameter,aleti2016systematic,smith2020self}.
The only common distinction here that listed previously mechanisms are extended by 3rd control type — \textit{self-adaptive} which implies encoding of mentioned parameter values in solution genomes those allowing them to co-evolve with solutions in the runtime. 

\subsubsection{Parameter Control in Simulated Annealing}\label{bg: parameter control: SA}
In Simulated Annealing, however, one of the most frequently controlled parameters are the \textit{cooling schedule} (the velocity of decreasing temperature) and the \textit{acceptance criteria}.

The control in cooling rate is motivated by the fact, that if the temperature decreasing too rapidly, an optimization process may stuck in local optima, while too slow cooling rate is usually computationally costly. Many researches were made to investigate which is the best \textit{deterministic} strategy among linear, exponential, proportional, non-linear, logarithmic or geometrical. In contrast, \cite{karabin2020simulated} propose the adaptive strategy based on statistical information evaluated on each optimization step.

The other example of adapting the acceptance criteria was proposed in~\cite{ghandeshtani2019entropy}, based on thermodynamics fundamentals such as \textit{entropy} and \textit{kinetic energy}. Authors suggest replacing the standard acceptance criteria (based on current temperature and found solution quality) with the criteria based on change in solutions entropy.

For other approaches of parameter control we suggest to check~\cite{ingber2000adaptive,de2003placement}.

\section{Beyond Parameters Settings} 

CASH, Architecture Search, Hyper-Heuristics with parameter control (do they exist?)


Field of automated machine learning is one of trending research directions those there exist many open-source (\textit{Auto-Weka}~\cite{thornton2013auto}, \textit{Hyperopt-Sklearn}~\cite{komer2014hyperopt}, \textit{Auto-Sklearn}~\cite{feurer2015efficient}, \textit{TPOT}~\cite{olson2019tpot}, \textit{Auto-Keras}~\cite{jin2019auto}, etc.) as well as commercial (\textit{RappidMiner.com}, \textit{DataRobot.com}, Microsoft’s \textit{Azure Machine Learning}, Google’s \textit{Cloud AutoML}, and Amazon's \textit{Machine Learning on AWS}) auto-ml systems.







\section{Conclusion}

%TODO: need to add a review of available techniques for making a predictions within conditional search spaces
% https://www.etsmtl.ca/Unites-de-recherche/LIVIA/Recherche-et-innovation/Publications/Publications-2017/Levesque_ijcnn_2017.pdf
% https://arxiv.org/pdf/1909.13404.pdf - omg, 30.09.2019 -_-, smth really similar to proposed here approach..

% remind a problem we want to solve, chosen solver, it's problems, solutions (selecting, tuning) weaknesses of approach to tackle later problem and propose own clearly outlining what we want to improve in later to tacke former.

It could be compared with a warp-engine usage on a spacecraft, which gives the possibility to travel faster than speed of light by orders of magnitude. It is definitely fast enough, however if you consider a space-folding engine for instant traveling and use it to jump onto a huge distance, while warp-engine is 


The meta-heuristic systems designers reported positive impact of parameter control embedding. 
However, as the outcome of the no-free-lunch theorem, those systems can not tolerate broad range of problems, for instance, problem classes.
In other hand, hyper-heuristics are designed with an aim to select the low level heuristics and those propose a possible solution of problem, stated in no-free-lunch theorem, but the lack of parameter control could dramatically decrease the performance of LLH (probably, I need to find a prove of this, or rephrase).

\paragraph{Scope of thesis defined.} In this thesis we try to achieve the best of both worlds applying the best fitting LLH and tuning it's parameters while solving the problem on-line.