\chapter{Concept Description}

% TODO: find out what are the other approaches for generic parameter control. by current time I am not able to findout anything generic, except proposed approaches for EAs. but they are for EAs..

Since there exist no universal approach to control the algorithms parameters (Subsection~\ref{bg: parameter control}), our conclusion on the literature analysis was the absence of existing approaches to combine the on-line algorithm selection and the parameter control techniques (Section~\ref{bg: conclusion}). In this Chapter we suggest our methodology to resolve this problem, excluding the implementation details.

In Section~\ref{concept:parameter control}, we suggest the generic parameter control technique and expand the use-case of our solution with algorithm selection. As concluded in the Section~\ref{bg: conclusion}, the main weakness of the reviewed approaches to tackle CASH problems lays in the inability of learning mechanisms to fit and predict in such `sparse' search spaces. The same issue arises in our case, and we resolve it on two levels: (1) in the search space structure and (2) in the prediction process. Firstly, in Section~\ref{concept:search space} we present the joint search space of both algorithm selection and parameter control problems. We outline the functional requirements for such space, followed by the methodology to provide them. Next, we describe the prediction process in Section~\ref{concept:prediction}. Here we highlight an importance of decoupling the learning models from the search space structure. In this way we provide the certain level of flexibility in different learning models usage.

% TODO: check if needed in this chapter
Finally, in Section~\ref{concept: llh} we pay attention onto the low level heuristics (LLH) — a working horses of the hyper-heuristic. Here we highlight the requirements to LLH that are crucial in our case.


\section{Combined Parameter Control and Algorithm Selection Problem}\label{concept:parameter control}
The base idea of the parameter control approaches lays in adapting the solver behavior in the runtime as the response to changes in the solving process (Subsection~\ref{bg: parameter control}). As we mentioned during the heuristics review, the algorithm performance is highly dependent on the provided exploration-exploitation balance (Section~\ref{bg: section heuristics}) which in turn, depends on (1) the algorithm itself and (2) its configuration. The task of parameter control is to optimize the later for gaining the best performance. 

In our work, we tend solve the parameter control problem using the \emph{Reinforcement Learning} (RL) approaches (what is similar to used approaches in EAs~\cite{karafotias2014generic}). % TODO: put it into BG and refer?
The underlying idea of RL could be described as a process of performing actions in some environment with order to maximize the reward obtained after each performed action. To apply this technique onto the parameter control problem, we define what are those \emph{actions} and how to estimate the \emph{reward}. 

Thus, for making the parameter control applicable to broad range of algorithms, we analyze not the solver state itself, but the solution process (in contrast to EAs, where population diversity metrics, etc. are analyzed). To do it, we interrupt~$I$ the solver, analyze~$A$ the intermediate results, set~$S$ the most promising parameters and continue~$C$ solving. The number $i$ of $I \rightarrow A \rightarrow S \rightarrow C$ iterations define the granularity of learning, where one should carefully balance between \emph{time to control} (TTC) the parameters vs \emph{time to solve} (TTS) the problem. Naturally, the limitation of proposed approach is the use-cases, where $TTS >> TTC$.

\todoy{Should I highlight the limitation(s) here or in conclusion and refer from here?}

To evaluate the gained in iteration $i$ reward, instead of using straight solution quality value, we calculate the quality improvement, obtained with the provided configuration $C_i$. Naturally, when the search process converges towards the global optimum, the improvement value tends to decrease, since the amount of significantly better solutions drops. Using the improvement values directly or could confuse the learning models and thus, cause the RL to struggle. To resolve this trouble, the relative improvement (RI) of solution quality is calculated using Formula~\ref{concept: RI formula}, where $S_{i-1}$ and $S_i$ are the solution qualities before and after $i^{th})$ iteration respectively.

The evaluated $C_i \rightarrow RI$ pairs in previous iterations are then used to predict the configuration for next iteration $C_{i+1}$. At this point, we split the sampling process in two steps: (1) hide the search space shape and (2) use the surrogate models for finding configurations that lead to the highest reward.

\todoy{I did not investigate decoupling the surrogate models from the search algorithm to optimize those surrogates (done in Sasha's thesis). Should I mention it somehow, or just postpone and raise the discussion in future work?}


\begin{equation}
RI = \frac{S_{i-1} - S_{i}}{S_{i-1}}
\label{concept: RI formula}
\end{equation}

After obtaining the $C_{i+1}$ configuration, we set it as the solver parameters. To proceed with the solving process, we seed the solver with the solutions from $i-1$ iteration as well.

When it comes to the algorithm selection problem (discussed in the Subsection~\ref{bg: hh}), it turns out that the proposed reinforcement learning approach is also applicable here. We treat the solver type itself as the subject of parameter control and use the proposed RL approach to estimate and use the best performing algorithm, while solving the problem. However, when we add the algorithm type parameter, the resulting search space of become `sparse' and requires special treatment. Two common approaches for tacking such a problem exist. The first requires special kinds of learning-prediction models usage, while the second suggests transforming the problem in a way of excluding undesired characteristics.

During the review of model-based parameter tuning approaches (Section~\ref{bg: parameter tuning}), we concluded that all broadly used system follows strictly the first approach. For instance, as the surrogate models, SMAC~\cite{hutter2011sequential} uses the random forest, BOHB~\cite{falkner2018bohb} and BRISE~\cite{brise2spl} — Bayesian probability models. While those surrogates could naturally fit to the described search space shape, none among proposed approaches is able to make the predictions effectively since the most of predicted configurations will violate the dependencies. As an instance, imagine after $i^{th}$ iteration, the surrogate models learn about two superior parameters: one indicates a well-performing heuristic type (the Genetic Algorithm), the other — an effective configuration for another algorithm type (an exponential cooling rate for the Simulated Annealing). In such a case, the reviewed systems sampling methods will tend to predict the configurations with those two parameter values, which turns to be invalid.

Here we follow the second approach namely, the transformation of the problem in order to sample the valid configurations only. The following section depicts a required preparation step, made in the search space, while the later is dedicated to the prediction process.


\section{Search Space}\label{concept:search space}
When the time comes to selecting not only the solver parameters, but also the solver itself, the united search space no longer could be presented as `flat' set of parameters since it tends to appearance vast amount of invalid parameter combinations.

Let us estimate the number of all possible configurations vs the amount of meaningful ones in rather simple example.
Suppose, we have $N_s$ solver types, each exposing the $N_{s,p}$ number of hyper-parameters with $N_{s,p,v}$ possible values. The aggregated quantity of configurations $N_c$ in the disjoint search spaces is calculated as the number of possible combinations using Formula~\ref{c: disjoint search space size}.

\begin{equation}
N_c = N_s \cdot \prod_{1}^{N_{s,p}} N_{s,p,v}
\label{c: disjoint search space size}
\end{equation}

However, if we decide to tune (or rather to control) the solver type itself, the resulting quantity of possible configurations is calculated using Formula~\ref{c: joint search space size}.

\begin{equation}
N_c = \prod_{1}^{N_{s}} \prod_{1}^{N_{s,p}} N_{s,p,v}
\label{c: joint search space size}
\end{equation}

For better intuition, lets try some numbers. By setting all $N_s = N_{s,p} = N_{s,p,v} = 3$ (the rather small example), the amount of configurations estimated separately for each solver equals to $N_c = 81$ (Formula~\ref{c: disjoint search space size}). However, if we join the solver parameter spaces, Formula~\ref{c: joint search space size} shows the significant growth in the search space size: $N_c = 19683$. Note, the number of different configurations remains the same thus, in the joint space it is only $\approx 0.4\%$. When setting $N_s = N_{s,p} = N_{s,p,v} = 4$, this number drops to $\approx 9 \cdot 10^{-8}\%$. It could decrease even further if the dependencies among $p$ exist. In such case, the predictive abilities of the parameter control approaches may straggle.

To overcome this, we utilize similar to the proposed in IRACE~\cite{lopez2016irace} framework idea: \emph{explicitly indicate the dependencies as a parent-child relationship among the search space entities $p$, firstly predict the parent parameter, afterwards — the children.} This give us an opportunity to threat the algorithm type as the regular categorical parameter and simplifies the structure of search space and makes the prediction process uniform.

This decision raises the following search space \emph{structural requirements}:
\begin{enumerate}[itemsep=8pt]
	\item[S.R.1] \textbf{Parent-child relationship} describe the dependencies between different parameter types. For instance, appearance of one requires another, but eliminates the other.

	\item[S.R.2] \textbf{Uniform parameter types} simplifies the structure and hides the domain-specific intent of each parameter thus, algorithm type and its configuration are treated in the same way.

	\item[S.R.3] \textbf{Value-specific dependencies} describe a concrete parent value(s), when the child (children) should be exposed. For instance, the entity \textit{algorithm type} has a number of possible values, each of them requires own set of hyper-parameters, which should remain hidden for the other solver types.
\end{enumerate}

\svgpath{{graphics/Concept/}}
\begin{figure}
	\centering
	\includesvg[width=1.0\textwidth]{feature tree}
	\caption{Search space representation.}
	\label{concept:pict:Search Space Representation}
\end{figure}

Figure~\ref{concept:pict:Search Space Representation} shows an example of such the search space with $s$ algorithm types, each having $p$ parameters with $v$ possible values. The entities with triangles $\bigtriangledown$, namely the parameter concrete values, form the joint-points to which the other parameters could be linked. 

\todoy{Should I mention that it is similar to SPL Feature Models? Only single sentence comes in my mind, but I am not sure if it is needed here: "This structure is similar to \emph{Feature Models}, used in Software Product Lines~\cite{bibid}."}

\section{Prediction Process}\label{concept:prediction}

After formalizing the structural requirements, let us switch to the prediction process and define the search space \emph{functional requirements}, which should be fulfilled to decouple the learning models from the complex search space shape.

The idea of this decoupling lays in resolving the value-specific dependencies among the parameters in a step-wise prediction process. To do so, we firstly predict the parent value, which in case of hyper-heuristic is the type of low-level heuristic (Level 0 on Figure~\ref{concept:pict:Level-wise prediction process}). Afterwards, the search space must expose the child parameters of this solver only, ignoring the others (Level 1 on Figure~\ref{concept:pict:Level-wise prediction process}). The dependencies among exposed parameters, are handled in the same way (Level 2 and further on Figure~\ref{concept:pict:Level-wise prediction process}).

While building the surrogates and making the prediction on each level, we need to ignore the data on the other levels. Making the prediction on parent level, we do not change it on levels below thus, we do not need to operate useless information. If the backward ignorance is painless, the forward omission puts the restriction on the surrogate models. Cutting off the deeper level parameter values, we may get the data points with the same parameters values, but different results. Thus, on this level(s) only those models should be used, which will not be confused. During the implementation description we will clarify, which models are the better choice in such cases and implement one of the promising.

Certainly, during the problem solving, the quality trends among parameter values change. For instance, at later stages the domination of one solver could be declined in comparison to other. Or, the previously good-performing parameter values are not that good anymore. It may be caused by the various reasons, but the old-trends should be definitely left out introducing some forgetting mechanism.

At this point, let us shortly clarify the resulting functional requirements, derived from the previous discussions:
\begin{enumerate}[itemsep=8pt]
	\item[F.R.1]
	
	\item[F.R.2]
	
	\item[F.R.3]
\end{enumerate}

\begin{figure}
	\centering
	\includesvg[width=1.0\textwidth]{feature tree pred}
	\caption{Level-wise prediction process.}
	\label{concept:pict:Level-wise prediction process}
\end{figure}

\paragraph{Importance explanation}
\paragraph{Requirements} generality, top-down approach of optimization
-- different views of same Configuration (level-dependent) - filtering, transformation
-- consider problem features? while selecting meta-heuristic \cite{kerschke2019automated} page 6
-- learning metrics (relative improvement), we postpone adding other metrics in future work.
IRACE\cite{lopez2016irace}

\section{Hyper-Heuristic}

\section{Low Level Heuristics}\label{concept: llh}
\paragraph{Importance explanation}
\paragraph{Requirements}


\section{Conclusion of concept}
to be done...