\chapter{Evaluation}\label{eval}
The concepts of search space representation and prediction processes were proposed in \cref{Concept description}. Based on them, in \cref{impl} we implemented a generalized parameter control for meta-heuristics, selection hyper-heuristic and the hyper-heuristic with parameter control approaches. In this Chapter we propose a broad evaluation of the implemented approaches. Experiments may be performed in a number of investigation directions, starting from the developed RL performance comparison to the baseline and ending with the scalability to different problem sizes.

We start this Chapter with brief presentation of the optimization problem at hand in \cref{eval: op} and short environment description in \cref{eval: environment}. We perform a parameter tuning of low-level heuristics in \cref{eval: mh tuning}, which will be used later through our tests for comparison. 

The evaluation presented in this thesis could be divided into two main parts.

The first part (\cref{eval:1}) is dedicated to the developed concept analysis in comparison to the baseline. We start the concept evaluation with planning in \cref{eval:1:plan} and proceed firstly reviewing the baseline in \cref{eval:1:baseline}, secondly the generic parameter control is presented in \cref{eval:1:PC}, followed by the selection hyper-heuristic with static hyper-parameters in low-level heuristics review in \cref{eval:1:hh-sp} and ending with the selection hyper-heuristic with parameter control in low-level heuristics review presented in \cref{eval:1:hh-pc}.

In the second part (\cref{eval:2}), we investigate an influence of hyper-heuristic with parameter control settings on its performance. In order to do so, once again we firstly perform the experiment planning in \cref{eval:2:plan}. Afterwards, in \cref{eval:2:learning granularity} we investigate an influence of a learning granularity on the HH-PC performance. In \cref{eval:2:learning models} we check learning models configurations and in \cref{eval:2:llh changes} we verify the influence of inter-LLH communication configuration.

Finally, \cref{eval: conclution} concludes our evaluation with a discussion of the obtained results.


\section{Optimization Problem}\label{eval: op}
In this thesis we are tackling a vehicle routing problem --- the traveling salesman OP. We present its short definition here, including the related to benchmark details, however, the detailed explanation could be found in \cref{BG: subsection OPs}: ``Given a set of cities and the distances among them, find the shortest path, which visits all cities''. It is a combinatorial OP with a number $n = N!$ of possible solutions. In our benchmarks we use several instances of symmetric TSP (distances $x_i \rightarrow x_j$ and $x_j \rightarrow x_i$ are equal) from a publicly available and broadly used in research TSP benchmark set TSPLIB95\footnote{TSPLIB95 website:~\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/}}. The advantage of choosing this benchmark set lays in a broad compatibility of solvers and frameworks (including jMetal and jMetalPy) with this standardized TSP instance description. The TSP in this case is defined as a set of city coordinates. Therefore, before starting to solve a problem, the distance matrix is usually built by heuristic, calculating the Euclidean distances between each pair of cities. For more detailed explanation of problem instance description, the proposed in TSPLIB95 please refer to~\cite{reinelt1995tsplib95}.

For our benchmarks we select four problem instances: \emph{kroA100}, \emph{pr439}, \emph{rat783} and \emph{pla7397} of sizes 100, 439, 783 and 7397 cities respectively. The optimal tours for each of these problem instances were previously obtained by the exact solvers and reported in aforementioned library. We present the optimal solutions in \cref{eval:table:tsp optimal tour length}.

\begin{table}[h!]
	\centering
	\begin{tabular}{cc}
		\rowcolor{gray!10}
		\hline
		\textbf{TSP instance} & \textbf{Optimal tour length} \\
		\hline
		kroA100 & 21282 \\
		pr439 & 107217 \\
		rat783 & 8806 \\
		pla7397 & 23260728 \\
		\hline
	\end{tabular}
	\caption{TSP instances optimal tour length.}
	\label{eval:table:tsp optimal tour length}
\end{table}


\section{Environment Setup}\label{eval: environment}
We run our experiments with an enhanced by our approach BRISEv2 and deploy it in Docker containers on host machine with the following characteristics:
\begin{itemize}
	\item \textbf{Hardware:} Fujitsu ESPRIMO P958 computer with 64GB 2667MHz RAM (16GB * 4 pcs), Intel Core i7-8700 CPU @ 3.2 GHz (6 cores * 2 threads) and Samsung 1TB SSD.
	
	\item \textbf{Software:} GNU/Linux Fedora 29 host OS and installed Docker engine version 1.13.1.
\end{itemize}

We deploy 6 homogeneous BRISEv2 workers with LLHs on the same host machine and run each experiment 9 times to gather the statistics. Each execution was performed with wall-clock-based termination criterion configured to shut down the optimization session after 15 minutes.


\section{Meta-heuristics Tuning}\label{eval: mh tuning}
As we conclude in \cref{bg: parameter setting conclution}, the goal of parameter control is at least to reach the parameter tuning approaches results quality. Therefore, before running the major set of evaluation experiments, we have to perform a parameter tuning for the underlying LLHs.

\subsection{Parameter Tuning System Configuration.} 
As a tuning system, we use our concept implementation in the tuner mode. As mentioned in \cref{concept: conclution}, to enable the parameter tuning mode we built a search space based on a singe LLH with its parameters and disable the solution transfer between configurations. In our particular case we define three search spaces for each underlying meta-heuristic. We run the tuning for 8 hours on 10 deployed worker nodes and give three minutes for each task (configuration) evaluation. The underlying prediction mechanism was configured to use TPE with 100\% window size. We disable the repetition strategy and outliers detection leaving each configuration evaluated only once, since our preliminary experiments showed that the variance among evaluations is negligible.


\subsection{Target Optimization Problem and Search Space of Parameters.} 
The role of optimization problem at hand is played by one of the selected TSP instances: \emph{rat783}. We base the algorithms tuning on this instance, since it is a middle-size problem among the instances selected for evaluation.

\paragraph{jMetalPy evolution strategy.} This meta-heuristic is implemented as a naive evolution strategy, however, we found an important recombination mechanism missing, therefore, the heuristic is acting mostly by means of mutation operations. As a configuration, it requires several hyper-parameters of different types. Integer parameter $\mu$ (\emph{mu}), which denotes a number of parents in the population. Integer parameter $\lambda$ (\emph{lambda}) defines a number of offspring. We tune both parameters in range $[1..1000]$. Boolean parameter \emph{elitist} defines a selection strategy, where \emph{true} value enables elitist selection $(\mu+\lambda)$, while \emph{false} value disables it $(\mu,\lambda)$ (more detains in \cref{BG: MH Examples}). Also, the framework proposes two possible \emph{mutation types} for combinatorial OPs: \emph{permutation swap} and \emph{scramble mutation}. The mutation probability is tuned in range $[0..1]$ respectively.

\paragraph{jMetalPy simulated annealing.} In this meta-heuristic authors defined the solution neighborhood by means of the same aforementioned mutation operators. Thus, we use them and similar mutation probability range for tuning the SA. Unfortunately, the authors did not provide other, but exponential cooling schedule and did not expose \emph{temperature} or \emph{alpha} (cooling rate) parameters. This is the reason of such tiny parameter space for this MH.

\paragraph{jMetal evolution strategy.} In this meta-heuristic a set of exposed hyper-parameters is almost the same, as for previously described Python-based evolution strategy implementation. The only difference is that the mutation type is fixed, therefore, we exclude it from the parameter space, but leaving the mutation probability and elitist parameters. All the other parameter ranges are the same as for the defined above ES.


\subsection{Parameter Tuning Results.} 
The process of parameter tuning is depicted in \cref{eval:pict:mh tuning}. 8 hours of tuning on 10 workers with 3 minutes for each configuration resulted in at least ~1500 evaluated parameter combinations. 

\svgpath{{graphics/Eval/tuning}}
\begin{figure}[h!]
	\centering
	\includesvg[width=\textwidth]{tuning progress}
	\caption{The low level heuristics parameter tuning process on rat783 TSP instance.}
	\label{eval:pict:mh tuning}
\end{figure}

In the figures below, we propose a visual analysis of the results. We separately present the numerical and categorical parameters for each meta-heuristic.

The numeric hyper-parameters are presented as scattered points of parameter value (\emph{x-axis}) and the respective objective function result (\emph{y-axis}), obtained for configuration with this parameter value. Although, such an isolated approach to analyze data may be error-prone. But still, it is enough to get a birds-eye view on the existing dependencies. To represent trends among the numeric parameter values we draw the regression line ($4^{th}$ degree) in green. At the top and to the right side of the plot value densities are presented. From the top density one can derive, which parameter values were sampled more often, while densities on the right side show, which objective values and how often were obtained. 

As for the categorical parameters, we plot their values in violin plots. It is a combination of box plot and kernel density plot. Since all categorical parameters of underlying algorithms have only two values, each site of violin plot shows, which objective results and how often was obtained for the respective category. We also distinguish values by different colors, while the violin shape shows an expected probability of the respective result value. Inside the figure we also draw three dashed lines. A middle line with long dashes is a median, while lower and upper lines with short dashes show first and third distribution quartiles respectively.


\paragraph{jMetalPy evolution strategy parameters.}
\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy numeric parameters}
	\caption{jMetalPy evolution strategy numeric hyper-parameters tuning.}
	\label{eval:pict:jmetalpy es numeric}
	\vspace{-15pt}
\end{figure}

\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy categorical parameters}
	\caption{jMetalPy evolution strategy categorical parameters tuning.}
	\label{eval:pict:jmetalpy es categoric}
	\vspace{-20pt}
\end{figure}

In \cref{eval:pict:jmetalpy es numeric} one may clearly see an explicit dependency between the number of parents (parameter \emph{mu}) and the objective function: by setting lower number of parents, better results are obtained more often. However, the offspring number dependency is not that clear (parameter \emph{lambda}). We may see that a high offspring number does not tend to provide good results, but the number of performed estimations for low \emph{lambda} is not enough to be strongly ensured that this value is better. Yet, even with a small amount of observations we may guess that a low \emph{lambda} is a good parameter value choice. With respect to the mutation probability, it may be observed that higher rates tend to produce better results. 

As for the categorical parameters presented in \cref{eval:pict:jmetalpy es categoric}, one may see a strong bias towards good results appears when using elitist algorithm version. Concerning the mutation type, the dominance is not obvious, but permutation mutation slightly outperforms scramble type.


\paragraph{jMetalPy simulated annealing parameters.}
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{jMetalPy simulated annealing numeric parameters}
		\caption{Mutation probability.}
		\label{eval:pict:jmetalpy sa numeric}
	\end{subfigure}
	\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{0.4\textwidth}
		\includesvg[width=\textwidth]{jMetalPy simulated annealing categorical parameters}
		\vspace{-5pt}
		\caption{Mutation type.}
		\label{eval:pict:jmetalpy sa categoric}
	\end{subfigure}
	\caption{jMetalPy simulated annealing parameters tuning.}
\end{figure}

In this meta-heuristic only two parameters were tuned: categorical mutation type, the results of which are presented in \cref{eval:pict:jmetalpy sa categoric} and numerical mutation probability depicted in \cref{eval:pict:jmetalpy sa numeric}. One may see a strong dominance of permutation mutation type, while scramble produce average, but rather stable results. The mutation probability trends are also clear: higher parameter values produce better results. Indeed, the dependency on the mutation probability is obvious, since the underlying algorithm is performing the search space traversal by means of solution mutation. The two clear lines of results, which can be viewed in \cref{eval:pict:jmetalpy sa numeric} are correlated with the mutation types: lower corresponds to permutation mutation type usage, while upper is the result scramble mutation usage.


\paragraph{jMetal evolution strategy parameters.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{jMetal evolution strategy numeric parameters}
	\caption{jMetal evolution strategy numeric parameters tuning.}
	\vspace{-15pt}
	\label{eval:pict:jmetal es numeric}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{jMetal evolution strategy categorical parameters}
	\label{eval:pict:jmetal es categoric}
	\caption{jMetal ES elitist parameter tuning.}
	\vspace{-30pt}
\end{wrapfigure}

The final heuristic under investigation is a Java-based implementation of evolution strategy. At the first glance, the regression lines are not looking the same as for jMetalPy.ES, but the overall trends are similar: lower values of \emph{mu} parameter result in better objective, while the mutation probability should be high. In contrast to jMetalPy.ES, here the middle-range values of \emph{lambda} parameter tend to produce the best results. The difference may be explained by the performance degradation of Python-based algorithm with large offspring number: the required computational effort for accomplishing the iteration increases, while Java-based version could handle a larger number of offspring. A dominance of elitist algorithm version is not obvious, but comparing the first quartiles positions, one will agree that elitist ES version is better.

\paragraph{}
We collect the best performing configurations of each meta-heuristic in \cref{eval: params jmetalpy es}. We also highlight here the default values for each parameter, which were selected with the motivation of being in the values ranges middle.

\begin{table}%[h!]
	\centering
	\begin{tabular}{rccc}
		\hline
		\textbf{Hyper-parameter} & \textbf{Default value} & \textbf{Tuned value} & \textbf{Parameter values range} \\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{\emph{jMetalPy evolution strategy}} \\
		%\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 22 & $[1..1000]$ \\
		elitist & False & True & \{True, False\} \\
		
		mutation type & Permutation & Permutation & \{Permutation, Scramble\} \\
		mutation probability & 0.5 & 0.99 & $[0..1]$\\
		%\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{\emph{jMetalPy simulated annealing}} \\
		%\hline
		mutation type & Permutation & Permutation & \{Permutation, Scramble\} \\
		mutation probability & 0.5 & 0.89  & $[0..1]$\\
		%\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{\emph{jMetal evolution strategy}} \\
		%\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 605 & $[1..1000]$ \\
		elitist & False & True  & \{True, False\}\\
		mutation probability & 0.5 & 0.99 & $[0..1]$ \\
		\hline
	\end{tabular}
	
	\caption{Static hyper-parameters of low-level meta-heuristics.}
	\label{eval: params jmetalpy es}
\end{table}


\section{Concept Evaluation}\label{eval:1}

\subsection{Evaluation Plan}\label{eval:1:plan}
To evaluate the developed approach performance, we firstly need to define the baseline. In most cases it is an isolated meta-heuristics, which are solving the OP using static hyper-parameters. However, to evaluate the applied generic parameter control to meta-heuristic, we must take a closer look on the respective meta-heuristic results with tuned in offline and controlled in online hyper-parameters. For the selection hyper-heuristic analysis, we compare the performances of all underlying MHs running separately and united together within our hyper-heuristic. Please note, in this case the hyper-parameters are static. To evaluate the selection hyper-heuristic with parameter control in low-level heuristics, we compare it to separately running underlying meta-heuristics with tuned parameters. In order to organize the evaluation plan, we distinguish two stages of configuration construction. At the first stage, the LLH selection occurs, while at the second stage, our system selects the respective hyper-parameters. At either stage we may use different prediction approaches. 

To select the LLH, apart from random and static selection we also use FRAMAB and Bayesian ridge regression model (BRR) from Scikit-learn framework (see \cref{impl: sklearn wrapper}). Please note, for the Bayesian ridge regression we use default parameters, which could be found in the framework documentation\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html}{https://scikit-learn.org}}. The accuracy threshold for BRR construction equals to 0.5 (see details in \cref{impl: sklearn wrapper}). We set a single FRAMAB parameter \emph{C} to be equal \emph{STD} (see details in \cref{impl: FRAMAB}). 

To select the LLHs hyper-parameters, apart from static default and tuned values we also use random selection, a previously available in BRISEv2 TPE with default values (split size equals to 30\%) and the mentioned above BRR. To perform a random search-based surrogate optimization, we sample 96 (default BRISEv2 configuration) parameter values combinations for each search space level.

The set of used sampling techniques is presented in \cref{eval: concept settings table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{ll}
		\hline
		\rowcolor{gray!10}
		\textbf{LLH selection} & \textbf{LLH parameters selection} \\
		\hline
		\textbf{1.} Random & \textbf{1.} Default \\
		\textbf{2.} Multi-armed bandit & \textbf{2.} Tuned beforehand \\
		\textbf{3.} Bayesian ridge regression & \textbf{3.} Random \\
		\textbf{4.1.} Static jMetalPy.ES & \textbf{4.} Tree Parzen Estimator (TPE) \\
		\textbf{4.2.} Static jMetalPy.SA & \textbf{5.} Bayesian ridge regression (BRR) \\
		\textbf{4.3.} Static jMetal.ES & \\
		\hline
	\end{tabular}
	
	\caption{Sampling techniques used for the concept evaluation.}
	\label{eval: concept settings table}
\end{table}


With this table we pick a set of sampling techniques to form a desired system configuration, which leads to different operation modes. For instance, baseline mentioned above could be formed by code \emph{4.1.1} for jMetalPy evolution strategy, used with the default hyper-parameters, or by code \emph{4.3.2} for jMetal evolution strategy, used with the hyper-parameters tuned in offline.

Our benchmark plan for the concept evaluation looks as a set of following experiment groups:
\begin{itemize}
	\item \textbf{Baseline (MH).} We evaluate each meta-heuristic separately with the default and tuned hyper-parameters: \emph{4.1.1} and \emph{4.1.2} for jMetalPy evolution strategy;  \emph{4.2.1} and \emph{4.2.2} for jMetalPy simulated annealing; \emph{4.3.1} and \emph{4.3.2} for jMetal evolution strategy respectively.

	\item \textbf{Meta-heuristics with parameter control (MH-PC).} This set of experiments is dedicated to verify an impact of the generic parameter control on meta-heuristic's performance and includes experiments: \emph{4.1.3, 4.2.3, 4.3.3} to investigate the influence of random parameter allocation; \emph{4.1.4, 4.2.4, 4.3.4} to verify TPE-based parameter control and \emph{4.1.5, 4.2.5, 4.3.5} to probe BRR-based parameter control.

	\item \textbf{Selection hyper-heuristic with static parameters in LLH (HH-SP).} These benchmarks are dedicated to the implemented online selection HH performance investigation. It implies the LLHs usage with static parameters, therefore, we evaluate HH-SP performance with the default and tuned beforehand LLH parameters. The experiment codes are following: \emph{2.1, 2.2} for FRAMAB-based HH-SP and \emph{3.1, 3.2} for BRR-based HH-SP with default and tuned parameters respectively.
	
	\item \textbf{Selection hyper-heuristic with parameter control in LLH (HH-PC).} It is a final set of benchmarks for concept evaluation. Here we evaluate an influence of simultaneous online LLH selection and parameter control on optimization results. The benchmark set is encoded with the following experiments: \emph{1.3, 2.4, 2.5, 3.4, 3.5.}
\end{itemize}

The aggregated concept benchmark plan is presented in \cref{eval: concept benchmark plan table}. The required running time approximately equals to 9 days and 18 hours on a single machine. Please note, the environment setup is presented in \cref{eval: environment}.

\begin{table}[h!]
	\centering
	\begin{tabular}{cp{3cm}}
		\hline
		\rowcolor{gray!10}
		\textbf{Experiment group} & \textbf{Related codes} \\
		\hline
		
		\multirow{2}{*}{MH} & 4.1.1., 4.2.1, 4.3.1 \newline 4.1.2, 4.2.2, 4.3.2 \\
		
		\rowcolor{gray!10}
		\multirow{3}{*}{MH-PC} & 4.1.3, 4.2.3, 4.3.3 \newline 4.1.4, 4.2.4, 4.3.4 \newline 4.1.5, 4.2.5, 4.3.5 \\
		
		\multirow{3}{*}{HH-SP} & 1.1, 1.2 \newline 2.1, 2.2 \newline 3.1, 3.2 \\

		\rowcolor{gray!10}
		\multirow{3}{*}{HH-PC} &  1.3 \newline 2.4, 2.5 \newline 3.4, 3.5 \\
		\hline
	\end{tabular}
	
	\caption{Concept benchmark plan.}
	\label{eval: concept benchmark plan table}
\end{table}


\subsection{Baseline Evaluation}\label{eval:1:baseline}
As we discussed previously, the comparison of obtained results should be performed with the defined baseline. In this section we review the meta-heuristics performance out-of-the-box on different problem sizes and parameter settings. For the visibility reasons, here and on we plot the intermediate and final performance evidences for each problem instance separately. All underlying TSP instances were previously solved by other exact solvers, therefore, we also present an optimal solution, available for each instance as a green doted line, however, if it appears in a range of interest.

%\newpage
\paragraph{kroA100 and pr439 TSP instances.}

Both 100 and 439 cities TSPs are relatively small problem instances. Therefore, all underlying MHs reach a local optimum after a few first external iterations. A difference between the external and internal iterations is the following: the first ends, when the main node sends the selected configuration and receives its results from the worked node. On the contrary, an internal iteration occurs inside LLH itself. Therefore, since the MHs quickly reach a local optimum on this TSP instances, there is no reason to spend much time for their results review. We put a visual representation of benchmarks for kroA100 and pr439 into the thesis appendix (\cref{app:eval:bl plots}).

The only observation that are worth mentioning are SA results with tuned parameters. They are worse in contrast to default values on kroA100 TSP instance. It is explained by the fact that for algorithm tuning we used a different problem instance (rat783). It only confirms a motivation of the parameter control approaches: tuning is not an instance-universal technique.

\paragraph{rat783 TSP instance.}
\svgpath{{graphics/Eval/baseline}}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on rat783.}
	\vspace{-5pt}
	\label{eval:pict:bl:rat783 intermediate}
\end{figure}

\setlength{\columnsep}{5pt}%
\setlength{\intextsep}{5pt}%
\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	\centering
	\includesvg[width=\linewidth]{rat783 baseline final boxplot}
	\label{eval:pict:bl:rat783 final}
	\caption{Final results of meta-heuristics with static parameters on rat783.}
	\vspace{-10pt}
\end{wrapfigure}
This is an average size problem among reviewed in the thesis. The bold lines in \cref{eval:pict:bl:rat783 intermediate} is a statistical mean of all 9 experiment runs with default (blue line) and tuned parameter values (orange line). A shadow around these lines is a confidence interval derived from 9 repeated runs. One may observe how parameter setting differently affects MHs: in evolution strategies the changes in performance is dramatic, while the results of simulated annealing (py.SA) are almost not affected. We also observe a decreased performance stability of tuned jMetalPy ES meta-heuristic (py.ES). It is reflected in a large confidence interval not only of the intermediate results, but also in a final solution quality statistics (\cref{eval:pict:bl:rat783 final}).

jMetal.ES (j.ES) with default parameters performs extremely slowly. However, using an optimized hyper-parameters it quickly reaches a local optimum (after ~50 external iterations) with the best produced results among other heuristics (\cref{eval:pict:bl:rat783 intermediate,eval:pict:bl:rat783 final}). Analyzing the performance evidences of Python-based solvers, we may conclude that they almost reached local optima in given 15 minutes using tuned parameters, and their final results are slightly worse than the ones produced by j.ES (\cref{eval:pict:bl:rat783 final}).

\textbf{Please note}, the trend's perturbation in the end of presented tuned py.ES is caused by the difference in number of external iterations among all runs (left figure in \cref{eval:pict:bl:rat783 intermediate}, tuned parameters). This number varies, since we used the wall-clock BRISEv2 termination criterion, but in some cases due to our implementation flaws LLHs reported with the delay or too hastily, ignoring given 15 seconds running time. Thus, the system managed to perform different numbers of external iterations. The used in this thesis visualization software\footnote{Python Seaborn data visualization framework web page:~\href{https://seaborn.pydata.org/}{seaborn.pydata.org}} estimates results average and deviation at each external iteration over all 9 performed runs. Thus, if one (or several) experiment execution(s) managed to perform more iterations than the majority of others, trends on this exceeding iterations will be respectively changed. Unfortunately, this perturbation appears in most of the progress charts, therefore, we perform the final results comparison by means of separately presented box-plots.


\paragraph{pla7397 TSP instance.} 

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:bl:pla7397 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	\centering
	\includesvg[width=\linewidth]{pla7397 baseline final boxplot}
	\label{eval:pict:bl:pla7397 final}
	\caption{Final results of meta-heuristics with static parameters on pla7397.}
	\vspace{-10pt}
\end{wrapfigure}

The largest TSP instance investigated in this thesis for a $~7.4$ thousand of cities is, however, referred as a middle-size OP in used TSPLIB95. With this instance, the performance evidences were changed the most, therefore, we discuss each MH behavior separately.

Python-based version of ES provides the worst results with both default and tuned parameters. Please note, the number of performed iterations by this MH with default parameters is less 50. It is caused by several reasons. Firstly, the amount of time required to perform an internal LLH iteration increased dramatically. Thus, with specified 15 seconds for one task run, it actually takes much more time (up to 1 minute). It may be caused by the algorithm code basis implementation. To implement generic termination criteria (and some other features) the authors utilized a push-observer design pattern~\cite{benitez2019jmetalpy}, according to which the underlying algorithm triggers its observers after finishing internal iteration. Therefore, the stopping criteria are evaluated only after finishing the iteration, which in case of running py.ES with TSP instance for $7.4$ thousand cities takes a while depending on the algorithm configuration. For instance, with parameters \texttt{\{mu=5, lambda=10\}} termination happens in time, while setting \texttt{\{mu=500, lambda=500\}} the required number of computations is much higher, therefore, we observe the ES algorithm termination after a very first internal iteration. This also causes a poor solution quality improvements. Naturally, there is also an overhead in the results sending through the network, but comparing the performance of Java-based ES with default parameter values, this overhead caused the decrease only in ~50 external iterations. We also eliminate a possible issue in a required time for problem loading (building the TSP distance matrix), since using problem instance caching (see \cref{impl: LLH scope}), the worker node does it only once. In any case, this issue requires a deeper investigation that we postpone to the future work due to the time limits. With tuned parameters for py.ES, the issue with task reporting delays disappears and, therefore, the number of external iterations is higher, but the solution improvements are still weak (see left picture on the \cref{eval:pict:bl:pla7397 intermediate}).

As in the previous case, py.SA produces good results quality improvements at each external iteration, least depending on the hyper-parameter values. Even with a default configuration, py.SA outperforms the final results of py.ES after the first 50 external iterations. Setting the tuned parameter values, the performance of algorithm increases, but not dramatically. A resulting progress curve, presented in the middle of \cref{eval:pict:bl:pla7397 intermediate}, shows that py.SA requires more time to converge that it was provided and still far from its potential local optima on pla7397 after 15 minutes.

j.ES is a perfect candidate to show, how important is a parameter setting. With default configuration j.ES is weak in making improving steps and cannot compete with other algorithms. Our guess here is the same as for the Python-based version: the number of internal iterations is extremely low for making a good search space traversal. However, a tuned version of j.ES outperforms all other solvers in intermediate (\cref{eval:pict:bl:pla7397 intermediate}) and final performance terms (\cref{eval:pict:bl:pla7397 final}).


\paragraph{Discussion.} The observed results of meta-heuristics execution confirmed the algorithm parameter setting problem importance, discussed in \cref{bg: section Parameters Setting}. An effect of proper parameter selection is different among available algorithms. In our case, the performance of two out of three solvers is highly dependent on the hyper-parameter settings (ESs). Thus, an application of the proposed in \cref{Concept description} generic parameter control approach to these algorithms is rather intriguing and may reveal the proposed methodology benefits.

From the other side, we observe the domination of only one algorithm among the others with static parameters. See how all MHs were solving each TSP instance with default parameters: in each case SA outperforms two other ESs. The usage of all three MHs in a selection hyper-heuristic with static hyper-parameters will reveal is applicability. On the contrary, when we switch to the tuned parameter usage, j.ES is preferred. We see it clearly when the MHs are applied to the largest TSP instance and, therefore, we expect to observe this MH dominance in a selection hyper-heuristic with tuned LLHs.

%\newpage
\subsection{Generic Parameter Control (MH-PC)}\label{eval:1:PC}
As discussed in \cref{bg: parameter control}, the goal of dynamic parameter setting is a maximization of underlying algorithm performance measurements at runtime. In this part of evaluation we compare the performance of algorithms with statically defined hyper-parameters (default and tuned) to performance of the same algorithms with enabled generic RL-based parameter control.

\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/control}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 PC progress}
	\caption{Intermediate results of MH-PC on kroA100.}
	\vspace{-5pt}
	\label{eval:pict:pc:kroA100 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 PC final boxplot}
	\caption{Final results of MH-PC on kroA100.}
	\vspace{-5pt}
	\label{eval:pict:pc:kroA100 final}
\end{figure}

Generic parameter control on a small problem instance is able to outperform the results of static parameters after first 50 iterations on all MHs (\cref{eval:pict:pc:kroA100 intermediate}). We may observe that even the random changes hyper-parameters at runtime result in better solutions, comparing with statically defined parameters (\cref{eval:pict:pc:kroA100 final}). It is mainly caused by the changes in a neighborhood definition (mutation type) and traversal process (mutation probability). In most cases, given enough time the learning-based parameter assignment outperforms random allocation.

Please note, the number of performed by j.ES iterations in \cref{eval:pict:pc:kroA100 intermediate}. According to our plan, time for optimization session equals 15 minutes. We define 15 seconds for running one configuration (external iteration). We run 6 workers in parallel, which in the most optimistic case should perform $\frac{15\cdot60\cdot6}{15} = 360$ external iterations. However, we observe even more than 400. After an investigation, have come to the conclusion that it is caused by an implementation flaw of j.ES. While adding time-based termination criterion, we did not remove the previously existing iteration-based. For the iteration counter in jMetal meta-heuristics a regular Java integer is used, which we set to its maximal value when using a time-based criterion. Given a `light' algorithm configuration (low $\mu$, $\lambda$ and mutation probability) and a relatively small OP, MH is able to reach the maximal number of iteration in less than 15 seconds, therefore, terminating earlier and triggering a new external iteration. Certainly, it is our implementation flaw, which should be fixed in a future work.

\paragraph{pr439 and rat783 TSP instances.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 PC progress}
	\caption{Intermediate results of MH-PC on rat783.}
	\vspace{-5pt}
	\label{eval:pict:pc:rat783 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 PC final boxplot}
	\caption{Final results of MH-PC on rat783.}
	\vspace{-5pt}
	\label{eval:pict:pc:rat783 final}
\end{figure}

In this two cases, the behavior of solvers were similar, therefore, we decided to join their discussion and present only the plots for a larger instance (rat783). Still, the intermediate and final performance representation for pr439 TSP instance can be found in \cref{app:eval:pc plots}.

When the parameter control is applied to a larger problem instance, it starts to require more evidences (external iterations) for finding a good-performing settings for py.ES. Concretely, the TPE-based parameter control was the closest in approaching the quality of tuned parameters. All techniques produced highly unstable intermediate and final results: please, draw your attention to the left side of \cref{eval:pict:pc:rat783 intermediate}, and filled with blue boxes in \cref{eval:pict:pc:rat783 final} respectively.

The results of parameter control application to less sensitive py.SA are the following: randomized parameter sampling settled on the level of default parameters quality. BRR-based parameter control yielded a slightly better results, while TPE model approached the quality of tuned parameters (\cref{eval:pict:pc:rat783 final}).

On the contrary, generic parameter control in j.ES MH leads to results quality comparable  with tuned parameters (\cref{eval:pict:pc:rat783 final}). Please note, as for previous problem instance, even a random-based parameter sampling outperforms default parameters when given enough time.

\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 PC progress}
	\caption{Intermediate results of MH-PC on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:pc:pla7397 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 PC final boxplot}
	\caption{Final results of MH-PC on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:pc:pla7397 final}
\end{figure}

For the final problem instance we omit the results for py.ES, because it did not manage to perform even the slightest improvement, comparing with the default parameter values. It is caused by the fact that in early stages our approach acts as a random search, since not enough evidences were obtained to build prediction models.

As in previous cases, the least configuration-sensitive py.SA shows an ability to perform almost equally with any parameter settings (\cref{eval:pict:pc:pla7397 intermediate}). Neither among used control techniques was able to outperform the results obtained by the tuned in offline algorithm (\cref{eval:pict:pc:pla7397 final}). 

As for j.ES, the model-based MH-PC outperform the randomized parameter values allocation. Moreover, TPE-based control reached and outperformed even the results of hyper-parameters tuned in offline.

\paragraph{Discussion.} In general, the review of meta-heuristic performance on different problem instances showed that the proposed generic parameter control approach is able to yield not only the near-tuned parameters quality, but in some cases even outperforming results.

Taking into account the results with random parameter allocation we make two conclusions. Firstly, even randomized parameters changes are able to improve a potentially bad static hyper-parameter setting (j.ES case). Secondly, the learning mechanisms should and must be improved further by means of different surrogate models usage. The proper technique for surrogate optimization should be used. Leaving the improvement steps for future work, we conclude that the developed in this thesis generic parameter control concept may be proposed as a replacement of the parameter tuning for meta-heuristics.


\subsection{Selection Hyper-Heuristic with Static LLH Parameters (HH-SP)}\label{eval:1:hh-sp}
The second mode of the developed approach is the RL-based selection hyper-heuristic, which description can be found in \cref{concept: conclution}. Here we group three available LLHs (aforementioned py.ES, py.SA and j.ES) with static parameter (default and tuned) into selection hyper-heuristic (HH-SP).

We present the problem solving process in two forms. Firstly, we distinguish the selected at each external iteration LLH. In order to do so, we visualize only the first repetition (out of 9 available). Secondly, we present the final results of all runs in form of box-plots and compare them with the performance of underlying LLHs used executed separately (baseline). The left group of box-plots presents the final solution quality obtained with the default parameter values, while on the right site the results of tuned parameters are outlined.

\paragraph{kroA100, pr439 and rat783 TSP instances.}
\svgpath{{graphics/Eval/selection}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP progress}
	\caption{Intermediate performance of HH-SP on rat783 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP final boxplot}
	\caption{Final results of HH-SP on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 final}
\end{figure}
Once again, we group relatively small problem instances on which the implemented HH-SP performs similarly. To analyze this group, we selected the largest instance among them: rat783, while the figures depicting kroA100 and pr439 TSP instances may be found in \cref{app:eval:hh-sp}.

% default parameters
We would like to draw the reader's attention to HH-SP cases, in which the LLHs were used with the default parameter values (upper row in \cref{eval:pict:hh-sp:rat783 intermediate}). According to baseline evaluation, there is only one algorithm with a strong performance dominance: py.SA. Therefore, in \cref{eval:pict:hh-sp:rat783 intermediate} we observe a high frequency of py.SA sampling by both learning-based selection strategies. One may distinguish a repetitive pattern in LLH allocation with FRAMAB (middle column). It is caused by a deterministic essence of the algorithm. Reaching a critical point, where other heuristics should be verified, the FRAMAB's exploration mechanism fully guides a selection. Due to the time-based LLH termination usage, all workers are starting the next round (mostly) in bunches. Thus, when a new round starts, FRAMAB operates on static information and allocates all next configurations with the same LLH, which turns to be the second best performing algorithm: j.ES. Therefore, we conclude that FRAMAB behaves slightly inertly in our setup. One may argue this will cause a decrease in performance, which is a rather logical conclusion. However, it requires a further investigation, which we postpone for the future work. In case of BRR usage (right column in \cref{eval:pict:hh-sp:rat783 intermediate}), the bias is strongly shifted towards py.SA. It definitely may cause the performance issues due to the lack of exploration. According to presented in \cref{eval:pict:hh-sp:rat783 final} final results statistics, given at least one dominating LLH, HH-SP utilizes it enough times to obtain a good final solution quality.

The next setup is LLHs with tuned parameters (lower row in \cref{eval:pict:hh-sp:rat783 intermediate} and right side of \cref{eval:pict:hh-sp:rat783 intermediate}). According to the baseline evaluation, all among available LLHs are able to tackle the problem instance producing comparable solution quality, however, the performance difference still exists (\cref{eval:pict:hh-sp:rat783 final}). As a consequence, FRAMAB HLH learns it and frequently utilizes the best performing j.ES (lower row, middle plot in \cref{eval:pict:hh-sp:rat783 intermediate}). On the contrary, BRR and random-based approaches sample all LLH types evenly. We conclude that BRR is not as sensitive to the performance evidences and was `confused', since the process quickly converged into a local optimum. The quality of all HP-SP final results presented in \cref{eval:pict:hh-sp:rat783 final} are at least as good, as the solution quality provided by the best underlying LLH.

\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP progress}
	\caption{Intermediate performance of HH-SP on pla7397 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pla7397 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP final boxplot}
	\caption{Final results of HH-SP on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pla7397 final}
\end{figure}
Our observations of HH-SP performance on the largest problem instance is as following. During the baseline evaluation, py.ES with default parameters had the worst performance, while the tuned algorithm version was able to outperform only default j.ES. On the contrary, j.ES with tuned parameters produced the best results, outperforming py.SA. The best meta-heuristic with default parameters was py.SA. Therefore, we observe an expected behavior of HH-SP with default LLHs (upper row in \cref{eval:pict:hh-sp:pla7397 intermediate}): the most frequently sampled by learning-based HLH was py.SA. However, the number of py.ES usages is suspiciously high in BRR case. Referring to the final results presented in \cref{eval:pict:hh-sp:pla7397 final}, we observe a high diverse in quality when py.ES is allocated frequently (codes 1.1. and 3.1), which is an expected behavior, when the performance of py.ES with default parameters is taken into account.

Talking about the tuned LLHs usage, we observe almost equal performance of all LLH sampling approaches, comparable to the baseline results. The solution quality of random-based HLH is slightly worse, in comparison to the results of FRAMAB- and BRR-based HH-SP due to their j.ES preference (see right chart in \cref{eval:pict:hh-sp:pla7397 final}).

\paragraph{Discussion.} According to our observations of the developed HH-SP performance, we conclude that the proposed concept implementation operates as expected: HH-SP provides similar to the best underlying LLH results. Two implemented selection HLH are performing slightly differently when reaching a local optimum. We claim the FRAMAB is a more perspective HLH, since it starts to balance between previously good performing LLH exposing good exploration abilities. In the cases when an advantage of one LLH changes to another, BRR may need more time to learn this.

The observed issues call not only for a thorough investigation (pla7397 code 1.1, 3.1), but also for a generic approach to handle potential flaws in the LLH implementation that may cause issues in overall execution process.


\subsection{Selection Hyper-Heuristic with Parameter Control (HH-PC)}\label{eval:1:hh-pc}
The final evaluation is dedicated to the performance analysis of the suggested approach of merging the online selection hyper-heuristic with the generic parameter control technique. A minimal goal is to reach the best underlying LLH performance with tuned hyper-parameters. In this evaluation set we follow the used for HH-SP method of intermediate results visualization, distinguishing allocated LLH types at each iteration for single repetition. We compare the quality of final results over all repetitions with a baseline using box-plots.

\paragraph{kroA100, pr439 and rat783 TSP instances.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC progress}
	\caption{Intermediate performance of HH-PC on rat783 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:rat783 intermediate}
\end{figure}

\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:rat783 final}
\end{figure}

The decision of joining all three TSP instances is motivated by a similar to aforementioned reasons: the intermediate and final performance are rather similar among problem instances and do not require separate review, therefore, here we present only a single case (rat783). The results for all other problems may be found in~\cref{app:eval:hh-pc}.

During the solving process similar to HH-SP patterns of algorithm allocation may be observed for both FRAMAB-based (codes 2.4, 2.5) and BRR (codes 3.4, 3.5) HLHs. However, in this case the intermediate results are slightly differing, since the parameter control started to search for a good LLHs configuration (\cref{eval:pict:hh-pc:rat783 intermediate}). 

Let us firstly draw the reader's attention to HH-PC with FRAMAB-based LLH selection and TPE-based parameter sampling (code 2.4 in \cref{eval:pict:hh-pc:rat783 intermediate}). At the beginning of solving process, j.ES was performing extremely well, for this reason FRAMAB was sampling it with a higher frequency. When a solving process reached its local optima (nearly $50^{th}$ iteration), FRAMAB switched to exploration of the other algorithms. An appeared `noise' in the results of both ES heuristics is caused by a boolean parameter \emph{elitist}, which defines the selection strategy and may result in the solution quality degradation (more details in \cref{BG: MH Examples}). From an absence of the noise in later stages of 2.4 and 3.4, we may conclude that TPE has found \texttt{elitist=True} parameter value to be perspective. On the contrary, BRR-based parameter controller did not find these parameters and glancing on the final results quality (\cref{eval:pict:hh-pc:rat783 final}) we conclude the BRR finds statistically worse performing parameters, in comparison to TPE. Also, we must not ignore the fact of early reaching a local optimum by the search processes (see shapes of progress curves in \cref{eval:pict:hh-pc:rat783 intermediate}). In such case, the parameter search may be biased towards exploration. For that we need to introduce the other progress metrics, such as stagnation detection (used in EA parameter control approaches~\cite{karafotias2014generic}) and perform multi-objective surrogate optimization, maximizing improvement and minimizing stagnation. Since it is a rather considerable amount of effort, we postpone this enhancement for the future work.

In \cref{eval:pict:hh-pc:rat783 final} we clearly see the dominating j.ES MH with tuned parameters (code 4.3.2). The quality of final solution, produced by HH-PC is slightly lower than the best performing tuned j.ES. This may be explained by a lack of performance evidences obtained for the learning models, since the optimization reached its local optima too quickly.


\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC progress}
	\caption{Intermediate performance of HH-PC on pla7397 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pla7397 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pla7397 final}
\end{figure}

During these benchmarks, the issue with py.ES made a crucial change in the overall number of iterations where it was used many times (codes 1.3, 3.4, 3.5 in \cref{eval:pict:hh-pc:pla7397 intermediate}). While the case of fully randomized HH-PC (code 1.3) is clear, a BRR LLH selection did use this LLH frequently, because it may have produced good quality solutions as a result of parameter control. On the contrary, FRAMAB LLH selection in combination with TPE parameter tuning (code 2.4) managed to find good parameters for py.SA, therefore, utilized it most often. When the FRAMAB's exploration component weight reached critical point, the other LLHs usage was triggered and as a result, HH-PC switched to j.ES usage. This switch later gave dramatic result improvements (code 2.4 \cref{eval:pict:hh-pc:pla7397 intermediate}). The FRAMAB LLH selection with BRR parameter control (code 2.5) at the beginning was using the mixture of mainly two j.ES and py.SA, but later switched to the simulated annealing-only usage. As we may see, it gave fast coarse-grained solution improvements at the beginning, and stable, but rather slow fine-grained improvements in the later stage.

The final results quality charts (\cref{eval:pict:hh-pc:pla7397 final}) depict a dominance of FRAMAB-based LLH selection (codes 2.4, 2.5) over BRR-based and TPE-based parameter control (codes 2.4 and 3.4) over BRR-based (codes 2.5, 3.5). line, the obtained results with BRR-based parameter control are more stable than the provided by TPE-surrogates. The quality of final results did not reach the best performing tuned j.ES (code 4.3.2) due to the issue with py.ES. However, since the optimization process did not settle in the local optima, we have a doubt that HH-PC will not outperform j.ES given same number of external iterations. For a better intuition, let us draw the reader's attention to \cref{eval:pict:hh-pc vs jES on pla7397 process}. Please note, how HH-PCs are approaching j.ES progress curve. If it were not for the issue with a number of external iterations, the results would be better and, probably, outperforming the best available tuned j.ES.

\begin{figure}[h]
	\centering
	\vspace{-5pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC vs jES progress}
	\caption{HH-PC and tuned jMetal ES solving process comparison on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc vs jES on pla7397 process}
\end{figure}

\paragraph{Discussion.} The observed results of tackling the united APSP problem by HH-PC are encouraging. Our approach managed to reach and in some cases even outperform the results of the best underlying LLH with tuned hyper-parameters on small problem instances (kroA100, pr439, rat783). When the problem size significantly grows (pla7397), the gap between HH-PC and the best performing LLH started to increase. An explanation for this is simple: the system needs to build surrogate models not for single, but for multiple LLHs parameters and, therefore, requires more performance evidences in comparison to MH-PC or HH-SP. line, an amount of information only decreases as a consequence of an issue with py.ES. Since the parameter values are not selected to properly reflect the LLHs performance, the algorithm selection process also straggles. To overcome the problem with lack of information we (1) need to fix the aforementioned issue and (2) may execute an additional meta-learning step before the beginning of optimization session. An investigation of this rather intriguing idea is postponed for the future work.


\subsection{Concept Evaluation Results Discussion}
We aggregate the results of each implemented mode in \cref{eval:1: best final results comparison table,eval:1: avg final results comparison table}. In each of them the baseline is defined as the best results obtained by the underlying meta-heuristics used separately with static parameters. As an example, the baseline for kroA100 TSP instance is defined by jMetalPy.SA with default parameters, while tuned jMetal.ES produced the best solutions for pla7397. The results for each experiment separately are presented in \cref{app:eval:1:tables}.

In \cref{eval:1: best final results comparison table} we compare the baseline with the \emph{best} results in each mode. For instance, consider MH-PC. Statistically, the best average results for kroA100 (27178) were produced by jMetal.ES with TPE control (code 4.3.4), while the baseline is 39560. Therefore, the average result of the best MH-PC equals to 0.687 of the baseline. We perform a similar aggregation for HH-SP, excluding the experiments, where LLHs were used with tuned parameters (therefore, selecting the best among 1.1, 2.1 and 3.1 codes). As for HH-PC, we pick the best averaged results in all available experiments, for instance, on kroA100 it was code 3.4 (HH-PC with FRMAB LLH selection and TPE parameter control) with 30396 average result.

Our conclusion on \cref{eval:1: best final results comparison table} is as follows: if one \textbf{knows} which among available meta-heuristics statistically produces better results (with properly selected parameters), the preference is to use the proposed generic parameter control (\textbf{MH-PC}) approach. 

\begin{samepage}
	\begin{table}[!htbp]
		%\centering
		\caption{The best solution quality obtained by each mode compared with the best underlying meta-heuristic (baseline) on four TSP instances (lower is better).}
		\label{eval:1: best final results comparison table}
		\begin{tabular}{cccccc}
			\hline
			\rowcolor{gray!10}
			\textbf{TSP Instance} & \textbf{Baseline} & \textbf{MH-PC} & \textbf{HH-SP} & \textbf{HH-PC} \\
			\hline		
			kroA100 & 1 & \textbf{0.687} & 0.961 & 0.768 \\
			pr439 & 1 & \textbf{0.973} & 1.072 & 0.988 \\
			rat783 	& 1 & 1.081 & 1.448 & \textbf{1.064} \\
			pla7397 & 1 & \textbf{0.985} & 2.228 & 1.546 \\
			\hline
		\end{tabular}
	\end{table}
	
	
	\begin{table}[!htbp]
		%\centering
		\caption{Average solution quality obtained by each mode compared with the best underlying meta-heuristic (baseline) on four TSP instances (lower is better).}
		\label{eval:1: avg final results comparison table}
		\begin{tabular}{cccccc}
			\hline
			\rowcolor{gray!10}
			\textbf{TSP Instance} & \textbf{Baseline} & \textbf{MH-PC} & \textbf{HH-SP} & \textbf{HH-PC} \\
			\hline		
			kroA100 & 1 & 0.841 & 0.975 & \textbf{0.773} \\
			pr439 & 1 & 1.106 & 1.1 & \textbf{1.017} \\
			rat783 	& 1 & 2.034 & 1.45 & \textbf{1.1} \\
			pla7397 & 1 & 2.139 & 2.36 & \textbf{1.93} \\
			\hline
		\end{tabular}
	\end{table}
\end{samepage}

However, the situation changes dramatically if one \textbf{does not know}, which meta-heuristic is the best among available. In \cref{eval:1: avg final results comparison table} we aggregate the average results over \emph{all} experiments in each mode. It means that the results of all meta-heuristics with parameter control are taken into account to estimate averaged MH-PC gain. However, we need to exclude several experiments to perform a fair comparison. For MH-PC we exclude the random-based parameter selection (4.1.3, 4.2.3, 4.3.3 codes). For both HH-SP and HH-PC modes we similarly exclude the random-based LLH and parameter selection (1.1 and 1.3 codes). Also, for HH-SP we ignore the experiments with tuned in offline meta-heuristics (1.2, 2.2, 3.2).

Therefore, our conclusion for cases, when one has several meta-heuristics, but \textbf{does not know}, which is the best among them, the complex approach of simultaneous online algorithm selection and parameter tuning (\textbf{HH-PC}) should be preferred due to its strong dominance over MH-PC and HH-SP modes.

\section{Analysis of HH-PC Settings}\label{eval:2}
The second benchmark is dedicated to the evaluation of system settings influence on the solving process.
Due to limited time we decided to perform the parameter analysis only for the most complex system mode: online selection hyper-heuristic with parameter control in low-level heuristics (HH-PC).

\subsection{Evaluation Plan}\label{eval:2:plan}
Similarly to the concept evaluation, we start this set of benchmarks from the planning. For comparison basis we selected one statistically better performing HH-PC setup, in which FRAMAB was used for the algorithm selection, and TPE for parameter control respectively. Implemented FRAMAB algorithm exposes only one hyper-parameter: the balancing coefficient \emph{C}. In \cref{impl: FRAMAB} we discussed it values and also proposed an approach to replace this static value by dynamically derived from results deviation. In all previous tests we used exactly this approach. TPE implementation exposes \emph{split size} parameter, which defines the percentage of available data, used to create a distribution of good-performing parameter values (see TPE description during HpBandSter framework review in \cref{bg: bohb}). In the previous experiments we used $\sfrac{1}{3}$ of available data to construct a \emph{good} distribution. The overall amount of information, used to construct the surrogate models was controlled by a shared parameter \emph{window size}. In our evaluations we were using 80\% of available information at each point in time. As one may remember, we did not implement an  elegant algorithm for surrogates optimization, but used the random search instead to select the best parameter set by means of their results on surrogates. Thus, we used a predefined number (96, the default value in BRISEv2) of randomly sampled parameter values on each level (more details in \cref{impl: prediction models}). Also, our setup included 6 workers and task execution time was set to 15 seconds. The implemented RL approach implies the iterative solving process, therefore, the workers were sending a complete bunch of obtained solutions to main node after accomplishing a task. 

All aforementioned characteristics form three groups of experiments:
\begin{enumerate}
	\item \textbf{Learning granularity} group is designed to investigate the influence of performance evidence amount and quality to optimization process. It is formed by such parameters as \emph{window size}, \emph{task time} and \emph{number of workers}.
	
	\item \textbf{Learning models configuration} group of experiments is dedicated to investigate the influence of HLH parameters on the results quality and includes \emph{FRAMAB C coefficient}, \emph{TPE split size} and \emph{random search size}.
	
	\item \textbf{Amount of warming-up information} is a self-describing experiment, dedicated to investigate an influence of warming-up solutions numbers to initialize LLH at each external iteration.
\end{enumerate}

We define the estimation values for the proposed parameters in \cref{eval:2: planning table}.

\begin{table}[h!]
	\centering
	\begin{tabular}{ccc}
		\hline
		\textbf{Parameter} & \textbf{Investigated values} & \textbf{Default value} \\
		\hline
		\rowcolor{gray!10}
		\multicolumn{3}{c}{\textit{Learning granularity}} \\
		Window size & 30\%, 50\%, 100\% & 80\% \\
		Task time & 5, 10, 30 seconds & 15 seconds \\
		Number of workers & 3, 9, 12 & 6 \\
		\rowcolor{gray!10}
		\multicolumn{3}{c}{\textit{Learning models configuration}} \\
		TPE split size & 10\%, 50\%, 70\% & 30\% \\
		FRAMAB C coefficient & Static 0.001, 0.01, 0.1 & STD-based \\
		Random search size & 50, 200 & 96 \\
		\rowcolor{gray!10}
		\multicolumn{3}{c}{\textit{Amount of warming-up information}} \\
		Warming-up solutions & one & all \\
		\hline
	\end{tabular}
	\caption{Prediction techniques used for the concept evaluation.}
	\label{eval:2: planning table}
\end{table}

We set all other parameters as they were configured during HH-PC evaluation: the experiment running time is set to 15 minutes, the number of experiment repetitions is 9 and the search space is unchanged.

The idea of performing the full factorial design was quickly abandoned, since it requires 46 days of non-stop experiment running.
Therefore, the performed one-exchange benchmark design resulted in 18 experiments, which needed 40,5 hours to perform 9 repetitions.

\subsection{Learning Granularity}\label{eval:2:learning granularity}
In this experiment we investigate the influence of RL routines configuration on learning process. The idea is as follows. Changing the \emph{window size}, \emph{task time} and \emph{number of workers}, the underlying surrogate models may learn a different dependencies picture. For instance, if we increase a task time, each sampled configuration will be measured more thoroughly, however, given the same experiment running time, the overall number of iteration will be decreased. On the contrary, by increasing a number of workers, a portion of investigated APSP space will be increased, therefore, the underlying learning models will construct more precise surrogates.

\textbf{Please note}, the perturbations in the end of progress charts are caused by the same issue with the number of performed iterations, discussed in \cref{eval:1:baseline}.

\paragraph{Window size.}
\svgpath{{graphics/Eval/2bench}}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{window size plots}
	\caption{Influence of HH-PC (code 2.4) window size on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:window size}
\end{figure}
According to our expectations in \cref{concept:prediction}, this parameter is responsible for the forgetting mechanism, which is required to follow possibly changing trends in optimization process.

In \cref{eval:2:pict:window size} we presented the results obtained with four different window sizes. We observe that the best results are obtained, when the learning models were used all the available information, in other words, with disabled forgetting mechanism. On the contrary, with the smallest window size the quality of final results is expectedly the worst. A trend could be observed, according to which HH-PC provides a better quality of results with larger window size. In our previous benchmarks we were using 80\% of the available information, which corresponds to the middle-quality parameter value.

Our conclusion is the following: with this problem instance and system setup, changes in the learning process and parameter preference are mostly negligible, therefore, the forgetting mechanism should be disabled. In the future, it would be rather intriguing to investigate this parameter influence on other, potentially dynamic problem instances.

\paragraph{Task time.}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{task time plots}
	\caption{Influence of HH-PC (code 2.4) task time on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:task time}
\end{figure}
Varying the time for one task evaluation (external iteration), one will dramatically change the granularity of obtained results. For instance, in our experiments with the least task time (5 seconds) HH-PC performed more than 600 external iterations, while with the largest budged (30 seconds), this number was approximately 150 (\cref{eval:2:pict:task time}). As we previously observed, more information does not necessary imply better surrogate models creation. Used during the concept evaluation 15 seconds task time provided the worst results. Boundary cases of 5 and 30 seconds task time gave rather unstable result (see box-plots in \cref{eval:2:pict:task time}). The former is full of too approximately evaluated parameters, while the later simply did not manage to perform enough iterations. Balancing between results stability and quality, we conclude that for the current setup statistically better choice would be to set a task running time equal 10 seconds. 

\paragraph{Number of workers.}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{number of workers plots}
	\caption{Influence of HH-PC (code 2.4) workers number on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:number of workers}
\end{figure}
The influence of workers number was a surprise to us. According to our expectations, with a growing number of LLH runners, the proportion of estimated parameter space increases. However, instead of getting better quality surrogate models and, as a result, improving sampled parameters performance, we observe the opposite behavior. For increasing number of workers the results became worse and worse (see box-plots in \cref{eval:2:pict:number of workers}). Currently, we do not have a comprehensive explanation of the observed behavior, except for the possible surrogate models over-fitting. It is a broadly studied problem in the ML field, according to which the model learns too complex hypothesis, which cannot generalize well to unforeseen data. Thus, in our case it is possible that being over-fitted, surrogates did not adequately predict results for sampled parameters and, therefore, guided prediction process in a wrong direction. In any case, this behavior requires more comprehensive investigation in the future work.

\subsection{Learning Models Configuration}\label{eval:2:learning models}
We perform this set of experiments for getting an intuition about the underlying high-level heuristic configuration influence on a general performance.

\paragraph{TPE split size.}
\begin{figure}[b]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{TPE split size plots}
	\caption{Influence of HH-PC (code 2.4) TPE split size on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:tpe split size}
\end{figure}
The internals of Bayesian TPE approach for parameter sampling were described in \cref{bg: bohb}. With small split size, only elite parameter values form a \emph{good} distribution, therefore, an overall sampling process happens to be more greedy or, in other words, biased towards exploitation. According to our observations, visualized in \cref{eval:2:pict:tpe split size}, the more greedy parameter allocation produces statistically better results (10\%), while usage of 70\% split showed the worst potential.

\paragraph{FRAMAB C coefficient.}
\begin{figure}[t]
	\centering
	\begin{subfigure}{\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{FRAMAB c plots}
		\caption{Statistical results comparison.}
		\label{eval:2:pict:framab c statistic}
	\end{subfigure}
	%\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{\textwidth}
		\includesvg[width=\textwidth]{FRAMAB c plots chonics}
		\vspace{-5pt}
		\caption{One run distinguishing different LLHs allocation.}
		\label{eval:2:pict:framab c one run}
	\end{subfigure}
	\caption{Influence of HH-PC (code 2.4) FRAMAB C coefficient on pla7397 TSP instance solving process.}
	\label{eval:2:pict:framab c}
\end{figure}

As the reader remembers from FRAMAB description, presented in \cref{impl: FRAMAB}, the role of coefficient C is to control the EvE balance while selecting the LLH. In our implementation we proposed to replace C with the improvements standard deviation, therefore, in first set of benchmarks we exclusively used this, STD-based FRAMAB regime.

The results of performed benchmarks with different C values are presented in \cref{eval:2:pict:framab c}. More concretely, in \cref{eval:2:pict:framab c statistic} we compare statistics of all repetitions with different parameter value. We conclude, the proposed parameter-less (STD-based) FRAMAB version produces similar results in comparison to other cases. When C (or STD) is large, the exploration-related component of FRAMAB's UCB value is increased, therefore, more different algorithms are used for optimization. However, when the C value is decreased, only several switches may be observed (\cref{eval:2:pict:framab c one run}). An intriguing idea arises to introduce the technique for FRAMAB parameter control, according to which, the entire LLH portfolio is utilized at the beginning with high C value, while approaching the end C is increased to concentrate on the best-performing LLH.


\paragraph{Random search size.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{Random search size}
	\caption{Influence of HH-PC (code 2.4) random search size for surrogate optimization on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict: random search size}
\end{figure}
This HLH parameter configures the random search process, performed over the created surrogate models. According to our expectations, the increased random search size should result in a more precise parameter values prediction and as a consequence, to performance gain. However, evaluating random search sizes of 50 and 200 samples respectively, the obtained results happen to be not as we expected. In general, we observe a quality fluctuation, which is caused by the randomized processes. It only motivates us to implement a proper surrogate optimization technique for improving a robustness of the prediction process.

\subsection{Amount of Warming-up Information}\label{eval:2:llh changes}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{warming-up solutions plots}
	\caption{Influence of HH-PC (code 2.4) warming-up solution number on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:warming-up solutions}
\end{figure}
Here we evaluate how the \emph{number of warming-up solutions} affects the search. Our intuition during the implementation was following: we should initialize the solver with all previously obtained solutions not to lose the derived optimization trajectory and traversal velocity. However, it is relevant only to the population-based algorithms, such as evolution strategy or potential genetic algorithm.

After changing this behavior to only one warming-up solution usage, we surprisingly found out that the results quality was almost not affected: please, pay attention to overlapping progress curves in a left side of \cref{eval:2:pict:warming-up solutions}. Moreover, by passing only one solution between LLHs we dramatically increased the overall number of external iterations (for more than 50\%). It is caused by the reduced overhead for information processing and sending through the network. We conclude that changing the behavior to only one warming-up solution usage provides a positive impact on the final results quality and do not affect the intermediate performance of meta-heuristics.



\section{Conclusion}\label{eval: conclution}
The evaluation of proposed concept was presented in this Chapter and performed in two stages. At the first stage an analysis of the implemented concept applicability was performed. We compared it with a baseline, defined by the executed in isolation underlying meta-heuristics with static hyper-parameters. Our conclusions on the concept applicability are following:
\begin{itemize}
	\item Firstly, the proposed reinforcement learning-based generic parameter control approach (MH-PC) is able to significantly improve the performance of meta-heuristic's static hyper-parameters and in some cases even outperform the quality of tuned at the design time parameters.
	
	\item Secondly, the developed heuristic selection technique (HH-SP) is able to reach a quality of the best performing underling low-level heuristic. 
	
	\item Finally, the approach for simultaneous online algorithm selection and parameter tuning (HH-PC) outperforms the best underlying algorithm with tuned parameters on rather small problem instances (kroA100, pr439). With growing complexity, HH-PC only approaches the quality of the best available algorithm with tuned parameters.
\end{itemize}

Our conclusions on the implemented concept usage are the following: when the dominance among meta-heuristics is known beforehand, one should consider the MH-PC usage with this MH; if the dominance is not known, but the parameters of meta-heuristics are already tuned (which may be rare use-case), one should consider HH-SP usage; but if the dominance of MHs is not known and proper parameter values are unknown as well, one should naturally utilize the proposed HH-PC concept to tackle the optimization problem at hand.


In the second part of evaluation we investigated the HH-PC configuration influence on its performance. The impact of some among evaluated parameters was not as we expected, which only motivates the importance of a proper parameter values search and usage. More concretely, an insertion of the relatively light \emph{forgetting mechanism} instead of improving only made the results statistically sightly worse. The increased number of workers only decreased the surrogate models accuracy. The decision on all available solutions usage for LLH initialization introduced a redundant overhead, but did not improve final results quality, therefore, it should be reconsidered. Nevertheless, a set of experiments from the second evaluation stage should also be performed over the other system operating modes (MH-PC and HH-SP) and problem instances. By doing so, we will be able to make a confident conclusion about the stability and impact of different parameter values. Up until now, it revealed the system adaptation ability with help of exposed parameters and helped us find several incorrect decisions.