\chapter{Evaluation}\label{eval}
The concepts, proposed in \cref{Concept description}, implemented in \cref{imp} of search space representation, prediction process and based on this generalized parameter control approach, selection hyper-heuristic and the hyper-heuristic with parameter control should be broadly evaluated. The experiments may be performed in number of investigation directions, starting from the developed reinforcement learning performance with respect to system configuration and ending with the scalability to different problem sizes.

The structure of this Chapter is as follows. We start the evaluation process with a review of TSP benchmark set in \cref{eval: op}. The following benchmarks could be divided to two main parts. The first is dedicated to the evaluation of developed concept in comparison to the base line and is presented in \cref{eval: concept}, while in the second we investigate an influence of the proposed hyper-heuristic with parameter control settings on its performance (\cref{eval: hh-pc}). Finally, in \cref{eval: conclution} we conclude a discussion of the obtained results.


\section{Optimization Problem}\label{eval: op}
Through this thesis we are tackling a vehicle routing problem â€” the traveling salesman OP, which explanation could be found in \cref{BG: subsection OPs}. Nevertheless, as a reminder we repeat its short definition here. We also include the other details, related to the benchmarks.

``Given a set of cities and the distances among them, find the shortest path, which visits all cities''. It is a combinatorial OP with a number $n = N!$ of possible solutions. For the benchmarks we use several instances of symmetric TSP (distances $x_i \rightarrow x_j$ and $x_j \rightarrow x_i$ are equal) from a publicly available and broadly used benchmark set TSPLIB95~\footnote{TSPLIB95 website:~\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/}}. The advantage of choosing this benchmark set lays in a broad compatibility of solvers and frameworks with the proposed standardized problem instance description (including used jMetal and jMetalPy). The TSP in this case is defined as a set of city coordinates therefore. Thus, before starting to solve a problem, the distance matrix should be built, calculating Euclidean distances between cities. For more detailed explanation of TSPLIB95 problem instance files refer to~\cite{reinelt1995tsplib95}.

For our benchmarks we select four problem instances from a simpler case harder they are: \emph{kroA100}, \emph{pr439}, \emph{rat783} and \emph{pla7397} of sizes 100, 439, 783 and 7397 cities respectively. The optimal tours for each of these problem instances were previously obtained by exact solvers and reported in aforementioned library. While presenting our evaluation results, we refer to them therefore, we present the optimal solution results on \cref{eval:table:tsp optimal tour length}.

\begin{table}[h!]
	\centering
	\begin{tabular}{c||c}
		\textbf{TSP instance} & \textbf{Optimal tour length} \\
		\hline
		\hline
		kroA100 & 21282 \\
		pr439 & 107217 \\
		rat783 & 8806 \\
		pla7397 & 23260728 \\
	\end{tabular}
	\caption{TSP instances optimal tour length.}
	\label{eval:table:tsp optimal tour length}
\end{table}

Note, that the goal of this thesis and the evaluation in particular is not to beat the exact solvers in any case, but to investigate the applicability of proposed generic parameter control concept.

\section{Environment Setup}\label{eval: environment}
To run our experiments we use an enhanced by our approach BRISEv2 and deploy it in Docker containers on a single host machine with following characteristics:
\begin{itemize}
	\item \textbf{Hardware:} Fujitsu ESPRIMO P958 computer with 64GB 2667MHz RAM (16GB * 4 pcs), Intel Core i7-8700 CPU @ 3.2 GHz (6 cores * 2 threads) and Samsung 1TB SSD.
	
	\item \textbf{Software:} GNU/Linux Fedora 29 host OS and installed docker version 1.13.1.
\end{itemize}

We deploy 6 homogeneous BRISEv2 workers with LLHs on the same host machine to carry the problem solving process.
We run each experiment 9 times for 15 minutes to obtain the statistical data.


\section{Meta-heuristics Tuning}\label{eval: mh tuning}
As we conclude in \cref{bg: parameter setting conclution}, the goal of parameter control is to reach at least the quality of parameter tuning approaches. Therefore, before running the major set of evaluation experiments, we have to perform a parameter tuning for the underlying LLHs.

\subsection{Parameter Tuning System Configuration.} 
As a tuning system, we used our the implemented concept but in the tuner mode. As we described in \cref{concept: conclution}, to enable the parameter tuning mode, we built a search space based on the singe LLH with its parameters. In our particular case it were three search spaces for each underlying meta-heuristic respectively. We also disabled the solution transfer between each configuration, forcing LLH to use the OP each time from scratch.

For each LLH we run the tuning for 8 hours on 10 deployed worker nodes and three minutes for task evaluation. The underlying prediction mechanism was configured to use TPE with 100\% window size. We also disabled the repetition strategy (\emph{repeater} entity), leaving each configuration evaluated only once (with one task). We do so since our preliminary experiments have shown that the variance among evaluations is negligible. As one may expect, since the repetition strategy was disabled, outliers detection was turned off as well.

\subsection{Target Optimization Problem and Search Space of Parameters.} 
The role of target optimization problem was played by one of evaluated TSP instances: \emph{rat783}. We selected this instance because, it is a middle size problem, comparing all the used through evaluation OPs.

\paragraph{jMetalPy evolution strategy.} This meta-heuristic is implemented in a framework as na\"ive evolution strategy however, we found an important recombination mechanism missing therefore, the heuristic is performing mostly by means of the mutation operations. As a configuration, this ES implementation requires providing several hyper-parameters. Integer $\mu$ (\emph{mu}), which denotes the number of parents in the population, while integer $\lambda$ (\emph{lambda}) defines the number of offspring. We tune both parameters in ranges $[1..1000]$. Boolean \emph{elitist} defines the selection strategy, which true value enables elitist selection $(\mu+\lambda)$, while false disables the elitist selection $(\mu,\lambda)$ (more detains in \cref{BG: MH Examples}). Also, the framework proposes two possible \emph{mutation types} for combinatorial OPs: permutation swap and scramble mutation, which we use for tuning. The respective mutation probability is tuned in range $[0..1]$.

\paragraph{jMetalPy simulated annealing.} In this meta-heuristic authors defined the solution neighborhood by means of the same mutation operators, mentioned above. Thus, we use them and the same mutation probability range for tuning the SA. Unfortunately, the authors did not provide other but exponential cooling schedule and did not expose parameters temperature or alpha. This is the reason of such tiny parameter space for this MH.

\paragraph{jMetal evolution strategy.} The set of exposed hyper-parameters is almost the same, as we described for the Python-based MH implementation. The only difference that the mutation is represented only by one type, therefore we exclude it from the parameter space but leaving the mutation probability. All the other parameter ranges are the same as for the defined above ES.


\subsection{Parameter tuning results.} 
The process of parameter tuning is depicted in \cref{eval:pict:mh tuning}. During the session each MH was probed with at least $1.5k$ configurations. 


\svgpath{{graphics/Eval/tuning}}
\begin{figure}[h!]
	\centering
	\includesvg[width=\textwidth]{tuning progress}
	\caption{The low level heuristics parameter tuning process.}
	\label{eval:pict:mh tuning}
\end{figure}

In the figures below we propose a visual analysis of the parameter tuning results. For each meta-heuristic we separately present the numerical and categorical parameters.

The numeric hyper-parameters are showed as scattered points of parameter value (\emph{x-axis}) and the respective objective function result (\emph{y-axis}), obtained for configuration with this parameter value. Although such an isolated approach to analyze data in some cases may be error-prone, still it enough to get a birds-eye view on the existing dependencies. To represent trends among numeric parameter values we draw the regression line ($4^{th}$ degree) in green. At the top and to the right of the graph presented also the axis value densities. Thus, the density on a right side shows which objective values and how often were obtained, changing the underlying parameter, while the density on the top shows which parameter values were selected more often.

As for the categorical parameters, we plot their values as violin plots. It is a combination of box plot with the addition of a kernel density plot on each side. Since in our case, all categorical parameters of underlying algorithms have only two values, each violin plot shows which results of an objective function and how often were obtained. Using colors we depict different value of underlying parameter, while the shape of violin shows an expected result value and its probability. Inside the figure we also draw three dashed lines. A middle line with long dashes is a median, while lower and upper lines with short dashes show first and third quartiles respectively.


\paragraph{jMetalPy evolution strategy parameters.}
\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy numeric parameters}
	\caption{jMetalPy evolution strategy numeric parameters values.}
	\label{eval:pict:jmetalpy es numeric}
	\vspace{-20pt}
\end{figure}

\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy categorical parameters}
	\caption{jMetalPy evolution strategy categorical parameters values.}
	\label{eval:pict:jmetalpy es categoric}
	\vspace{-20pt}
\end{figure}

Looking on the \cref{eval:pict:jmetalpy es numeric}, one will see an explicit dependency between the number of parents (\cref{eval:pict:jmetalpy es numeric} parameter \emph{mu}) and the objective function: less amount of parents are tended to produce the better results. However, the dependency is such clearly observable for the number of offspring (\cref{eval:pict:jmetalpy es numeric} parameter \emph{lambda}). We may see, that a high number of offspring does not tend to provide good results, but the number of performed estimations for low \emph{lambda} is not enough to be strongly ensured that this value is better. Yet, even with small amount of observations we may make a guess that low \emph{lambda} is a good parameter choice. With respect to the mutation probability, it may be observed, that the higher mutation rates tend to produce a better results. 

As for the categorical parameters, one may see a strong bias towards bad results when using non-elitist algorithm version (\cref{eval:pict:jmetalpy es categoric} parameter \emph{elitist}). When concerning the mutation type, the dominance is not an obvious, but permutation version of mutation is slightly outperforms scramble type (\cref{eval:pict:jmetalpy es categoric} parameter \emph{mutation}).


\paragraph{jMetalPy simulated annealing parameters.}
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{jMetalPy simulated annealing numeric parameters}
		\caption{Mutation probability.}
		\label{eval:pict:jmetalpy sa numeric}
	\end{subfigure}
	\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{0.4\textwidth}
		\includesvg[width=\textwidth]{jMetalPy simulated annealing categorical parameters}
		\vspace{-5pt}
		\caption{Mutation type.}
		\label{eval:pict:jmetalpy sa categoric}
	\end{subfigure}
	\caption{jMetalPy simulated annealing parameters.}
\end{figure}


This heuristic were tuned by means of only two parameters: categorical mutation type, which results are presented on \cref{eval:pict:jmetalpy sa categoric} and numerical mutation probability with graphs on \cref{eval:pict:jmetalpy sa numeric}. One may see a strong dominance of permutation mutation type, while scramble produce an average but stable results. The mutation probability trends are also clear: higher parameter values produce better results. Indeed, the dependency on mutation probability is obvious, since the underlying algorithm is performing the search space traversal by means of solution mutation. The two lines of results, that could be viewed on the \cref{eval:pict:jmetalpy sa numeric} are correlated with the mutation type: lower corresponds to usage of permutation, while upper to scramble mutation.


\paragraph{jMetal evolution strategy parameters.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{jMetal evolution strategy numeric parameters}
	\caption{jMetal evolution strategy numeric parameters values.}
	\vspace{-15pt}
	\label{eval:pict:jmetal es numeric}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{jMetal evolution strategy categorical parameters}
	\label{eval:pict:jmetal es categoric}
	\caption{jMetal ES elitist parameter.}
	\vspace{-30pt}
\end{wrapfigure}

The final heuristic under investigation is the Java-based implementation of viewed above ES. Even if at the first glance the regression lines are not looking the same, the overall trends are similar: lower values of \emph{mu} parameter result in better objective, while the mutation probability should be kept high. In a contrary to Python-based ES, here the middle-range values of parameter \emph{lambda} produce the best results. It may be explained by the fact of performance straggling in Python-based version: with large offspring number, the computational effort, required to accomplish the iteration increases, while Java-based version could handle it. A dominance of elitist version of algorithm is non-obvious, but this could be seen from a distribution first quartile.

\paragraph{}
We collected the best performing configurations of each meta-heuristic and presented them in \cref{eval: params jmetalpy es}. We also highlight here the default parameter values, which were selected with motivation of being in the middle of the values ranges.

\begin{table}%[h!]
	\centering
	\begin{tabular}{r||c|c|c}
		\textbf{Hyper-parameter} & \textbf{Default value} & \textbf{Tuned value} & \textbf{Estimated range} \\
		\hline
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 22 & $[1..1000]$ \\
		\emph{elitist} & False & True & {True, False} \\
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy simulated annealing} \\
		\hline
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.89  & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetal evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 605 & $[1..1000]$ \\
		\emph{elitist} & False & True  & {True, False}\\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$ \\
	\end{tabular}
	
	\caption{Static hyper-parameters of low-level meta-heuristics.}
	\label{eval: params jmetalpy es}
\end{table}




\section{Concept Evaluation}\label{eval: concept}

\subsection{Evaluation Plan}\label{eval: concept plan}
To evaluate the performance of developed approach we firstly need to define the base line. In most cases it is the single meta-heuristics, which are solving the OP using static hyper-parameters. However, to evaluate the parameter control feature we must make a closer look on the performance of each separate heuristic with static and dynamic hyper-parameters. For selection hyper-heuristic analysis we compare the performances of all underlying MHs running separately and together within a hyper-heuristic. Note, in this case the hyper-parameters are statically defined. And last, but not least, to evaluate a selection hyper-heuristic with enabled parameter control we compare it to separately running underlying meta-heuristics with parameter control and to selection hyper-heuristic.

In order to organize the evaluation plan, we distinguish two stages.
At the first stage the LLH selection occurs, while at the second one we chose hyper-parameters for the selected LLH. At each stage we may use different prediction approaches, which description could be found in \cref{impl: prediction models}. To select the LLH, apart from random and static selection we also use FRAMAB (see \cref{impl: FRAMAB}) and Bayesian ridge regression model implementation from Scikit-learn framework (see \cref{impl: sklearn wrapper}). Note, for the Bayesian ridge regression model we use a default parameters, which could be found in the framework documentation~\footnote{~\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html}}. To select the hyper-parameters for LLHs, apart from static default and tuned variants we also use random selection, available in BRISEv2 TPE and the mentioned above Bayesian ridge. The set of used techniques is presented in the \cref{eval: concept settings table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{l||l}
		\textbf{LLH selection} & \textbf{LLH parameters selection} \\
		\hline
		\hline
		\textbf{1.} Random & \textbf{1.} Default \\
		\textbf{2.} Multi-armed bandit & \textbf{2.} Tuned beforehand \\
		\textbf{3.} Bayesian ridge regression & \textbf{3.} Random \\
		\textbf{4.1.} Static jMetalPy.ES & \textbf{4.} Tree Parzen Estimator (TPE) \\
		\textbf{4.2.} Static jMetalPy.SA & \textbf{5.} Bayesian ridge regression (BRR) \\
		\textbf{4.3.} Static jMetal.ES & 
	\end{tabular}
	
	\caption{Prediction techniques used for the concept evaluation.}
	\label{eval: concept settings table}
\end{table}


Using this table, we now could pick a prediction technique to form a desired system configuration. For instance, mentioned above baseline could be encoded into configurations starting from \emph{4.1.1} for the evolution strategy from jMetalPy framework, running with default hyper-parameters and ending with \emph{4.3.2} for evolution strategy from jMetal framework, running with tuned beforehand parameters.

Our benchmark plan for the concept evaluation looks as a set of following experiment groups:
\begin{itemize}
	\item \textbf{Meta-heuristics (MH).} The baseline. We evaluate each used meta-heuristic separately with default and tuned hyper-parameters: \emph{4.1.1} and \emph{4.1.2} for jMetalPy evolution strategy;  \emph{4.2.1} and \emph{4.2.2} for jMetalPy simulated annealing; \emph{4.3.1} and \emph{4.3.2} for jMetal evolution strategy respectively.

	\item \textbf{Meta-heuristics with parameter control (MH-PC).} The set of experiments dedicated to verify an impact of the generic parameter control on meta-heuristics performance. A selected set of experiments looks as follows: \emph{4.1.3, 4.2.3, 4.3.3} to investigate the influence of random parameter allocation; \emph{4.1.4, 4.2.4, 4.3.4} to check TPE-based parameter control and \emph{4.1.5, 4.2.5, 4.3.5} to probe Bayesian-ridge-based parameter control.

	\item \textbf{Selection hyper-heuristic with static parameters (HH-SP).} These benchmarks are dedicated to an investigation of the implemented on-line selection HH performance. It implies the LLHs usage with static parameters therefore, we evaluate HH-SP performance with default and tuned beforehand LLH parameters. Experiment codes are following: \emph{2.1, 2.2} for FRAMAB-based HH-SP and \emph{3.1, 3.2} for Bayesian-ridge-based HH-SP.
	
	\item \textbf{Selection hyper-Heuristic with parameter control in LLH (HH-PC).} This is a final set of benchmarks for concept evaluation. By this we evaluate an influence of simultaneous on-line LLH selection and parameter control on system performance. The respective experiment set is following: \emph{1.3, 2.4, 2.5, 3.4, 3.5.}
\end{itemize}

The aggregated concept benchmark plan is presented in \cref{eval: concept benchmark plan table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{c||p{3cm}}
		\textbf{Experiment group} & \textbf{Related codes} \\
		\hline
		\hline
			
		\multirow{2}{*}{MH} & 4.1.1., 4.2.1, 4.3.1 \newline 4.1.2, 4.2.2, 4.3.2 \\
		
		\rowcolor{gray!10}
		\multirow{3}{*}{MH-PC} & 4.1.3, 4.2.3, 4.3.3 \newline 4.1.4, 4.2.4, 4.3.4 \newline 4.1.5, 4.2.5, 4.3.5 \\
		
		\multirow{3}{*}{HH-SP} & 1.1, 1.2 \newline 2.1, 2.2 \newline 3.1, 3.2 \\

		\rowcolor{gray!10}
		\multirow{3}{*}{HH-PC} &  1.3 \newline 2.4, 2.5 \newline 3.4, 3.5 \\
	\end{tabular}
	
	\caption{Concept benchmark plan.}
	\label{eval: concept benchmark plan table}
\end{table}


\subsection{Concept Evaluation Results}\label{eval: concept results}
\subsubsection{Baseline Evaluation}\label{eval: concept baseline}
As we discussed previously, our results comparison should be done against the defined baseline. Therefore, this section is dedicated to review of the meta-heuristics performance out-of-the-box on different problem sizes and parameter settings. For visibility reasons we plot the intermediate and the final performance evidences for each problem instance separately, since they naturally imply different result ranges.

Since we are tackling a set of TSP instances, which were previously solved by other exact solvers, we also present an optimal solution, available for each instance as a green dashed line.

%\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/baseline}}
\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{kroA100 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on kroA100.}
	\label{eval:pict:bl:kroA100 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{kroA100 baseline final boxplot}
	\label{eval:pict:bl:kroA100 final}
	%\ffigbox[\FBwidth]
	\caption{Final results of meta-heuristics with static parameters on kroA100.}
	\vspace{-20pt}
\end{wrapfigure}
The TSP for 100 cities is a relatively small problem instance. Therefore, all underlying MHs reach a local optimum after first few minutes of the run and stuck there till the end, making a relatively small moves (\cref{eval:pict:bl:kroA100 intermediate}). Note, the bold line is a statistical mean of all 9 experiment runs, while a shadow around it is a confidence interval. One may observe how the parameter tuning affects different MHs: in some the difference is dramatic (ES), while others are almost not affected (\cref{eval:pict:bl:kroA100 final}).

An observation of a worse SA results with tuned parameters, in contrast to default values is explained by the fact that for algorithm tuning we used different problem instance (rat783). It only confirms a motivation of the parameter control approaches: tuning is not problem-instance-universal technique.

\newpage
\paragraph{pr439 TSP instance.} 
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pr439 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pr439.}
	\label{eval:pict:bl:pr439 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{pr439 baseline final boxplot}
	\label{eval:pict:bl:pr439 final}
	\caption{Final results of meta-heuristics with static parameters on pr439.}
	\vspace{-20pt}
\end{wrapfigure}
The next problem instance comprises a 4 hundred cities. Since the number of possible solutions increased, the required time for MHs to settle in a local optima is increased as well. We may see a similar to the previous experiment set trends: produced by SA results are almost not affected by the parameter tuning, however with tuned parameters the performance is slightly better. From the other side, Python version of tuned ES requires more time to converge in local optima, while Java-based reaches it after a couple of first iterations (\cref{eval:pict:bl:pr439 intermediate}).

The final result quality of underlying MHs is the same as on previously reviewed problem instance: MHs with tuned parameters (an SA with default) produce solutions of a similar quality (\cref{eval:pict:bl:pr439 final}).

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{rat783 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on rat783.}
	\label{eval:pict:bl:rat783 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{rat783 baseline final boxplot}
	\label{eval:pict:bl:rat783 final}
	\caption{Final results of meta-heuristics with static parameters on rat783.}
	\vspace{-20pt}
\end{wrapfigure}
This is an average size problem among reviewed in this thesis. The behavior of solvers in this case changes slightly. For instance, ES from jMetal framework with default parameters is unable to solve a problem however, while using an optimized hyper-parameters it quickly reaches a local optima (after ~50 iterations) with the best produced results (\cref{eval:pict:bl:rat783 intermediate}). Analyzing performance evidences of the other heuristics (Python-based), we may conclude that they did not reach a local optima in a given 15 minutes therefore, their final results are slightly worse (\cref{eval:pict:bl:rat783 final}). Taking into account previous problem instances we may guess that given enough time they will reach the results of jMetal ES. Note the decreased stability of jMetalPy ES reflected by a large confidence interval.

\newpage
\paragraph{pla7397 TSP instance.} 
The largest investigated here TSP instance for ~$7.4k$ cities is however, referred as a middle-size OP in used TSPLIB95. Here the performance evidences changed the most therefore, we discuss each MH separately.

\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pla7397 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pla7397.}
	\label{eval:pict:bl:pla7397 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{pla7397 baseline final boxplot}
	\label{eval:pict:bl:pla7397 final}
	\caption{Final results of meta-heuristics with static parameters on pla7397.}
	\vspace{-20pt}
\end{wrapfigure}

Python-based version of ES provide the worst results with both default and tuned parameters. Note the amount of performed iterations by this MH in a given 15 minutes â€” less than 100. It is affected by a several reasons. Firstly, the amount of time required to perform an internal iteration increased dramatically. Thus, even with specified 15 seconds for one task run, actually it requires much more (up to 1 minute) to accomplish a task. We have a several guesses, what may cause such a behavior. Firstly, it is a general Python performance issues, in most of the times caused by usage of global interpreter lock (GIL), which explanation is out of this thesis scope. Secondly, it may be caused by the algorithms code basis implementation in jMetalPy framework. To implement a generic termination criteria (and some other features) the authors utilized a push-observer design pattern~\cite{benitez2019jmetalpy} according to which the underlying algorithm (ES in our case) triggers its observers after each iteration. Therefore, stopping criteria may be evaluated only after finishing an iteration, which in case running ES with TSP for $7.4k$ cities, may take a while. We observed the ES algorithm termination after the first internal iteration, which causes a poor solution quality improvement. Naturally, there is also an overhead for the results sending through the network, but as Java-based ES with default parameter shows, this causes decrease only in ~100 configurations. We also eliminate the possible issue in a required time for problem loading (building distance matrix), since the worker node does it only once and stores its cached version. In any case, this issue requires deeper investigation, that we postpone to the future work. Running jMetalPy ES with tuned parameters fixes the issue with task delays and therefore, results in higher number of external iterations, but the solution improvement are still weak (see left picture on the \cref{eval:pict:bl:pla7397 intermediate}).

As in the previous case, the jMetalPy-based simulated annealing produce a good quality improvement, least depending on the hyper-parameter values. A resulting progress curve, presented in a central picture of \cref{eval:pict:bl:pla7397 intermediate} shows that SA requires more time to converge that was provided and is still far from its potential local optima.

The final evidences obtained from the jMetalPy ES show it dominance in intermediate and final results (see \cref{eval:pict:bl:pla7397 intermediate} and \cref{eval:pict:bl:pla7397 final} respectively). In this example we observe a dramatic impact of the solver parameter tuning. Using default configuration, ES is struggling in making improvements. Our guess here is the same as for the Python-based version â€” the number of internal iterations is extremely low a good search space traversal, but they are finishing much faster.


\paragraph{Discussion.} The observed results of meta-heuristics execution confirm the algorithm parameter setting problem importance, discussed in \cref{bg: section Parameters Setting}. An effect of proper parameter selection is different among algorithms. In our case, the performance of two out of three solvers are highly dependent on the hyper-parameter settings. Thus, an application of our generic parameter control approach to these algorithms is rather intriguing and may partially reveal the overall methodology benefits.

From the other side, we also observe the only one algorithm domination among the others with static parameters. See how all MHs were solving each TSP instance with default parameters. In each case SA outperforms two other ES. The usage of all three MHs in a selection hyper-heuristic with static (default) hyper-parameters will reveal the implemented selection HH applicability. In this case we expect to observe the results close to provided by pure SA. The other case â€” the application of MHs to the biggest TSP instance with tuned hyper-parameters. Here Java-based ES is the best. Thus, we expect to observe such a behavior of selection hyper-heuristic. Also, it should be rather interesting to see the impact of Python ES struggling with default parameters on HH.

%\newpage
\subsubsection{Generic Parameter Control}
As we discussed in \cref{bg: parameter control}, the goal of parameter tuning lays in adaptive changing of underlying algorithm parameters with to optimize some performance measurement. In our case, we apply the proposed in \cref{Concept description} approach to set the parameters of anytime algorithms. Here is a brief reminder: at each RL step HLH is analyzing the performance of solver with previous configurations to choose the parameters values, which hopefully lead to the higher solution quality improvements. Afterwards, we run the solver with sampled parameters for a predefined time (15 seconds) to get new evidences. 

Here we compare the performance of algorithms with statically defined default and tuned hyper-parameters to dynamically changing parameter values by means of RL control. As previously, we perform the comparison for each problem instance separately.

\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/control}}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{kroA100 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on kroA100.}
	\label{eval:pict:pc:kroA100 intermediate}
\end{figure}

Comparing to the baseline, parameter control in a small problem instance was able to reach and even outperform the results of MHs on static parameters after first 50 iterations (\cref{eval:pict:pc:kroA100 intermediate}). We may observe, that even randomly changing the parameters of heuristics (however, this random sample still emits valid configurations) in a runtime results in better solutions comparing with static parameters. It is caused by the changes in a neighborhood definition (mutation type) and traversal process (mutation probability). In most cases, given enough time the learning-based parameter assigning outperforms random allocation (\cref{eval:pict:pc:kroA100 final}).

Note the amount of configurations (iterations) performed by jMetal ES, which could be seen on \cref{eval:pict:pc:kroA100 intermediate}. According to our plan, a given time for MH run is 15 minutes, 15 seconds for running one configuration on 6 available workers. Thus, in the most optimistic case, the number of iterations should be $ \frac{15\cdot60\cdot6}{15} = 360$ but, we observe even more than 400. After an investigation, we came to conclusion that it is caused by an implementation flaw an insight of which is following. jMetal MHs provide only iteration-number-based termination criterion, which is not encapsulated how it is done in jMetalPy. For our needs we added also a time-based but did not remove the previously existing. For the iteration counter used a regular integer number, which we set up to its maximal value when using a time-based criterion. Given a specific `light' algorithm configuration (low $\mu$, $\lambda$ and mutation probability), with this OP MH is able to reach the maximal number of iteration in less than 15 seconds therefore, terminating early and triggering a new parameter control iteration. Certainly, it is our implementation bug, fix of which we postpone to the future work.

%Let us have a closer look on the jMetal ES. Two observations could be made. Firstly, the stability of results in case of random-based parameter tuning weak in comparison to model-based, which is a reasonable behavior. However, in case of jMetalPy ES, the TPE-based stability is less than random-based. This leads us to 

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{kroA100 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on kroA100.}
	\label{eval:pict:pc:kroA100 final}
\end{figure}

\newpage
\paragraph{pr439 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pr439 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on pr439.}
	\label{eval:pict:pc:pr439 intermediate}
\end{figure}

When parameter control is applied to a larger problem, it is starting to require more time for finding a better quality configurations for jMetalPy ES. In particular, only the TPE-based approach was able to reach tuned parameters results quality. The intermediate performance was reduced, since model-based tuning required more knowledge to find a good-performing settings. However, looking on the progress curves on \cref{eval:pict:pc:pr439 intermediate}, RL TPE-based control technique will probably be able to reach and outperform tuned beforehand parameters.

The case with jMetalPy SA shows (1) reduction in the final results qualities with controlled parameters (\cref{eval:pict:pc:pr439 final}) and moreover (2) unstable behavior of MH with TPE-based control (\cref{eval:pict:pc:pr439 intermediate}). As we concluded during the baseline results analysis, the parameter settings in this MH does not dramatically affect its performance therefore, the results of random-based parameter allocation are similar to model-based approaches.

On a contrary, applying generic parameter control to jMetal ES MH leads to a better final results, comparing with static tuned parameters(\cref{eval:pict:pc:pr439 final}). Note, as for previous problem instance, even a random-based MH parameters allocation outperforms statically defined but, the required time to converge and a result floating increased as well. 
\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{pr439 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on pr439.}
	\label{eval:pict:pc:pr439 final}
\end{figure}

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{rat783 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on rat783.}
	\label{eval:pict:pc:rat783 intermediate}
\end{figure}

This is a problem instance, which was used to set the meta-heuristic parameters by means of off-line tuning. In this case, the parameter control in jMetalPy SA and jMetal ES did not manage to outperform the tuned parameters but only nearly reached their quality. All approaches, including the random selection resulted in a similar intermediate performance (\cref{eval:pict:pc:rat783 intermediate}).

For jMetalPy ES MH, only TPE-based parameter control produced good but rather unstable final performance, comparing to tuned hyper-parameters. The other (random- and BRR-based) approaches did not manage to select the parameters, which would perform well enough and therefore were left out of presented quality ranges on \cref{eval:pict:pc:rat783 final}.

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{rat783 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on rat783.}
	\label{eval:pict:pc:rat783 final}
\end{figure}

\newpage
\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pla7397 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on pla7397.}
	\label{eval:pict:pc:pla7397 intermediate}
\end{figure}

For the final problem instance we omit the parameter control results of jMetalPy ES since it did not manage to perform even slightest improvement in comparison to the default parameter values, presented in a baseline description. It is caused by a fact that in early stages our approach acts as a random search, since not enough evidences were obtained to build models for prediction. Thus, is case of jMetalPy ES, the MH was running with badly performing configurations and as we explained in \cref{eval: concept baseline}, did not manage to perform enough iterations to improve solution quality and was left out of this instance discussion. To resolve this issue, the time-based task termination methodology should be properly implemented in MH wrappers, but we postpone it for the future work.

As in previous cases, the least parameter-settings-sensitive jMetalPy SA shows an ability to perform well with any approach of the parameter settings (\cref{eval:pict:pc:pla7397 intermediate}). However, neither among them outperform tuned beforehand algorithm configuration in final results quality (\cref{eval:pict:pc:pla7397 final}).

As for jMetal ES, model-based approaches are outperforming the randomized parameter values allocation. Moreover, TPE model outperforms even the results of algorithm with tuned beforehand hyper-parameters.

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{pla7397 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on pla7397.}
	\label{eval:pict:pc:pla7397 final}
\end{figure}

\newpage
\paragraph{Discussion.} In general, the review of meta-heuristic performance on different problem instances showed that the proposed generic parameter control approach is applicable. It is capable to improve a final algorithm performance in comparison to statically selected default parameters. Moreover, in some cases (all MHs with kroA100, jMetal ES with pr439 and pla7397) the algorithm showed better results than with tuned parameters.

Taking into account the results with random parameter allocation we conclude that learning mechanisms should and must be improved further by means of different surrogate models usage or proper optimization over surrogates. Leaving the improvement steps to future work we conclude that the proposed generic parameter control concept is able to produce better results while solving an unforeseen problem on-line.


\subsubsection{Selection Hyper-Heuristic with Static LLH Parameters}
The second mode of developed approach and at the same time a main goal of the thesis is a process of dynamic algorithm selection. It is implemented in form of described in \cref{Concept description} reinforcement learning-based on-line selection hyper-heuristic. In this part of evaluation we combine three available meta-heuristics with static parameter (default and tuned) into a single selection hyper-heuristic. Three approaches to select the LLH were investigated, in combination to two possible parameters settings it results in 6 combinations for each problem instance.

We present the process of problem solving in two forms. Firstly, we present the process of problem solving, distinguishing selected at each iteration LLH and the results, which it gave. For doing it, we selected only the first repetition (out of 9 available), since presenting all repetitions will not be possible to understand. Nevertheless, we present the final results of all runs in form of box-plots, comparing them to the underlying LLH performance. LLHs solely and combined into the selection hyper-heuristics are configured to use a static hyper-parameter values (HH-SP). On the left side we present final performance with the default parameter values, while on the right site â€” the tuned beforehand values.



\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/selection}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on kroA100 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:kroA100 intermediate}
\end{figure}
The intermediate results with random LLH allocation show as, as expected, unbiased uniform selection of LLH (codes 1.1 and 1.2). We also observe the same behavior with all HH-SP approaches, while we utilize tuned parameter values (codes 1.2, 2.2, 3.2). It is motivated by an equal performance of all LLHs on this problem instance (see baseline evaluation in \cref{eval: concept baseline}). Therefore, a point of interest here are default parameter values(codes 2.1, 3.1. in \cref{eval:pict:hh-sp:kroA100 intermediate}). When we change the HLH to FRAMAB, a bias appears towards jMetal evolution strategy (j.ES) and jMetalPy simulated annealing (py.SA) LLHs. In case of BRR usage, the bias dominates in j.ES however, in both experiments j.ES does not provide dominating results. Our guess of this behavior explanation is the overall instance solving process: after a couple of first py.SA allocations, the solving process gets stuck in a local optimum (see \cref{eval: concept baseline}). Therefore, HH-SP increases exploration and usage of other LLHs, which we observe.

According to presented on \cref{eval:pict:hh-sp:kroA100 final} final results statistics for this problem instance, given at least one dominating LLH (default parameters values case), even random selection HH-SP could utilize it enough times to obtain a good results.

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on kroA100 (statistics of 9 experiment results).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:kroA100 final}
\end{figure}

\newpage
\paragraph{pr439 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on pr439 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pr439 intermediate}
\end{figure}
Evaluating the 439 cities instance solving process, we observe a similar behavior while using tuned hyper-parameters therefore, we do not draw our attention here: every LLH reach a local optimum in a couple of first iterations and no dominant LLH may be distinguished.

In a contrary, when using the default parameter values a bias towards py.SA should occur, since it showed the best results improvement during the baseline evaluation. However, in our observations the FRAMAB-based HH-SP (code 2.1) frequently allocates two heuristics: py.SA and j.ES, while BRR-based (code 3.1) performs as expected allocating mostly py.SA (\cref{eval:pict:hh-sp:pr439 intermediate}). Comparing to the previous problem instance, BRR dramatically changed its behavior to utilize in the most cases the best performing py.SA. Thus, we can conclude the BRR HLH provides more exploitation capabilities, when compared to FRAMAB HLH.

The final result quality of all HH-SP on this problem instance are roughly similar to provided by the best performing py.SA with default parameters. Once again, all tuned LLHs separately provide a similar final result quality therefore, their combination in HH-SP leads to the same final quality (\cref{eval:pict:hh-sp:pr439 final}).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on pr439 (statistics of 9 experiment results).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pr439 final}
\end{figure}

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on rat783 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:rat783 intermediate}
\end{figure}
With the 783 cities instance heuristics are starting to perform in a slightly different manner even with the tuned parameters. As you remember from baseline evaluation, here the best performing was j.ES, afterwards appeared py.SA and lastly â€” py.ES. As a consequence, using FRAMAB HLH, HH-SP frequently utilizes j.ES (see code 2.2 in \cref{eval:pict:hh-sp:rat783 intermediate}). In a contrary, BRR HLH utilizes all LLHs almost evenly. We may conclude that BRR was `confused' by performance evidences from other heuristics, since the process quickly converged into a local optima.

In the case with default parameter values, both learning models most frequently predicted the best performing py.SA, which is an expected behavior.

The final result quality of all model-based HP-SP are at least as good, as the solution quality provided by the best underlying LLH for both default and tuned parameter values (\cref{eval:pict:hh-sp:rat783 final}). Even in a case with the default parameters, a random allocation of one good performing LLH among three available (codes 1.1 and 1.2) results in a good final solution quality however, may require more time to converge (see intermediate and final performances for code 1.1).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on rat783 (statistics of 9 experiment results).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 final}
\end{figure}

\newpage
\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on pla7397 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pla7397 intermediate}
\end{figure}
Finally, the largest TSP instance tackled in this thesis. Our observations different MH heuristics on this problem were as following: py.ES with default parameters had the worst performance, while the tuned algorithm version was able to outperform only default parameters of j.ES. In a contrary, j.ES with tuned parameters produced the best results, outperforming py.SA. The Python version of simulated annealing with both parameter settings produced average results, which were better than default version of j.ES and both py.ES. All these performance evidences are presented in \cref{eval:pict:bl:pla7397 intermediate}. Therefore, here we expected to observe a high frequency of usage: (1) py.SA for default parameter case, and (2) j.ES for tuned parameters. Naturally, it holds only for learning-based HH-SP.


\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on pla7397 (statistics of 9 experiment results).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pla7397 final}
\end{figure}

The displayed chronic of LLHs allocation on \cref{eval:pict:hh-sp:pla7397 intermediate} completely match our expectations. In case of usage LLHs with default parameters, the most frequent choice of FRAMAB HLH was py.SA (code 2.1). We observe the same behavior of BRR HLH, but this model was also frequently selecting other LLHs such as py.ES(code 3.1). The results are happening to be unexpected, when comparing random allocation with BRR-based (codes 1.1 and 3.1): the frequent usage of py.ES in 1.1 `killed' the optimization process, while the same frequent MH calls in 3.1 were performing much better. Referring to the final results, presented on \cref{eval:pict:hh-sp:pla7397 final} we observe a high diverse in final result qualities when py.ES was allocated frequently (codes 1.1. and 3.1.). This is an unexpected behavior, especially for BRR HLH (code 3.1) that should be thoroughly examined, but due to the time limit we are forced to postpone the investigation for a future work.

As for the case with tuned LLHs, all selection approaches produced comparable to the best available LLHs final results. However, due to frequent usage of j.ES by FRAMAB and BRR, their final result quality slightly outperform random-based selection.


\paragraph{Discussion.} According to our observations of the developed selection hyper-heuristic performance we conclude that a proposed concept may be used in such way. It naturally requires not only a thorough investigation of previously seeing issues (pla7397 code 1.1, 3.1), but also a generic approach to handle a potential performance issues in LLH that may cause struggling of overall HH-SP execution. The implemented system should be evaluated by means of HLH configuration influence on the performance. Also, a further investigation of adding several new LLHs should be evaluated by means of required computation effort to find good LLHs vs pure performance of these LLHs.


\subsubsection{Selection Hyper-Heuristic with Parameter Control}

\paragraph{Results Description and Explanation}





\section{Hyper-Heuristic with Parameter Control Settings Evaluation}\label{eval: hh-pc}
\subsection{Evaluation Plan}\label{eval: hh-pc plan}
\subsection{Evaluation Results}\label{eval: hh-pc results}



\section{Conclusion}\label{eval: conclution}
