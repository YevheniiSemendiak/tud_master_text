\chapter{Evaluation}\label{eval}
The concepts, proposed in \cref{Concept description}, implemented in \cref{impl} of search space representation, prediction process and based on this generalized parameter control approach, selection hyper-heuristic and the hyper-heuristic with parameter control should be broadly evaluated. The experiments may be performed in number of investigation directions, starting from the developed RL performance evaluation with respect to system configuration and ending with the scalability to different problem sizes.

The structure of this Chapter is as follows. We start with the optimization problem presentation in \cref{eval: op}, a short environment description in \cref{eval: environment} and parameter tuning of low-level heuristics in \cref{eval: mh tuning}, which will be used later through our tests. The evaluation process of the proposed concept could be divided into two main parts. 

The first part is dedicated to the developed concept analysis in comparison to the baseline and is presented in \cref{eval:1}. We start the concept evaluation with experiments planning in \cref{eval:1:plan} and proceed firstly reviewing the baseline in \cref{eval:1:baseline}, afterwards the generic parameter control is presented in \cref{eval:1:PC}, followed by selection hyper-heuristic with static hyper-parameters in low-level heuristics review in \cref{eval:1:hh-sp} and finally, the selection hyper-heuristic with parameter control in low-level heuristics review is presented in \cref{eval:1:hh-pc}.

In the second part of evaluation we investigate an influence of hyper-heuristic with parameter control settings on its performance in \cref{eval:2}. For doing so once again we firstly perform the experiment planning in \cref{eval:2:plan}. Afterwards, in \cref{eval:2:learning granularity} we investigate the influence of a learning granularity on a HH-PC performance, in \cref{eval:2:learning models} we perform changes of a learning models configurations and in \cref{eval:2:llh changes} we check the influence of possible changes in LLH behavior.

Finally, \cref{eval: conclution} concludes our discussion of the obtained results.


\section{Optimization Problem}\label{eval: op}
Through this thesis we are tackling a vehicle routing problem â€” the traveling salesman OP, which explanation could be found in \cref{BG: subsection OPs}. Nevertheless, as a reminder we repeat its short definition here. We also include the other details, related to the benchmarks.

``Given a set of cities and the distances among them, find the shortest path, which visits all cities''. It is a combinatorial OP with a number $n = N!$ of possible solutions. For the benchmarks we use several instances of symmetric TSP (distances $x_i \rightarrow x_j$ and $x_j \rightarrow x_i$ are equal) from a publicly available and broadly used benchmark set TSPLIB95~\footnote{TSPLIB95 website:~\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/}}. The advantage of choosing this benchmark set lays in a broad compatibility of solvers and frameworks with the proposed standardized problem instance description (including used jMetal and jMetalPy). The TSP in this case is defined as a set of city coordinates therefore. Thus, before starting to solve a problem, the distance matrix should be built, calculating Euclidean distances between cities. For more detailed explanation of TSPLIB95 problem instance files refer to~\cite{reinelt1995tsplib95}.

For our benchmarks we select four problem instances from a simpler case harder they are: \emph{kroA100}, \emph{pr439}, \emph{rat783} and \emph{pla7397} of sizes 100, 439, 783 and 7397 cities respectively. The optimal tours for each of these problem instances were previously obtained by exact solvers and reported in aforementioned library. While presenting our evaluation results, we refer to them therefore, we present the optimal solution results in \cref{eval:table:tsp optimal tour length}.

\begin{table}[h!]
	\centering
	\begin{tabular}{c||c}
		\textbf{TSP instance} & \textbf{Optimal tour length} \\
		\hline
		\hline
		kroA100 & 21282 \\
		pr439 & 107217 \\
		rat783 & 8806 \\
		pla7397 & 23260728 \\
	\end{tabular}
	\caption{TSP instances optimal tour length.}
	\label{eval:table:tsp optimal tour length}
\end{table}

Please note that the goal of this thesis and the evaluation in particular is not to beat the exact solvers in any case, but to investigate the applicability of proposed generic parameter control concept.

\section{Environment Setup}\label{eval: environment}
To run our experiments we use an enhanced by our approach BRISEv2 and deploy it in Docker containers on a single host machine with following characteristics:
\begin{itemize}
	\item \textbf{Hardware:} Fujitsu ESPRIMO P958 computer with 64GB 2667MHz RAM (16GB * 4 pcs), Intel Core i7-8700 CPU @ 3.2 GHz (6 cores * 2 threads) and Samsung 1TB SSD.
	
	\item \textbf{Software:} GNU/Linux Fedora 29 host OS and installed docker version 1.13.1.
\end{itemize}

We deploy 6 homogeneous BRISEv2 workers with LLHs on the same host machine to carry the problem solving process.
We run each experiment 9 times for 15 minutes to obtain the statistical data.


\section{Meta-heuristics Tuning}\label{eval: mh tuning}
As we conclude in \cref{bg: parameter setting conclution}, the goal of parameter control is to reach at least the quality of parameter tuning approaches. Therefore, before running the major set of evaluation experiments, we have to perform a parameter tuning for the underlying LLHs.

\subsection{Parameter Tuning System Configuration.} 
As a tuning system, we used our the implemented concept but in the tuner mode. As we described in \cref{concept: conclution}, to enable the parameter tuning mode, we built a search space based on the singe LLH with its parameters. In our particular case it were three search spaces for each underlying meta-heuristic respectively. We also disabled the solution transfer between each configuration, forcing LLH to use the OP each time from scratch.

For each LLH we run the tuning for 8 hours on 10 deployed worker nodes and three minutes for task evaluation. The underlying prediction mechanism was configured to use TPE with 100\% window size. We also disabled the repetition strategy (\emph{repeater} entity), leaving each configuration evaluated only once (with one task). We do so since our preliminary experiments have shown that the variance among evaluations is negligible. As one may expect, since the repetition strategy was disabled, outliers detection was turned off as well.

\subsection{Target Optimization Problem and Search Space of Parameters.} 
The role of target optimization problem was played by one of evaluated TSP instances: \emph{rat783}. We selected this instance because, it is a middle size problem, comparing all the used through evaluation OPs.

\paragraph{jMetalPy evolution strategy.} This meta-heuristic is implemented in a framework as na\"ive evolution strategy however, we found an important recombination mechanism missing therefore, the heuristic is performing mostly by means of the mutation operations. As a configuration, this ES implementation requires providing several hyper-parameters. Integer $\mu$ (\emph{mu}), which denotes the number of parents in the population, while integer $\lambda$ (\emph{lambda}) defines the number of offspring. We tune both parameters in ranges $[1..1000]$. Boolean \emph{elitist} defines the selection strategy, which true value enables elitist selection $(\mu+\lambda)$, while false disables the elitist selection $(\mu,\lambda)$ (more detains in \cref{BG: MH Examples}). Also, the framework proposes two possible \emph{mutation types} for combinatorial OPs: permutation swap and scramble mutation, which we use for tuning. The respective mutation probability is tuned in range $[0..1]$.

\paragraph{jMetalPy simulated annealing.} In this meta-heuristic authors defined the solution neighborhood by means of the same mutation operators, mentioned above. Thus, we use them and the same mutation probability range for tuning the SA. Unfortunately, the authors did not provide other but exponential cooling schedule and did not expose parameters temperature or alpha. This is the reason of such tiny parameter space for this MH.

\paragraph{jMetal evolution strategy.} The set of exposed hyper-parameters is almost the same, as we described for the Python-based MH implementation. The only difference that the mutation is represented only by one type, therefore we exclude it from the parameter space but leaving the mutation probability. All the other parameter ranges are the same as for the defined above ES.


\subsection{Parameter tuning results.} 
The process of parameter tuning is depicted in \cref{eval:pict:mh tuning}. During the session each MH was probed with at least $1.5k$ configurations. 


\svgpath{{graphics/Eval/tuning}}
\begin{figure}[h!]
	\centering
	\includesvg[width=\textwidth]{tuning progress}
	\caption{The low level heuristics parameter tuning process.}
	\label{eval:pict:mh tuning}
\end{figure}

In the figures below we propose a visual analysis of the parameter tuning results. For each meta-heuristic we separately present the numerical and categorical parameters.

The numeric hyper-parameters are showed as scattered points of parameter value (\emph{x-axis}) and the respective objective function result (\emph{y-axis}), obtained for configuration with this parameter value. Although such an isolated approach to analyze data in some cases may be error-prone, still it enough to get a birds-eye view on the existing dependencies. To represent trends among numeric parameter values we draw the regression line ($4^{th}$ degree) in green. At the top and to the right of the graph presented also the axis value densities. Thus, the density on a right side shows which objective values and how often were obtained, changing the underlying parameter, while the density on the top shows which parameter values were selected more often.

As for the categorical parameters, we plot their values as violin plots. It is a combination of box plot with the addition of a kernel density plot on each side. Since in our case, all categorical parameters of underlying algorithms have only two values, each violin plot shows which results of an objective function and how often were obtained. Using colors we depict different value of underlying parameter, while the shape of violin shows an expected result value and its probability. Inside the figure we also draw three dashed lines. A middle line with long dashes is a median, while lower and upper lines with short dashes show first and third quartiles respectively.


\paragraph{jMetalPy evolution strategy parameters.}
\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy numeric parameters}
	\caption{jMetalPy evolution strategy numeric parameters values.}
	\label{eval:pict:jmetalpy es numeric}
	\vspace{-15pt}
\end{figure}

\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy categorical parameters}
	\caption{jMetalPy evolution strategy categorical parameters values.}
	\label{eval:pict:jmetalpy es categoric}
	\vspace{-20pt}
\end{figure}

Looking on the \cref{eval:pict:jmetalpy es numeric}, one will see an explicit dependency between the number of parents (\cref{eval:pict:jmetalpy es numeric} parameter \emph{mu}) and the objective function: less amount of parents are tended to produce the better results. However, the dependency is such clearly observable for the number of offspring (\cref{eval:pict:jmetalpy es numeric} parameter \emph{lambda}). We may see that a high number of offspring does not tend to provide good results, but the number of performed estimations for low \emph{lambda} is not enough to be strongly ensured that this value is better. Yet, even with small amount of observations we may make a guess that low \emph{lambda} is a good parameter choice. With respect to the mutation probability, it may be observed that the higher mutation rates tend to produce a better results. 

As for the categorical parameters, one may see a strong bias towards bad results when using non-elitist algorithm version (\cref{eval:pict:jmetalpy es categoric} parameter \emph{elitist}). When concerning the mutation type, the dominance is not an obvious, but permutation version of mutation is slightly outperforms scramble type (\cref{eval:pict:jmetalpy es categoric} parameter \emph{mutation}).


\paragraph{jMetalPy simulated annealing parameters.}
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{jMetalPy simulated annealing numeric parameters}
		\caption{Mutation probability.}
		\label{eval:pict:jmetalpy sa numeric}
	\end{subfigure}
	\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{0.4\textwidth}
		\includesvg[width=\textwidth]{jMetalPy simulated annealing categorical parameters}
		\vspace{-5pt}
		\caption{Mutation type.}
		\label{eval:pict:jmetalpy sa categoric}
	\end{subfigure}
	\caption{jMetalPy simulated annealing parameters.}
\end{figure}


This heuristic were tuned by means of only two parameters: categorical mutation type, which results are presented in \cref{eval:pict:jmetalpy sa categoric} and numerical mutation probability with graphs in \cref{eval:pict:jmetalpy sa numeric}. One may see a strong dominance of permutation mutation type, while scramble produce an average but stable results. The mutation probability trends are also clear: higher parameter values produce better results. Indeed, the dependency on mutation probability is obvious, since the underlying algorithm is performing the search space traversal by means of solution mutation. The two lines of results that could be viewed on the \cref{eval:pict:jmetalpy sa numeric} are correlated with the mutation type: lower corresponds to usage of permutation, while upper to scramble mutation.


\paragraph{jMetal evolution strategy parameters.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{jMetal evolution strategy numeric parameters}
	\caption{jMetal evolution strategy numeric parameters values.}
	\vspace{-15pt}
	\label{eval:pict:jmetal es numeric}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{jMetal evolution strategy categorical parameters}
	\label{eval:pict:jmetal es categoric}
	\caption{jMetal ES elitist parameter.}
	\vspace{-30pt}
\end{wrapfigure}

The final heuristic under investigation is the Java-based implementation of viewed above ES. Even if at the first glance the regression lines are not looking the same, the overall trends are similar: lower values of \emph{mu} parameter result in better objective, while the mutation probability should be kept high. On contrary to Python-based ES, here the middle-range values of parameter \emph{lambda} produce the best results. It may be explained by the fact of performance straggling in Python-based version: with large offspring number, the computational effort, required to accomplish the iteration increases, while Java-based version could handle it. A dominance of elitist version of algorithm is non-obvious, but this could be seen from a distribution first quartile.

\paragraph{}
We collected the best performing configurations of each meta-heuristic and presented them in \cref{eval: params jmetalpy es}. We also highlight here the default parameter values, which were selected with motivation of being in the middle of the values ranges.

\begin{table}%[h!]
	\centering
	\begin{tabular}{r||c|c|c}
		\textbf{Hyper-parameter} & \textbf{Default value} & \textbf{Tuned value} & \textbf{Estimated range} \\
		\hline
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 22 & $[1..1000]$ \\
		\emph{elitist} & False & True & {True, False} \\
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy simulated annealing} \\
		\hline
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.89  & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetal evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 605 & $[1..1000]$ \\
		\emph{elitist} & False & True  & {True, False}\\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$ \\
	\end{tabular}
	
	\caption{Static hyper-parameters of low-level meta-heuristics.}
	\label{eval: params jmetalpy es}
\end{table}


\section{Concept Evaluation}\label{eval:1}

\subsection{Evaluation Plan}\label{eval:1:plan}
To evaluate the performance of developed approach we firstly need to define the base line. In most cases it is the single meta-heuristics, which are solving the OP using static hyper-parameters. However, to evaluate the parameter control feature we must make a closer look on the performance of each separate heuristic with static and dynamic hyper-parameters. For selection hyper-heuristic analysis we compare the performances of all underlying MHs running separately and together within a hyper-heuristic. Note, in this case the hyper-parameters are statically defined. And last, but not least, to evaluate a selection hyper-heuristic with enabled parameter control we compare it to separately running underlying meta-heuristics with parameter control and to selection hyper-heuristic.

In order to organize the evaluation plan, we distinguish two stages.
At the first stage the LLH selection occurs, while at the second one we chose hyper-parameters for the selected LLH. At each stage we may use different prediction approaches, which description could be found in \cref{impl: prediction models}. To select the LLH, apart from random and static selection we also use FRAMAB (see \cref{impl: FRAMAB}) and Bayesian ridge regression model implementation from Scikit-learn framework (see \cref{impl: sklearn wrapper}). Note, for the Bayesian ridge regression model we use a default parameters, which could be found in the framework documentation\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html}{scikit-learn.org}}. To select the hyper-parameters for LLHs, apart from static default and tuned variants we also use random selection, available in BRISEv2 TPE and the mentioned above Bayesian ridge. The set of used techniques is presented in the \cref{eval: concept settings table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{l||l}
		\textbf{LLH selection} & \textbf{LLH parameters selection} \\
		\hline
		\hline
		\textbf{1.} Random & \textbf{1.} Default \\
		\textbf{2.} Multi-armed bandit & \textbf{2.} Tuned beforehand \\
		\textbf{3.} Bayesian ridge regression & \textbf{3.} Random \\
		\textbf{4.1.} Static jMetalPy.ES & \textbf{4.} Tree Parzen Estimator (TPE) \\
		\textbf{4.2.} Static jMetalPy.SA & \textbf{5.} Bayesian ridge regression (BRR) \\
		\textbf{4.3.} Static jMetal.ES & 
	\end{tabular}
	
	\caption{Prediction techniques used for the concept evaluation.}
	\label{eval: concept settings table}
\end{table}


Using this table, we now could pick a prediction technique to form a desired system configuration. For instance, mentioned above baseline could be encoded into configurations starting from \emph{4.1.1} for the evolution strategy from jMetalPy framework, running with default hyper-parameters and ending with \emph{4.3.2} for evolution strategy from jMetal framework, running with tuned beforehand parameters.

Our benchmark plan for the concept evaluation looks as a set of following experiment groups:
\begin{itemize}
	\item \textbf{Meta-heuristics (MH).} The baseline. We evaluate each used meta-heuristic separately with default and tuned hyper-parameters: \emph{4.1.1} and \emph{4.1.2} for jMetalPy evolution strategy;  \emph{4.2.1} and \emph{4.2.2} for jMetalPy simulated annealing; \emph{4.3.1} and \emph{4.3.2} for jMetal evolution strategy respectively.

	\item \textbf{Meta-heuristics with parameter control (MH-PC).} The set of experiments dedicated to verify an impact of the generic parameter control on meta-heuristics performance. A selected set of experiments looks as follows: \emph{4.1.3, 4.2.3, 4.3.3} to investigate the influence of random parameter allocation; \emph{4.1.4, 4.2.4, 4.3.4} to check TPE-based parameter control and \emph{4.1.5, 4.2.5, 4.3.5} to probe Bayesian-ridge-based parameter control.

	\item \textbf{Selection hyper-heuristic with static parameters (HH-SP).} These benchmarks are dedicated to an investigation of the implemented on-line selection HH performance. It implies the LLHs usage with static parameters therefore, we evaluate HH-SP performance with default and tuned beforehand LLH parameters. Experiment codes are following: \emph{2.1, 2.2} for FRAMAB-based HH-SP and \emph{3.1, 3.2} for Bayesian-ridge-based HH-SP.
	
	\item \textbf{Selection hyper-Heuristic with parameter control in LLH (HH-PC).} This is a final set of benchmarks for concept evaluation. By this we evaluate an influence of simultaneous on-line LLH selection and parameter control on system performance. The respective experiment set is following: \emph{1.3, 2.4, 2.5, 3.4, 3.5.}
\end{itemize}

The aggregated concept benchmark plan is presented in \cref{eval: concept benchmark plan table}. The required running time of this experiment set is approximately 9 days and 18 hours on a single machine.
\begin{table}[h!]
	\centering
	\begin{tabular}{c||p{3cm}}
		\textbf{Experiment group} & \textbf{Related codes} \\
		\hline
		\hline
			
		\multirow{2}{*}{MH} & 4.1.1., 4.2.1, 4.3.1 \newline 4.1.2, 4.2.2, 4.3.2 \\
		
		\rowcolor{gray!10}
		\multirow{3}{*}{MH-PC} & 4.1.3, 4.2.3, 4.3.3 \newline 4.1.4, 4.2.4, 4.3.4 \newline 4.1.5, 4.2.5, 4.3.5 \\
		
		\multirow{3}{*}{HH-SP} & 1.1, 1.2 \newline 2.1, 2.2 \newline 3.1, 3.2 \\

		\rowcolor{gray!10}
		\multirow{3}{*}{HH-PC} &  1.3 \newline 2.4, 2.5 \newline 3.4, 3.5 \\
	\end{tabular}
	
	\caption{Concept benchmark plan.}
	\label{eval: concept benchmark plan table}
\end{table}


\subsection{Baseline Evaluation}\label{eval:1:baseline}
As we discussed previously, our results comparison should be done against the defined baseline. Therefore, this section is dedicated to review of the meta-heuristics performance out-of-the-box on different problem sizes and parameter settings. For visibility reasons we plot the intermediate and the final performance evidences for each problem instance separately, since they naturally imply different result ranges.

Since we are tackling a set of TSP instances, which were previously solved by other exact solvers, we also present an optimal solution, available for each instance as a green doted line.

%\newpage
\paragraph{kroA100 and pr439 TSP instances.}

Both TSP for 100 and 439 cities are a relatively small problem instances. Therefore, all underlying MHs reach a local optimum after few first external iterations. The difference between the external and internal iteration is following: the first one happens, when the main node selects configuration and sends it to the worked node, which carries out the LLH (MH) execution (see detailed description in \cref{impl}). On a contrary, the internal iteration occurs inside LLH itself, since it is an any-time algorithm. Thus, while the MHs reach a local optimum, there is no reason to spend much time for such cases review. We put a visual representation of benchmarks for kroA100 and pr439 into the thesis appendix (\cref{app:eval:bl plots}).

The only worth to mention observation is a worse SA results with tuned parameters, in contrast to default values on kroA100 TSP instance. It is explained by the fact that for algorithm tuning we used a different problem instance (rat783). It only confirms a motivation of the parameter control approaches: tuning is not problem-instance-universal technique.

\paragraph{rat783 TSP instance.}
\svgpath{{graphics/Eval/baseline}}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on rat783.}
	\vspace{-5pt}
	\label{eval:pict:bl:rat783 intermediate}
\end{figure}

\setlength{\columnsep}{5pt}%
\setlength{\intextsep}{5pt}%
\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	\centering
	\includesvg[width=\linewidth]{rat783 baseline final boxplot}
	\label{eval:pict:bl:rat783 final}
	\caption{Final results of meta-heuristics with static parameters on rat783.}
	\vspace{-10pt}
\end{wrapfigure}
This is an average size problem among reviewed in the thesis. In contrast to previous instances, a behavior of solvers in this case changes slightly. For instance, ES from jMetal framework (j.ES) with default parameters is performing extremely slowly on a problem, however, while using an optimized hyper-parameters it quickly reaches a local optima (after ~50 iterations) with the best produced results among other heuristics (\cref{eval:pict:bl:rat783 intermediate}). Analyzing performance evidences of Python-based solvers, we may conclude that they almost reached a local optima in a given 15 minutes, therefore, their final results are slightly worse than the produced by j.ES (\cref{eval:pict:bl:rat783 final}).

\textbf{Please note}, the perturbation of trends in the end of tuned py.ES run is caused by the difference in number of iterations among all runs (left figure in \cref{eval:pict:bl:rat783 intermediate}, tuned parameters). The software~\footnote{Python Seaborn data visualization framework web page:~\href{https://seaborn.pydata.org/}{seaborn.pydata.org}}, used in this thesis for results presentation estimates an average and the result deviation at each iteration. Thus, if one (or several) experiment execution(s) managed to perform more iterations than the majority of others, the average among and their deviation will be changed respectively. The number of such external iterations varies, since as a termination criterion we used the wall-clock time. Unfortunately, this perturbation appear in most of the \emph{progress charts}, therefore the comparison of final results is done by means of presented separately box-plots.


The bold lines in \cref{eval:pict:bl:rat783 intermediate} is a statistical mean of all 9 experiment runs with default parameter values (blue line) and tuned parameter values (orange line). A shadow around these lines is a confidence interval. One may observe how differently the parameter setting affects MHs: in evolution strategies the changes in performance is dramatic, while simulated annealing is almost not affected (\cref{eval:pict:bl:rat783 final}). We also observe a decreased stability of tuned jMetalPy ES meta-heuristic (py.ES). It is reflected in a large confidence interval not only of the intermediate results, but also in a statistic of final solution quality.


\paragraph{pla7397 TSP instance.} 

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:bl:pla7397 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	\centering
	\includesvg[width=\linewidth]{pla7397 baseline final boxplot}
	\label{eval:pict:bl:pla7397 final}
	\caption{Final results of meta-heuristics with static parameters on pla7397.}
	\vspace{-10pt}
\end{wrapfigure}

The largest investigated here TSP instance for ~$7.4k$ cities is, however, referred as a middle-size OP in used TSPLIB95. In this case the performance evidences changed the most, therefore, we discuss each MH behavior separately.

Python-based version of ES provide the worst results with both default and tuned parameters. Note the number of performed iterations by this MH with default parameters in a given 15 minutes is less 50. It is affected by a several reasons. Firstly, the amount of time required to perform an internal iteration increased dramatically. Thus, with specified 15 seconds for one task run, it actually takes much more time (up to 1 minute) to accomplish the task. We have a several guesses, what caused such a behavior. Firstly, it is a general Python performance issues, in most of the times caused by a global interpreter lock (GIL) usage. The explanation of GIL is out of this thesis scope, but roughly speaking, a multi-threading in Python causes struggle in comparison with single-threaded execution. Secondly, it may be caused by the algorithm code basis implementation, performed in jMetalPy framework. To implement a generic termination criteria (and some other features) the authors utilized a push-observer design pattern~\cite{benitez2019jmetalpy} according to which the underlying algorithm (ES in our case) triggers its observers after finishing each iteration. Therefore, stopping criteria may be evaluated only after finishing this internal iteration, which in case of running py.ES with TSP instance for $7.4k$ cities, may take a while depending on the algorithm configuration. For instance, with parameters \texttt{\{mu=5, lambda=10\}} the algorithm will terminate in time, while setting \texttt{\{mu=500, lambda=500\}} the algorithm will perform a higher number of computations on arrays of $7.4k$ integers long and as a result, may struggle. We observed the ES algorithm termination after a very first internal iteration. This also causes a poor solution quality improvements. Naturally, there is also an overhead in the results sending through the network, but as we observe on the Java-based ES performance with default parameter values, this overhead caused the decrease only in ~20 external iterations. We also eliminate the possible issue in a required time for problem loading (building the TSP distance matrix), since with caching implemented in jMetalPy wrapper (see \cref{impl: LLH scope}), the worker node does it only once and stores its cached version. In any case, this issue requires deeper investigation that we postpone to the future work. Running jMetalPy ES with tuned parameters fixes the issue with task reporting delays and therefore, results in higher number of external iterations, but the solution improvements are still weak (see left picture on the \cref{eval:pict:bl:pla7397 intermediate}).

As in the previous case, jMetalPy simulated annealing produce good quality improvements at each external iteration, least depending on the hyper-parameter values. Even with a default configuration, py.SA outperforms the final results of py.ES after the first 50 external iterations. Setting the tuned parameter values, the performance of algorithm increases, but not dramatically. A resulting progress curve, presented in the middle of \cref{eval:pict:bl:pla7397 intermediate} shows that py.SA requires more time to converge that was provided and is still far from its potential local optima.

jMetal evolution strategy is a perfect candidate to show, how important is a parameter setting. With default configuration j.ES is struggling in making improving steps and can not compete with other algorithms. Our guess here is the same as for the Python-based version: the number of internal iterations is extremely low for making a good search space traversal. However, a tuned version of j.ES outperforms all other solvers (see \cref{eval:pict:bl:pla7397 intermediate} and \cref{eval:pict:bl:pla7397 final} respectively).


\paragraph{Discussion.} The observed results of meta-heuristics execution confirm the algorithm parameter setting problem importance, discussed in \cref{bg: section Parameters Setting}. An effect of proper parameter selection is different among algorithms. In our case, the performance of two out of three solvers are highly dependent on the hyper-parameter settings. Thus, an application of the proposed in \cref{Concept description} generic parameter control approach to these algorithms is rather intriguing and may partially reveal the overall methodology benefits.

From the other side, we observe of only one algorithm domination among the others with static parameters. See how all MHs were solving each TSP instance with default parameters: in each case SA outperforms two other ES. The usage of all three MHs in a selection hyper-heuristic with static hyper-parameters will reveal the implemented approach applicability. In this case we expect to observe the results close to provided by pure SA. On a contrary, when we switch to tuned parameter usage, j.ES is preferred. We see it clearly when the MHs are applied to the biggest TSP instance with tuned hyper-parameters. Thus, we expect to observe such a behavior of selection hyper-heuristic.

%\newpage
\subsection{Generic Parameter Control}\label{eval:1:PC}
As we discussed in \cref{bg: parameter control}, the goal of parameter tuning lays in adaptive changing of underlying algorithm parameters with to optimize some performance measurement. In our case, we apply the proposed in \cref{concept: conclution} methodology to set the parameters of meta-heuristics in a runtime. Here is a brief reminder: at each RL step HLH is analyzing the past performance evidences of solver depending on its configuration to choose the parameters values, which hopefully lead to the higher solution quality improvements. Afterwards, we run the solver with sampled parameters for a predefined time (15 seconds) to get new evidences, attaching the previously best obtained solutions. Please note, according to our setup, we have 6 simultaneously running and reporting workers.

In this part of evaluation we compare the performance of algorithms with statically defined default and tuned hyper-parameters to dynamically changing parameter values by means of RL control.

\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/control}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on kroA100.}
	\vspace{-5pt}
	\label{eval:pict:pc:kroA100 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on kroA100.}
	\vspace{-5pt}
	\label{eval:pict:pc:kroA100 final}
\end{figure}

Comparing to the baseline, parameter control in a small problem instance was able to reach and even outperform the results of MHs on static parameters after first 50 iterations (\cref{eval:pict:pc:kroA100 intermediate}). We may observe that even random changes of the heuristic parameters in a runtime results in finding a better solutions comparing with statically defined case (\cref{eval:pict:pc:kroA100 final}). It is caused by the changes in a neighborhood definition (mutation type) and traversal process (mutation probability). In most cases, given enough time the learning-based parameter assignment outperforms random allocation.

Note the amount of configurations (iterations) performed by jMetal ES, which could be seen in \cref{eval:pict:pc:kroA100 intermediate}. According to our plan, a given time for MH run is 15 minutes, 15 seconds for running one configuration on 6 available workers. Thus, in the most optimistic case, the number of iterations should be $ \frac{15\cdot60\cdot6}{15} = 360$ but, we observe even more than 400. After an investigation, we came to conclusion that it is caused by an implementation flaw an insight of which is following. jMetal MHs provide only iteration-number-based termination criterion, which is not encapsulated how it is done in jMetalPy. For our needs we added also a time-based but did not remove the previously existing. For the iteration counter used a regular integer number, which we set up to its maximal value when using a time-based criterion. Given a specific `light' algorithm configuration (low $\mu$, $\lambda$ and mutation probability), with this OP MH is able to reach the maximal number of iteration in less than 15 seconds therefore, terminating early and triggering a new parameter control iteration. Certainly, it is our implementation flaw, which should be fixed in a future work.

%Let us have a closer look on the jMetal ES. Two observations could be made. Firstly, the stability of results in case of random-based parameter tuning weak in comparison to model-based, which is a reasonable behavior. However, in case of jMetalPy ES, the TPE-based stability is less than random-based. This leads us to 


\paragraph{pr439 and rat783 TSP instances.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on rat783.}
	\vspace{-5pt}
	\label{eval:pict:pc:rat783 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on rat783.}
	\vspace{-5pt}
	\label{eval:pict:pc:rat783 final}
\end{figure}

In this two cases, the behavior of solvers were similar, therefore, we decided to join their discussion and present only the plots for a larger instance (rat783). Still, the graphical representation of intermediate and final performance on pr439 TSP instance is presented in \cref{app:eval:pc plots}.

When the parameter control is applied to a larger problem, it is starting to require more evidences for finding a good-performing settings of jMetalPy evolution strategy. Concretely, only the TPE-based parameter control was the closest in approaching the solution quality of tuned parameters. All techniques produced highly unstable intermediate and final results: please, draw your attention to the left side of \cref{eval:pict:pc:rat783 intermediate}, and filled with blue boxes in \cref{eval:pict:pc:rat783 final} respectively.

The results of parameter control application to less sensitive py.SA are following: randomized parameter sampling settled on the level of default parameters quality. BRR-based parameter control yielded a slightly better results, but not such stable, while TPE model approached the quality of tuned parameters (\cref{eval:pict:pc:rat783 final}).

On contrary, applying generic parameter control to j.ES MH leads to a comparable with the quality of tuned parameters (\cref{eval:pict:pc:rat783 final}). Note, as for previous problem instance, even a random-based parameter sampling outperforms default parameters.

\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:pc:pla7397 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on pla7397.}
	\vspace{-5pt}
	\label{eval:pict:pc:pla7397 final}
\end{figure}

For the final problem instance we omit the parameter control results of py.ES, since it did not manage to perform even slightest improvement in comparison to the default parameter values, presented in a baseline description. It is caused by a fact that in early stages our approach acts as a random search, since not enough evidences were obtained to build a prediction models. Thus, is case of py.ES, the MH was running with badly performing configurations and as we explained in \cref{eval:1:baseline}, did not manage to perform enough iterations to improve solution in a given 15 minutes.

As in previous cases, the least parameter-settings-sensitive py.SA shows an ability to perform almost equally with any parameter settings (\cref{eval:pict:pc:pla7397 intermediate}). Neither among available control techniques was able to outperform tuned beforehand algorithm configuration in the final results quality (\cref{eval:pict:pc:pla7397 final}). Among used models, only the BRR shabbily settled in the middle between default and tuned parameters quality.

As for j.ES, model-based approaches are outperforming the randomized parameter values allocation, not even talking about default parameters. Moreover, TPE-based control outperformed even the results of tuned beforehand hyper-parameters.


\paragraph{Discussion.} In general, the review of meta-heuristic performance on different problem instances showed that the proposed generic parameter control approach is applicable and able to yield not only the near-tuned parameters quality, but in sometimes even outperforming results: all MHs with kroA100 TSP instance jMetal evolutionary strategy with pr439 and pla7397.

Taking into account the results with random parameter allocation we are making two conclusions. Firstly, even a randomized parameters changes are able to improve a potentially bad static hyper-parameter setting (j.ES case). Secondly, the learning mechanisms should and must be improved further by means of different surrogate models usage and proper technique for surrogates optimization to more resize parameter search. Leaving the improvement steps to future work we conclude that the proposed generic parameter control concept is able to produce better results while solving an unforeseen problem on-line.


\subsection{Selection Hyper-Heuristic with Static LLH Parameters}\label{eval:1:hh-sp}
The second mode of developed approach and at the same time a main goal of this thesis is a process of dynamic heuristic selection. It was implemented in form of RL-based on-line selection hyper-heuristic, described in \cref{concept: conclution}. Here we use three available LLHs (mentioned above py.ES, py.SA and j.ES) with static parameter (default and tuned) into selection hyper-heuristic (HH-SP). Three approaches to select the LLH were investigated: randomized, FRAMAB- and BRR-based HLH.

We present the problem solving process in two forms. Firstly, we distinguish the selected at each iteration LLH and the results, which it gave. For doing so, we took only the first repetition (out of 9 available). Secondly, we present the final results of all runs in form of box-plots, comparing them to the underlying LLHs performance. The left group of box-plots presents the final solution quality, obtained with the default parameter values, while on the right site the results of tuned beforehand LLHs are outlined.

\paragraph{kroA100, pr439 and rat783 TSP instances.}
\svgpath{{graphics/Eval/selection}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP progress}
	\caption{Intermediate performance of HH-SP on rat783 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 intermediate}
\end{figure}
\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP final boxplot}
	\caption{Final results of HH-SP on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 final}
\end{figure}
Once again, we are grouping relatively small problem instances, on which the implemented HH-SP performs similarly. For the analysis we selected the largest instance among them: rat783. Nevertheless, the figures depicting kroA100 and pr439 TSP instances may be found in \cref{app:eval:hh-sp}.

% default parameters
Firstly, we would like to draw the reader's attention to HH-SP cases, in which the LLHs are used with the default parameter values (upper row in \cref{eval:pict:hh-sp:rat783 intermediate}). We do so, while according to the presented earlier baseline evaluation, there is only one algorithm with a strong performance dominance: py.SA. Thus, in \cref{eval:pict:hh-sp:rat783 intermediate} we may observe a high frequency of py.SA sampling by both learning-based selection strategies. One may distinguish a repetitive pattern in LLH allocation with FRAMAB (middle column). It is caused by a deterministic essence of the algorithm. When it reaches the critical point of changing the favor of utilizing py.SA and exploiting other heuristic, the FRAMAB's exploration mechanism fully guides a selection. Due to the usage of a similar time-based LLH termination, all workers are starting the next round in bunches. Thus, when a new round starts, FRAMAB operates on static information and allocates all next configurations with the same LLH, which turns to be the second best performing LLH: j.ES. Therefore, in such a setup we conclude FRAMAB behaves slightly inertly. One may argue this will cause a performance struggling, which is rather a logical conclusion, but it requires a further investigation, which we are forced to postpone for the future work due to a lack of time. In case of BRR usage (right column in \cref{eval:pict:hh-sp:rat783 intermediate}), the bias is strongly shifted towards j.ES, which in some cases may cause performance issues due to lack of exploration. According to presented in \cref{eval:pict:hh-sp:rat783 final} final results statistics, given at least one dominating LLH (default parameters values case), even a random LLH selection could utilize enough times to obtain a good solution quality, however, it may require more time to converge. The model-based LLH selectors produce a better results quality.

The next setup is a set of LLHs with tuned parameters (lower row in \cref{eval:pict:hh-sp:rat783 intermediate}). According to the baseline evaluation, all among available LLHs are able to tackle the problem and produce a similar solution quality, however, the light difference in a dominance present and is as following (descending): j.ES, py.SA, py.ES. As a consequence, FRAMAB HLH frequently utilizes the best j.ES (see middle of lower row in \cref{eval:pict:hh-sp:rat783 intermediate}). On contrary, BRR HLH, similarly to random-based, samples all LLH types almost evenly. We may conclude that BRR is not as sensitive to performance evidences and was `confused' since the process quickly converged into a local optima. The quality of final result presented in \cref{eval:pict:hh-sp:rat783 final} of all HP-SP versions with tuned LLHs are at least as good, as the solution quality provided by the best underlying LLH due to its fast convergence (see \cref{eval:pict:bl:rat783 intermediate}).

\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP progress}
	\caption{Intermediate performance of HH-SP on pla7397 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pla7397 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP final boxplot}
	\caption{Final results of HH-SP on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pla7397 final}
\end{figure}
Our observations of different MH heuristics on the largest tackled in this thesis problem are as following. During the baseline evaluation py.ES with default parameters had the worst performance and not able to accomplish external iteration in time, while the tuned algorithm version was able to outperform only default j.ES. On contrary, j.ES with tuned parameters produced the best results, outperforming both middle-quality py.SAs. The best performing meta-heuristic with default parameters was py.SA. According to this information, we observe an expected behavior of HH-SP with default LLHs (upper row in \cref{eval:pict:hh-sp:pla7397 intermediate}): the most frequently sampled by learning-based HLH was py.SA. However, the number of py.ES usages is suspiciously high in BRR case. According to baseline evaluation, the optimization process was not able to advance in a search using the py.ES (which we observe in a random-based sampling case, upper left cell in \cref{eval:pict:hh-sp:pla7397 intermediate}). Till the present time we do not have a comprehensive explanation of this behavior. Referring to the final results, presented in \cref{eval:pict:hh-sp:pla7397 final} we observe a high diverse in quality when py.ES is allocated frequently (codes 1.1. and 3.1.), which is an expected behavior, taking into account the performance of py.ES with default parameters.

Talking about the tuned LLHs case, we observe almost equal performance of all LLH sampling approaches, comparable to the best available LLHs final results. The solution quality of random-based HLH is slightly worse, in comparison to the results of FRAMAB- and BRR-based HH-SP due to their frequent usage of j.ES (see right chart in \cref{eval:pict:hh-sp:pla7397 final}).

\paragraph{Discussion.} According to our observations of the developed selection hyper-heuristic performance we conclude that the proposed concept implementation operates as expected: HH-SP exposes similar performance to the best available underlying LLH. Two implemented selection HLH are performing slightly differently when reaching a local optimum. We claim the FRAMAB is a more perspective HLH, since it starts to balance between previously seeing good performing LLH exposing a good exploration abilities. On a contrary, BRR continues to utilize only the best performing heuristic. In case when the advantage of one LLH changes to another, BRR may need more time to learn this. Nevertheless, our guesses require more evaluation proves.

The observed issues call for not only a thorough investigation (pla7397 code 1.1, 3.1), but also a generic approach to handle a potential flaws in LLH implementation that may cause struggling of overall HH-SP execution. The implemented system should be evaluated by means of HLH configuration influence on the performance. Also, a further investigation of adding several new LLHs should be evaluated by means of required computation effort for finding a good LLHs vs pure performance of these LLHs.


\subsection{Selection Hyper-Heuristic with Parameter Control}\label{eval:1:hh-pc}
The final evaluation is dedicated to performance analysis of the suggested approach of merging the selection hyper-heuristic with the generic parameter control technique. A minimal goal is to obtain a performance of the best underlying LLH algorithm with tuned hyper-parameters. In this evaluation set we follow a similar to used for HH-SP method of intermediate results review distinguishing allocated LLH types at each iteration for one repetition and comparing the quality of final results over all repetitions with a baseline. As specified in the evaluation plan (\cref{eval:1:plan}), for the LLH selection we use three approaches: random, FRAMAB and BRR sampling, while for LLH parameter control we use the random, TPE and BRR sampling.

\paragraph{kroA100, pr439 and rat783 TSP instances.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC progress}
	\caption{Intermediate performance of HH-PC on rat783 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:rat783 intermediate}
\end{figure}

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:rat783 final}
\end{figure}

The decision to join all three TSP instances is motivated by a similar to mentioned in HH-SP discussion reasons: the system intermediate and final performance are rather similar among problem instances, therefore, here we review only a single (rat783) case. The results for all other problems may be found in~\cref{app:eval:hh-pc}.

During the solving process a similar to HH-SP patterns of algorithm allocation may be observed for both FRAMAB-based (codes 2.4, 2.5) and BRR (codes 3.4, 3.5) HLHs. However, in this case the intermediate results are slightly differing, since the parameter control has started to search for good LLHs settings (\cref{eval:pict:hh-pc:rat783 intermediate}). 

Let us firstly draw the reader's attention to HH-PC with FRAMAB for LLH and TPE for parameter sampling (code 2.4 in \cref{eval:pict:hh-pc:rat783 intermediate}). At the beginning of solving process, j.ES was performing extremely well, for this reason FRAMAB was sampling it with a higher frequency. When a solving process reached its local optima (nearly $50^{th}$ iteration), FRAMAB started exploration of the other heuristics. The appeared `noise' in the results of both j.ES and py.ES heuristics is caused by a boolean parameter \emph{elitist}, which defines the selection strategy and may result in the solution quality degradation (a more detailed description of ES parameters could be found in \cref{BG: MH Examples}). From an absence of noise in later stages of 2.4 and 3.4 its may be seeing that TPE has found \emph{elitist=False} parameter value to be perspective in both ESs. On a contrary, BRR-based parameter controller did not find these parameters and glancing on the quality of final results (\cref{eval:pict:hh-pc:rat783 final}) we conclude the BRR statistically finds worse performing parameters, comparing to TPE on rat783 problem instance. Also, we must not ignore the fact of early reaching a local optimum by the search process (see shapes of progress curves in \cref{eval:pict:hh-pc:rat783 intermediate}). In such case, the parameter search should be biased towards exploration. For that we need to introduce other progress metrics, such as stagnation detection (which is used in EA parameter control in~\cite{karafotias2014generic}) and perform multi-objective RL optimization, maximizing improvement and minimizing stagnation. We postpone this enhancement for a future work due to a lack of time.

In \cref{eval:pict:hh-pc:rat783 final} we clearly see the dominating j.ES MH with tuned parameters (code 4.3.2). The quality of final solution, produced by the proposed concept implementation is statistically slightly lower the best performing tuned j.ES. This may be explained by a lack of time for parameter control to find a good performing setting, since the optimization reached its local optima too quickly to find good parameters for the underlying j.ES.


\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC progress}
	\caption{Intermediate performance of HH-PC on pla7397 (single experiment).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pla7397 intermediate}
\end{figure}

\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pla7397 final}
\end{figure}

As we observed in previous experiments, py.ES with non-tuned parameters causes struggling in hyper-heuristic performance (see \cref{eval:pict:bl:pla7397 intermediate} and the discussion below \cref{eval:pict:pc:pla7397 intermediate}). That is why during these benchmarks py.ES made a crucial change in the overall number of iterations where it was used many times (codes 1.3, 3.4, 3.5 in \cref{eval:pict:hh-pc:pla7397 intermediate}). While the case of fully randomized HH-PC (code 1.3) is clear, a BRR LLH selection did use this LLH frequently because it may have produced a good solutions quality as a result of parameter control behavior. On a contrary, FRAMAB LLH selection in combination with TPE parameter tuning (code 2.4) managed to find good parameters for py.SA, therefore, utilized it most often. When the FRAMAB's exploration component weight reached exploitation's, the other LLHs usage was triggered and as a result, HH-PC switched to j.ES usage. This switch gave a dramatic result improvements (code 2.4 \cref{eval:pict:hh-pc:pla7397 intermediate}). The FRAMAB LLH selection with BRR parameter control (code 2.5) at the beginning was using the mixture of mainly two j.ES and py.SA, but later switched to simulated annealing-only mode. As we may see, it gave a fast coarse-grained solution improvements in the beginning, and stable, but rather slow fine-grained improvements in a later stage.

On the final results quality charts (\cref{eval:pict:hh-pc:pla7397 final}) we observe a dominance of FRAMAB-based LLH selection (codes 2.4, 2.5) over BRR-based and TPE-based parameter control (codes 2.4 and 3.4) over BRR-based (codes 2.5, 3.5). On a contrary, the results obtained with BRR-based parameter control are more stable than TPE-based. The quality of final results did not reach the best performing tuned j.ES (code 4.3.2). However, since the optimization process did not settle in a local optima, we have a doubt that HH-PC will not outperform j.ES when given same number of external iterations.

For a better intuition, let us draw the reader's attention to \cref{eval:pict:hh-pc vs jES on pla7397 process}. Note how HH-PC (code 3.4) approaches j.ES progress curve. If it were not for the issue with a number of external iterations, the results would be better and, probably, outperforming tuned j.ES. Nevertheless, for current implementation and given 15 minutes for all solvers j.ES managed to make more iterations and moved further.

\begin{figure}[h]
	\centering
	\vspace{-5pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC vs jES progress}
	\caption{HH-PC and tuned jMetal ES solving process comparison on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc vs jES on pla7397 process}
\end{figure}

\paragraph{Discussion.} The observed results of solving the united ASP and PSP problems by HH-PC are encouraging. With small problem instances (kroA100, pr439, rat783) HH-PC managed to approach and in some cases even outperform the best underlying LLH with tuned hyper-parameters. When the problem size significantly grows (pla7397), the gap between HH-PC and the best performing LLH started to increase. An explanation for this behavior is simple: the system needs to build surrogate models not only for single, but for multiple LLHs and, therefore, requires more performance evidences in comparison to MH-PC or HH-SP. On a contrary, the amount of information only decreases as a consequence of an issue with py.ES. Since the parameter values are not selected to properly reflect the LLHs performance, the algorithm selection process also straggles. To overcome the problem with lack of information we propose to execute an additional meta-learning step before the beginning of optimization session. Unfortunately, we are forced to postpone an investigation of this rather intriguing idea for the future work. Nevertheless, HH-PC significantly exceeds the results of MHs with static default parameters in all cases and the guided by models approach outperforms fully randomized HH-PC.


\section{Parameter Analysis of Hyper-Heuristic with Parameter Control}\label{eval:2}
The second benchmark is dedicated to an evaluation of system settings influence on the solving process.
Due to limited time we decided to perform the benchmarks only for the most complex system mode: on-line selection hyper-heuristic with parameter control in low-level heuristics (HH-PC).

\subsection{Evaluation Plan}\label{eval:2:plan}
As with the concept evaluation, this set of benchmarks we also start from the planning. For comparison basis we selected one statistically better performing HH-PC setup, in which FRAMAB was used for the algorithm selection, and TPE for parameter control respectively. Their default configuration of which was as follows. Implemented FRAMAB algorithm exposes only one parameter: the balancing coefficient \emph{C}. In \cref{impl: FRAMAB} we discussed it values and also proposed an approach to replace this static value by one, derived from the deviation of results. In all previous tests we used exactly this approach. TPE implementation exposes \emph{split size} parameter, which defines the percentage of available data, used to create a distribution of good-performing parameter values (see TPE description during HpBandSter framework review in \cref{bg: bohb}). In all our experiments we used $\sfrac{1}{3}$ of available data to construct a `good' distribution. The amount of information, which was used to construct the surrogate models was controlled by a shared parameter \emph{window size}. In our evaluations we were using 80\% of available information each time. As one may remember, since we did not implement a proper optimization algorithm over surrogates, but rather used a random sampling and afterwards selected the best parameter set by means of their results with surrogates, we used a predefined number of randomly sampled parameter values on each level (more details in \cref{impl: prediction models}). During our experiments we sampled 96 combinations of parameter values on each level (the default value in BRISEv2). Also, in our setup we used a predefined number of workers (6 ps) and static time for task execution (15 seconds). The implemented RL approach required continuing the solving process between iterations. For doing so, after accomplishing a task the workers were sending a complete bunch of obtained solutions to main node, which is then attached them to the new tasks. This difference is caused by the usage of single-solution simulated annealing and population-based evolution strategy meta-heuristics.

All aforementioned characteristics form a three groups of experiments:
\begin{enumerate}
	\item \textbf{Learning granularity} group is designed to investigate the influence of performance evidences amount and quality, obtained between external iterations. It is formed by such parameters as \emph{window size}, \emph{task time} and \emph{number of workers}.
	
	\item \textbf{Learning models configuration} group of experiments are dedicated to investigate the influence of HLH parameters on the results quality and includes \emph{FRAMAB C coefficient}, \emph{TPE split size}, \emph{random search size} over the surrogates.
	
	\item \textbf{Generic low-level heuristic configuration} group is currently formed only from one characteristic for investigation: amount of solver warming-up information, which is defined by the number of solutions, reported in the end of LLH external iteration.
\end{enumerate}

We aggregate the proposed parameters and define the values for evaluation in \cref{eval:2: planning table}.

\begin{table}[h!]
	\centering
	\begin{tabular}{l||c|c}
		\textbf{Parameter} & \textbf{Investigated values} & \textbf{Default value} \\
		\hline
		\hline
		\rowcolor{gray!10}
		\multicolumn{3}{c}{Learning granularity} \\
		\textbf{Window size} & 30\%, 50\%, 100\% & 80\% \\
		\textbf{Task time} & 5, 10, 30 seconds & 15 seconds \\
		\textbf{Number of workers} & 3, 9, 12 & 6 \\
		\rowcolor{gray!10}
		\multicolumn{3}{c}{Learning models configuration} \\
		\textbf{FRAMAB C coefficient} & Static 0.001, 0.01, 0.1 & STD-based \\
		\textbf{TPE split size} & 10\%, 50\%, 70\% & 30\% \\
		\textbf{Random search size} & 50, 200 & 96 \\
		\rowcolor{gray!10}
		\multicolumn{3}{c}{Generic LLH configuration} \\
		\textbf{Warming-up solutions} & one & all
	\end{tabular}
	\caption{Prediction techniques used for the concept evaluation.}
	\label{eval:2: planning table}
\end{table}

We set all other parameters as they were configured during HH-PC evaluation, in particular the experiment running time is set to 15 minutes, number of experiment repetition is 9, the search space is unchanged (3 LLHs with the same parameter ranges and default values).

The idea of performing the full factorial design was quickly abandoned since it requires 46 days of non-stop experiment running.
Therefore, we performed a one-exchange benchmark design, which resulted in 18 experiments, which needed 40,5 hours to perform 9 repetitions.

\subsection{Learning Granularity}\label{eval:2:learning granularity}
In this experiment we investigate the influence of RL routines configuration on learning process. The idea is as follows. Changing the \emph{window size}, \emph{task time} and \emph{number of workers}, the underlying models may learn a different picture of dependencies. For instance, if we increase a task time, a sampled configuration will be measured more thoroughly, however, given a fixed experiment running time, the overall number of iteration will be decreased. On a contrary, by increasing a number of workers for the same fixed experiment time a size of investigated \emph{parameter space} will be increased, therefore, the underlying learning models will construct a more precise surrogates.

\textbf{Please note}, the perturbations at the end of runs on progress charts are caused by different number of performed iterations. More detailed explanation could be found in \cref{eval:1:baseline}.

\paragraph{Window size.}
\svgpath{{graphics/Eval/2bench}}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{window size plots}
	\caption{Influence of HH-PC (code 2.4) window size on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:window size}
\end{figure}
According to our expectations in \cref{concept:prediction}, this parameter is responsible for the forgetting mechanism, which is required for system adaptation to changes in optimization process and domination of one parameters over others (including LLH).
In \cref{eval:2:pict:window size} we presented the system running results with four different window sizes. We observe that the best results were obtained, when the learning models used all available information, in other words, with disabled forgetting mechanism. On a contrary, with the smallest window size the quality of final results are the worst. A trend could be observed, according to which with a larger window size system provides a better quality of results. During the benchmarks, dedicated to the concept evaluation we used 80\% of available information, which corresponds to the middle-quality parameter value.
Our conclusion is as following: with this problem instance and system setup, the changes in a learning process and parameter preference are mostly negligible, therefore, the forgetting mechanism should be disabled.
In a future, it would be rather intriguing to investigate the influence of this parameter with other, potentially dynamic problem and/or instance, where the parameter preference is changing.

\paragraph{Task time.}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{task time plots}
	\caption{Influence of HH-PC (code 2.4) task time on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:task time}
\end{figure}
Varying the time for one task evaluation (external iteration), one will dramatically change the granularity of obtained results. For instance, in our experiments with the least amount of time (5 seconds) HH-PC managed to perform more than 600 external iterations, while with the largest budged (30 seconds) approximately 150 (\cref{eval:2:pict:task time}). However, as we mentioned previously, the more information does not necessary imply a more precise surrogate models. The used by us during the concept evaluation 15 seconds provided in the worst results. Boundary cases with 5 and 30 seconds task time gave rather unstable result (see box-plots in \cref{eval:2:pict:task time}). The former is full of too approximately evaluated parameters, while the later simply did not manage to perform enough iterations. Balancing between results stability and quality we conclude that for the current setup statistically better choice would be to set 10 seconds for a task running time. 

\paragraph{Number of workers.}
\begin{figure}[h]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{number of workers plots}
	\caption{Influence of HH-PC (code 2.4) workers number on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:number of workers}
\end{figure}
The influence of workers number was rather surprise to us. According to our expectations, with a growing number of LLH runners, the amount of evaluated configurations increases. However, instead of getting surrogate models with a better quality and as a result, improving sampled parameters performance, we observe the opposite behavior. For increasing number of workers the results became worse and worse, see box-plots in \cref{eval:2:pict:number of workers}. Currently, we do not have a comprehensive explanation of the observed behavior, except of possible surrogate models over-fitting. It is broadly studied problem in ML, according to which the models create a too complex hypothesis, which can not generalize well to unforeseen data. Thus, in our case it is possible that being over-fit, models did not adequately predict a possible results for sampled parameters, therefore, guided prediction process in a wrong direction. In any case, this behavior requires more comprehensive investigation.

\subsection{Learning Models Configuration}\label{eval:2:learning models}
We perform this set of experiments for getting an intuition about the influence of underlying high-level heuristic configuration on general performance. As mentioned above, in HH-PC code 2.4 we used following learning models: FRAMAB for LLH selection and TPE for parameter tuning.

\paragraph{FRAMAB C coefficient.}
\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{FRAMAB c plots}
		\caption{Statistical results comparison.}
		\label{eval:2:pict:framab c statistic}
	\end{subfigure}
	%\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{\textwidth}
		\includesvg[width=\textwidth]{FRAMAB c plots chonics}
		\vspace{-5pt}
		\caption{One run with distinguished LLH allocation.}
		\label{eval:2:pict:framab c one run}
	\end{subfigure}
	\caption{Influence of HH-PC (code 2.4) FRAMAB C coefficient on pla7397 TSP instance solving process.}
	\label{eval:2:pict:framab c}
\end{figure}

As you remember from FRAMAB description, presented in \cref{impl: FRAMAB}, the role of coefficient C is to give the user possibility to control the EvE balance while selecting the LLH. In our implementation we proposed the usage of result improvement standard deviation, motivated by (1) the exploration encourage, while the uncertainty in results exist and (2) possibility of FRAMAB parameter-less usage. Therefore, in first set of benchmarks we exclusively used this, STD-based FRAMAB regime.

The results of performed benchmarks with possibly different C values are presented in \cref{eval:2:pict:framab c}. More concretely, in \cref{eval:2:pict:framab c statistic} we compare statistic of all repetitions with different parameter value. Here we may conclude, the proposed parameter-less (STD-based) FRAMAB performs slightly worse in comparison to other algorithms. Glancing on the sequence of LLH allocation, presented in \cref{eval:2:pict:framab c one run}, we conclude that parameter reflects its intent, especially, comparing $C=0.1$ and $C=0.001$. When C is large, the exploration-related part of FRAMAB's UCB value is increased, therefore, more different algorithms are used for optimization. However, when the C value is decreased, only few switches may be observed. An intriguing idea arises to introduce the technique for FRAMAB parameter control, according to which, the entire LLH portfolio utilized at the beginning with high C value, while approaching the end C is increased, to more concentrate with the best-performing LLH.

\paragraph{TPE split size.}
\begin{figure}[b]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{TPE split size plots}
	\caption{Influence of HH-PC (code 2.4) TPE split size on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:tpe split size}
\end{figure}
The internals of Bayesian TPE approach for parameter sampling were described during BOHB algorithm discussion in \cref{bg: bohb}. Evaluated in this experiment \emph{split size} parameter defines the proportion of information, used to construct the probability distributions of good parameter values. 

With small split size, only elite parameter values form distribution, therefore, an overall sampling process happens to be more greedy or, in other words, more exploitation-biased. According to our observations, presented in \cref{eval:2:pict:tpe split size}, the least greedy parameter allocation produces statistically the worst results (70 \%), while usage of 10 \% split relieved the best potential. Used in previous experiments 30 \% split produced slightly worse results than 50 \%, which still could be caused by the randomized processes. Probably, this behavior should be investigated using more statistical data.

\paragraph{Random search size.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{Random search size}
	\caption{Influence of HH-PC (code 2.4) random search size for surrogate optimization on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict: random search size}
\end{figure}
This HLH parameter configures the random search process, performed over the created surrogate models. With given random search size equal $N$, for selecting the parameter values of LLH on each level $N$ randomized samples are taken. Afterwards, these samples are compared with each other using surrogate models and one with the best results became level prediction. Therefore, according to our expectations, by increasing the random search size, the parameter values prediction process become more precise and as a consequence, performance increases.

After evaluating two additional to default sizes of 50 and 200 samples respectively, the obtained results happen to be not as we expected. In general, we observe a quality fluctuation, which is caused by randomized processes. When the sampling size was decreased, the quality statistics was slightly improvement, which is non-logical behavior (see intermediate and final results in \cref{eval:2:pict: random search size}). It only motivates us to implement a proper optimization technique over surrogates for improving a robustness of the prediction process.

\subsection{Generic Low-Level Heuristics Configuration}\label{eval:2:llh changes}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{warming-up solutions plots}
	\caption{Influence of HH-PC (code 2.4) warming-up solution number on pla7397 TSP instance solving process.}
	\vspace{-5pt}
	\label{eval:2:pict:warming-up solutions}
\end{figure}
The generic LLH configuration benchmark is represented by only one experiment, which we performed to evaluate how does the \emph{number of warming-up solutions} affects the search. Our intuition during the implementation was following: we have to initialize the solver with all previously available solutions not to lose the obtained trajectory and traversal velocity. It is relevant only to the population-based algorithms, such as evolution strategy or potential genetic algorithm. The implemented py.SA simply used the best one among available population to start. When SA completes its run, it reports only a single obtained solution. If the following LLH is population-based ES, we do not copy SA-based solution, but rather allow ES to sample all the rest uniformly at random.

However, after changing this behavior to the only one warming-up solution usage, we surprisingly found out that the results quality was almost not affected: please, pay attention to overlapping progress curves in a left side of \cref{eval:2:pict:warming-up solutions}. Moreover, passing only one solution between LLHs the overall number of external iterations were increased dramatically (from ~200 to ~300). It is caused by the reduced overhead for information processing and sending through network. We conclude that changing the behavior to only one warming-up solution usage provides a positive impact on the final results quality and do not affect the intermediate progress.



\section{Conclusion}\label{eval: conclution}
The evaluation of proposed concept was presented in this chapter and performed in two stages. At the first stage an analysis of the implemented concept was performed. We compared it with a baseline, defined by the executed in isolation underlying meta-heuristics with static hyper-parameters. Our conclusions on the concept applicability are following:
\begin{itemize}
	\item Firstly, the proposed reinforcement learning-based generic parameter control approach (MH-PC) is able to significantly improve the performance of meta-heuristic's static hyper-parameters and in some cases even outperform the results of tuned beforehand parameters. From the implementation perspective, the system has lack of mechanism for bad-performing external iteration termination. On the generic level, it may be implemented in form of tasks termination mechanism, carried out by BRISEv2.
	
	\item Secondly, the developed heuristic selection technique (HH-SP) is able to reach a quality of the best performing underling low-level heuristic. 
	
	\item Finally, the approach for simultaneous algorithm selection and parameter tuning (HH-PC) in runtime outperforms the best underlying algorithm with tuned parameters on rather small problem instances (kroA100, pr439). With growing complexity, the aforementioned issue of LLH struggling results in a gap between the desired performance of the best tuned algorithm and our approach. But, the guided by RL and surrogate models HH-PC considerably outperforms randomized selection of both LLH and parameter, which is the only approach available in runtime. A more confident conclusion for larger problem instances should be done after thorough investigation, where low-level heuristics are given more time to converge in local optimum and aforementioned issues are resolved.
\end{itemize}

In the second evaluation part we investigated an influence of the proposed united approach (HH-PC) configuration setting on its performance. The influence of some among evaluated parameters was not as we expected, which only motivates the importance of proper parameter values search and usage. More concretely, an insertion of relatively `light' \emph{forgetting mechanism} instead of improving an optimization process made the results statistically sightly worse. The increased number of workers only decreased the surrogate models accuracy. The decision of all available solutions usage for LLH initialization introduced a redundant overhead, but did not improve final results quality, therefore, it should be reconsidered. Nevertheless, a set of experiments from the second evaluation stage should also be performed over the other system operating modes (MH-PC and HH-SP) for making a confident conclusion. Up until now it reviled the system adaptation ability with help of exposed parameters.
\todoy{add evaluation of 2.4 on a largest problem instance will all reviewed fixes?}