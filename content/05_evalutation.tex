\chapter{Evaluation}\label{eval}
The concepts, proposed in \cref{Concept description}, implemented in \cref{impl} of search space representation, prediction process and based on this generalized parameter control approach, selection hyper-heuristic and the hyper-heuristic with parameter control should be broadly evaluated. The experiments may be performed in number of investigation directions, starting from the developed reinforcement learning performance with respect to system configuration and ending with the scalability to different problem sizes.

The structure of this Chapter is as follows. We start the evaluation process with a review of TSP benchmark set in \cref{eval: op}. The following benchmarks could be divided to two main parts. The first is dedicated to the evaluation of developed concept in comparison to the base line and is presented in \cref{eval: concept}, while in the second we investigate an influence of the proposed hyper-heuristic with parameter control settings on its performance (\cref{eval: hh-pc}). Finally, in \cref{eval: conclution} we conclude a discussion of the obtained results.


\section{Optimization Problem}\label{eval: op}
Through this thesis we are tackling a vehicle routing problem — the traveling salesman OP, which explanation could be found in \cref{BG: subsection OPs}. Nevertheless, as a reminder we repeat its short definition here. We also include the other details, related to the benchmarks.

``Given a set of cities and the distances among them, find the shortest path, which visits all cities''. It is a combinatorial OP with a number $n = N!$ of possible solutions. For the benchmarks we use several instances of symmetric TSP (distances $x_i \rightarrow x_j$ and $x_j \rightarrow x_i$ are equal) from a publicly available and broadly used benchmark set TSPLIB95~\footnote{TSPLIB95 website:~\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/}}. The advantage of choosing this benchmark set lays in a broad compatibility of solvers and frameworks with the proposed standardized problem instance description (including used jMetal and jMetalPy). The TSP in this case is defined as a set of city coordinates therefore. Thus, before starting to solve a problem, the distance matrix should be built, calculating Euclidean distances between cities. For more detailed explanation of TSPLIB95 problem instance files refer to~\cite{reinelt1995tsplib95}.

For our benchmarks we select four problem instances from a simpler case harder they are: \emph{kroA100}, \emph{pr439}, \emph{rat783} and \emph{pla7397} of sizes 100, 439, 783 and 7397 cities respectively. The optimal tours for each of these problem instances were previously obtained by exact solvers and reported in aforementioned library. While presenting our evaluation results, we refer to them therefore, we present the optimal solution results in \cref{eval:table:tsp optimal tour length}.

\begin{table}[h!]
	\centering
	\begin{tabular}{c||c}
		\textbf{TSP instance} & \textbf{Optimal tour length} \\
		\hline
		\hline
		kroA100 & 21282 \\
		pr439 & 107217 \\
		rat783 & 8806 \\
		pla7397 & 23260728 \\
	\end{tabular}
	\caption{TSP instances optimal tour length.}
	\label{eval:table:tsp optimal tour length}
\end{table}

Note, that the goal of this thesis and the evaluation in particular is not to beat the exact solvers in any case, but to investigate the applicability of proposed generic parameter control concept.

\section{Environment Setup}\label{eval: environment}
To run our experiments we use an enhanced by our approach BRISEv2 and deploy it in Docker containers on a single host machine with following characteristics:
\begin{itemize}
	\item \textbf{Hardware:} Fujitsu ESPRIMO P958 computer with 64GB 2667MHz RAM (16GB * 4 pcs), Intel Core i7-8700 CPU @ 3.2 GHz (6 cores * 2 threads) and Samsung 1TB SSD.
	
	\item \textbf{Software:} GNU/Linux Fedora 29 host OS and installed docker version 1.13.1.
\end{itemize}

We deploy 6 homogeneous BRISEv2 workers with LLHs on the same host machine to carry the problem solving process.
We run each experiment 9 times for 15 minutes to obtain the statistical data.


\section{Meta-heuristics Tuning}\label{eval: mh tuning}
As we conclude in \cref{bg: parameter setting conclution}, the goal of parameter control is to reach at least the quality of parameter tuning approaches. Therefore, before running the major set of evaluation experiments, we have to perform a parameter tuning for the underlying LLHs.

\subsection{Parameter Tuning System Configuration.} 
As a tuning system, we used our the implemented concept but in the tuner mode. As we described in \cref{concept: conclution}, to enable the parameter tuning mode, we built a search space based on the singe LLH with its parameters. In our particular case it were three search spaces for each underlying meta-heuristic respectively. We also disabled the solution transfer between each configuration, forcing LLH to use the OP each time from scratch.

For each LLH we run the tuning for 8 hours on 10 deployed worker nodes and three minutes for task evaluation. The underlying prediction mechanism was configured to use TPE with 100\% window size. We also disabled the repetition strategy (\emph{repeater} entity), leaving each configuration evaluated only once (with one task). We do so since our preliminary experiments have shown that the variance among evaluations is negligible. As one may expect, since the repetition strategy was disabled, outliers detection was turned off as well.

\subsection{Target Optimization Problem and Search Space of Parameters.} 
The role of target optimization problem was played by one of evaluated TSP instances: \emph{rat783}. We selected this instance because, it is a middle size problem, comparing all the used through evaluation OPs.

\paragraph{jMetalPy evolution strategy.} This meta-heuristic is implemented in a framework as na\"ive evolution strategy however, we found an important recombination mechanism missing therefore, the heuristic is performing mostly by means of the mutation operations. As a configuration, this ES implementation requires providing several hyper-parameters. Integer $\mu$ (\emph{mu}), which denotes the number of parents in the population, while integer $\lambda$ (\emph{lambda}) defines the number of offspring. We tune both parameters in ranges $[1..1000]$. Boolean \emph{elitist} defines the selection strategy, which true value enables elitist selection $(\mu+\lambda)$, while false disables the elitist selection $(\mu,\lambda)$ (more detains in \cref{BG: MH Examples}). Also, the framework proposes two possible \emph{mutation types} for combinatorial OPs: permutation swap and scramble mutation, which we use for tuning. The respective mutation probability is tuned in range $[0..1]$.

\paragraph{jMetalPy simulated annealing.} In this meta-heuristic authors defined the solution neighborhood by means of the same mutation operators, mentioned above. Thus, we use them and the same mutation probability range for tuning the SA. Unfortunately, the authors did not provide other but exponential cooling schedule and did not expose parameters temperature or alpha. This is the reason of such tiny parameter space for this MH.

\paragraph{jMetal evolution strategy.} The set of exposed hyper-parameters is almost the same, as we described for the Python-based MH implementation. The only difference that the mutation is represented only by one type, therefore we exclude it from the parameter space but leaving the mutation probability. All the other parameter ranges are the same as for the defined above ES.


\subsection{Parameter tuning results.} 
The process of parameter tuning is depicted in \cref{eval:pict:mh tuning}. During the session each MH was probed with at least $1.5k$ configurations. 


\svgpath{{graphics/Eval/tuning}}
\begin{figure}[h!]
	\centering
	\includesvg[width=\textwidth]{tuning progress}
	\caption{The low level heuristics parameter tuning process.}
	\label{eval:pict:mh tuning}
\end{figure}

In the figures below we propose a visual analysis of the parameter tuning results. For each meta-heuristic we separately present the numerical and categorical parameters.

The numeric hyper-parameters are showed as scattered points of parameter value (\emph{x-axis}) and the respective objective function result (\emph{y-axis}), obtained for configuration with this parameter value. Although such an isolated approach to analyze data in some cases may be error-prone, still it enough to get a birds-eye view on the existing dependencies. To represent trends among numeric parameter values we draw the regression line ($4^{th}$ degree) in green. At the top and to the right of the graph presented also the axis value densities. Thus, the density on a right side shows which objective values and how often were obtained, changing the underlying parameter, while the density on the top shows which parameter values were selected more often.

As for the categorical parameters, we plot their values as violin plots. It is a combination of box plot with the addition of a kernel density plot on each side. Since in our case, all categorical parameters of underlying algorithms have only two values, each violin plot shows which results of an objective function and how often were obtained. Using colors we depict different value of underlying parameter, while the shape of violin shows an expected result value and its probability. Inside the figure we also draw three dashed lines. A middle line with long dashes is a median, while lower and upper lines with short dashes show first and third quartiles respectively.


\paragraph{jMetalPy evolution strategy parameters.}
\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy numeric parameters}
	\caption{jMetalPy evolution strategy numeric parameters values.}
	\label{eval:pict:jmetalpy es numeric}
	\vspace{-20pt}
\end{figure}

\begin{figure}[h!]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{jMetalPy evolution strategy categorical parameters}
	\caption{jMetalPy evolution strategy categorical parameters values.}
	\label{eval:pict:jmetalpy es categoric}
	\vspace{-20pt}
\end{figure}

Looking on the \cref{eval:pict:jmetalpy es numeric}, one will see an explicit dependency between the number of parents (\cref{eval:pict:jmetalpy es numeric} parameter \emph{mu}) and the objective function: less amount of parents are tended to produce the better results. However, the dependency is such clearly observable for the number of offspring (\cref{eval:pict:jmetalpy es numeric} parameter \emph{lambda}). We may see, that a high number of offspring does not tend to provide good results, but the number of performed estimations for low \emph{lambda} is not enough to be strongly ensured that this value is better. Yet, even with small amount of observations we may make a guess that low \emph{lambda} is a good parameter choice. With respect to the mutation probability, it may be observed, that the higher mutation rates tend to produce a better results. 

As for the categorical parameters, one may see a strong bias towards bad results when using non-elitist algorithm version (\cref{eval:pict:jmetalpy es categoric} parameter \emph{elitist}). When concerning the mutation type, the dominance is not an obvious, but permutation version of mutation is slightly outperforms scramble type (\cref{eval:pict:jmetalpy es categoric} parameter \emph{mutation}).


\paragraph{jMetalPy simulated annealing parameters.}
\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\vspace{-10pt}
		\includesvg[width=\linewidth]{jMetalPy simulated annealing numeric parameters}
		\caption{Mutation probability.}
		\label{eval:pict:jmetalpy sa numeric}
	\end{subfigure}
	\hfil 
	%\vspace{-5pt}
	\begin{subfigure}{0.4\textwidth}
		\includesvg[width=\textwidth]{jMetalPy simulated annealing categorical parameters}
		\vspace{-5pt}
		\caption{Mutation type.}
		\label{eval:pict:jmetalpy sa categoric}
	\end{subfigure}
	\caption{jMetalPy simulated annealing parameters.}
\end{figure}


This heuristic were tuned by means of only two parameters: categorical mutation type, which results are presented in \cref{eval:pict:jmetalpy sa categoric} and numerical mutation probability with graphs in \cref{eval:pict:jmetalpy sa numeric}. One may see a strong dominance of permutation mutation type, while scramble produce an average but stable results. The mutation probability trends are also clear: higher parameter values produce better results. Indeed, the dependency on mutation probability is obvious, since the underlying algorithm is performing the search space traversal by means of solution mutation. The two lines of results, that could be viewed on the \cref{eval:pict:jmetalpy sa numeric} are correlated with the mutation type: lower corresponds to usage of permutation, while upper to scramble mutation.


\paragraph{jMetal evolution strategy parameters.}
\begin{figure}[h]
	\centering
	\vspace{-10pt}
	\includesvg[width=\textwidth]{jMetal evolution strategy numeric parameters}
	\caption{jMetal evolution strategy numeric parameters values.}
	\vspace{-15pt}
	\label{eval:pict:jmetal es numeric}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{jMetal evolution strategy categorical parameters}
	\label{eval:pict:jmetal es categoric}
	\caption{jMetal ES elitist parameter.}
	\vspace{-30pt}
\end{wrapfigure}

The final heuristic under investigation is the Java-based implementation of viewed above ES. Even if at the first glance the regression lines are not looking the same, the overall trends are similar: lower values of \emph{mu} parameter result in better objective, while the mutation probability should be kept high. On contrary to Python-based ES, here the middle-range values of parameter \emph{lambda} produce the best results. It may be explained by the fact of performance straggling in Python-based version: with large offspring number, the computational effort, required to accomplish the iteration increases, while Java-based version could handle it. A dominance of elitist version of algorithm is non-obvious, but this could be seen from a distribution first quartile.

\paragraph{}
We collected the best performing configurations of each meta-heuristic and presented them in \cref{eval: params jmetalpy es}. We also highlight here the default parameter values, which were selected with motivation of being in the middle of the values ranges.

\begin{table}%[h!]
	\centering
	\begin{tabular}{r||c|c|c}
		\textbf{Hyper-parameter} & \textbf{Default value} & \textbf{Tuned value} & \textbf{Estimated range} \\
		\hline
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 22 & $[1..1000]$ \\
		\emph{elitist} & False & True & {True, False} \\
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetalPy simulated annealing} \\
		\hline
		\emph{mutation type} & Permutation & Permutation & {Permutation, Scramble} \\
		\emph{mutation probability} & 0.5 & 0.89  & $[0..1]$\\
		\hline
		\rowcolor{gray!10}
		\multicolumn{4}{c}{jMetal evolution strategy} \\
		\hline
		$\mu$ & 500 & 5 & $[1..1000]$ \\
		$\lambda$ & 500 & 605 & $[1..1000]$ \\
		\emph{elitist} & False & True  & {True, False}\\
		\emph{mutation probability} & 0.5 & 0.99 & $[0..1]$ \\
	\end{tabular}
	
	\caption{Static hyper-parameters of low-level meta-heuristics.}
	\label{eval: params jmetalpy es}
\end{table}




\section{Concept Evaluation}\label{eval: concept}

\subsection{Evaluation Plan}\label{eval: concept plan}
To evaluate the performance of developed approach we firstly need to define the base line. In most cases it is the single meta-heuristics, which are solving the OP using static hyper-parameters. However, to evaluate the parameter control feature we must make a closer look on the performance of each separate heuristic with static and dynamic hyper-parameters. For selection hyper-heuristic analysis we compare the performances of all underlying MHs running separately and together within a hyper-heuristic. Note, in this case the hyper-parameters are statically defined. And last, but not least, to evaluate a selection hyper-heuristic with enabled parameter control we compare it to separately running underlying meta-heuristics with parameter control and to selection hyper-heuristic.

In order to organize the evaluation plan, we distinguish two stages.
At the first stage the LLH selection occurs, while at the second one we chose hyper-parameters for the selected LLH. At each stage we may use different prediction approaches, which description could be found in \cref{impl: prediction models}. To select the LLH, apart from random and static selection we also use FRAMAB (see \cref{impl: FRAMAB}) and Bayesian ridge regression model implementation from Scikit-learn framework (see \cref{impl: sklearn wrapper}). Note, for the Bayesian ridge regression model we use a default parameters, which could be found in the framework documentation~\footnote{~\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html}}. To select the hyper-parameters for LLHs, apart from static default and tuned variants we also use random selection, available in BRISEv2 TPE and the mentioned above Bayesian ridge. The set of used techniques is presented in the \cref{eval: concept settings table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{l||l}
		\textbf{LLH selection} & \textbf{LLH parameters selection} \\
		\hline
		\hline
		\textbf{1.} Random & \textbf{1.} Default \\
		\textbf{2.} Multi-armed bandit & \textbf{2.} Tuned beforehand \\
		\textbf{3.} Bayesian ridge regression & \textbf{3.} Random \\
		\textbf{4.1.} Static jMetalPy.ES & \textbf{4.} Tree Parzen Estimator (TPE) \\
		\textbf{4.2.} Static jMetalPy.SA & \textbf{5.} Bayesian ridge regression (BRR) \\
		\textbf{4.3.} Static jMetal.ES & 
	\end{tabular}
	
	\caption{Prediction techniques used for the concept evaluation.}
	\label{eval: concept settings table}
\end{table}


Using this table, we now could pick a prediction technique to form a desired system configuration. For instance, mentioned above baseline could be encoded into configurations starting from \emph{4.1.1} for the evolution strategy from jMetalPy framework, running with default hyper-parameters and ending with \emph{4.3.2} for evolution strategy from jMetal framework, running with tuned beforehand parameters.

Our benchmark plan for the concept evaluation looks as a set of following experiment groups:
\begin{itemize}
	\item \textbf{Meta-heuristics (MH).} The baseline. We evaluate each used meta-heuristic separately with default and tuned hyper-parameters: \emph{4.1.1} and \emph{4.1.2} for jMetalPy evolution strategy;  \emph{4.2.1} and \emph{4.2.2} for jMetalPy simulated annealing; \emph{4.3.1} and \emph{4.3.2} for jMetal evolution strategy respectively.

	\item \textbf{Meta-heuristics with parameter control (MH-PC).} The set of experiments dedicated to verify an impact of the generic parameter control on meta-heuristics performance. A selected set of experiments looks as follows: \emph{4.1.3, 4.2.3, 4.3.3} to investigate the influence of random parameter allocation; \emph{4.1.4, 4.2.4, 4.3.4} to check TPE-based parameter control and \emph{4.1.5, 4.2.5, 4.3.5} to probe Bayesian-ridge-based parameter control.

	\item \textbf{Selection hyper-heuristic with static parameters (HH-SP).} These benchmarks are dedicated to an investigation of the implemented on-line selection HH performance. It implies the LLHs usage with static parameters therefore, we evaluate HH-SP performance with default and tuned beforehand LLH parameters. Experiment codes are following: \emph{2.1, 2.2} for FRAMAB-based HH-SP and \emph{3.1, 3.2} for Bayesian-ridge-based HH-SP.
	
	\item \textbf{Selection hyper-Heuristic with parameter control in LLH (HH-PC).} This is a final set of benchmarks for concept evaluation. By this we evaluate an influence of simultaneous on-line LLH selection and parameter control on system performance. The respective experiment set is following: \emph{1.3, 2.4, 2.5, 3.4, 3.5.}
\end{itemize}

The aggregated concept benchmark plan is presented in \cref{eval: concept benchmark plan table}.
\begin{table}[h!]
	\centering
	\begin{tabular}{c||p{3cm}}
		\textbf{Experiment group} & \textbf{Related codes} \\
		\hline
		\hline
			
		\multirow{2}{*}{MH} & 4.1.1., 4.2.1, 4.3.1 \newline 4.1.2, 4.2.2, 4.3.2 \\
		
		\rowcolor{gray!10}
		\multirow{3}{*}{MH-PC} & 4.1.3, 4.2.3, 4.3.3 \newline 4.1.4, 4.2.4, 4.3.4 \newline 4.1.5, 4.2.5, 4.3.5 \\
		
		\multirow{3}{*}{HH-SP} & 1.1, 1.2 \newline 2.1, 2.2 \newline 3.1, 3.2 \\

		\rowcolor{gray!10}
		\multirow{3}{*}{HH-PC} &  1.3 \newline 2.4, 2.5 \newline 3.4, 3.5 \\
	\end{tabular}
	
	\caption{Concept benchmark plan.}
	\label{eval: concept benchmark plan table}
\end{table}


\subsection{Concept Evaluation Results}\label{eval: concept results}
\subsubsection{Baseline Evaluation}\label{eval: concept baseline}
As we discussed previously, our results comparison should be done against the defined baseline. Therefore, this section is dedicated to review of the meta-heuristics performance out-of-the-box on different problem sizes and parameter settings. For visibility reasons we plot the intermediate and the final performance evidences for each problem instance separately, since they naturally imply different result ranges.

Since we are tackling a set of TSP instances, which were previously solved by other exact solvers, we also present an optimal solution, available for each instance as a green dashed line.

%\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/baseline}}
\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{kroA100 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on kroA100.}
	\label{eval:pict:bl:kroA100 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}
	\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{kroA100 baseline final boxplot}
	\label{eval:pict:bl:kroA100 final}
	%\ffigbox[\FBwidth]
	\caption{Final results of meta-heuristics with static parameters on kroA100.}
	\vspace{-20pt}
\end{wrapfigure}
The TSP for 100 cities is a relatively small problem instance. Therefore, all underlying MHs reach a local optimum after first few minutes of the run and stuck there till the end, making a relatively small moves (\cref{eval:pict:bl:kroA100 intermediate}). Note, the bold line is a statistical mean of all 9 experiment runs, while a shadow around it is a confidence interval. One may observe how the parameter tuning affects different MHs: in some the difference is dramatic (ES), while others are almost not affected (\cref{eval:pict:bl:kroA100 final}).

An observation of a worse SA results with tuned parameters, in contrast to default values is explained by the fact that for algorithm tuning we used different problem instance (rat783). It only confirms a motivation of the parameter control approaches: tuning is not problem-instance-universal technique.

\newpage
\paragraph{pr439 TSP instance.} 
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pr439 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pr439.}
	\label{eval:pict:bl:pr439 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{pr439 baseline final boxplot}
	\label{eval:pict:bl:pr439 final}
	\caption{Final results of meta-heuristics with static parameters on pr439.}
	\vspace{-20pt}
\end{wrapfigure}
The next problem instance comprises a 4 hundred cities. Since the number of possible solutions increased, the required time for MHs to settle in a local optima is increased as well. We may see a similar to the previous experiment set trends: produced by SA results are almost not affected by the parameter tuning, however with tuned parameters the performance is slightly better. From the other side, Python version of tuned ES requires more time to converge in local optima, while Java-based reaches it after a couple of first iterations (\cref{eval:pict:bl:pr439 intermediate}).

The final result quality of underlying MHs is the same as on previously reviewed problem instance: MHs with tuned parameters (an SA with default) produce solutions of a similar quality (\cref{eval:pict:bl:pr439 final}).

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{rat783 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on rat783.}
	\label{eval:pict:bl:rat783 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{rat783 baseline final boxplot}
	\label{eval:pict:bl:rat783 final}
	\caption{Final results of meta-heuristics with static parameters on rat783.}
	\vspace{-20pt}
\end{wrapfigure}
This is an average size problem among reviewed in this thesis. The behavior of solvers in this case changes slightly. For instance, ES from jMetal framework with default parameters is unable to solve a problem however, while using an optimized hyper-parameters it quickly reaches a local optima (after ~50 iterations) with the best produced results (\cref{eval:pict:bl:rat783 intermediate}). Analyzing performance evidences of the other heuristics (Python-based), we may conclude that they did not reach a local optima in a given 15 minutes therefore, their final results are slightly worse (\cref{eval:pict:bl:rat783 final}). Taking into account previous problem instances we may guess that given enough time they will reach the results of jMetal ES. Note the decreased stability of jMetalPy ES reflected by a large confidence interval.

\newpage
\paragraph{pla7397 TSP instance.} 
The largest investigated here TSP instance for ~$7.4k$ cities is however, referred as a middle-size OP in used TSPLIB95. Here the performance evidences changed the most therefore, we discuss each MH separately.

\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pla7397 baseline progress}
	\caption{Intermediate results of meta-heuristics with static parameters on pla7397.}
	\label{eval:pict:bl:pla7397 intermediate}
\end{figure}

\begin{wrapfigure}{R}{0.5\textwidth}%\setcapindent{1em}
	%\centering
	\vspace{-20pt}
	\includesvg[width=\linewidth]{pla7397 baseline final boxplot}
	\label{eval:pict:bl:pla7397 final}
	\caption{Final results of meta-heuristics with static parameters on pla7397.}
	\vspace{-20pt}
\end{wrapfigure}

Python-based version of ES provide the worst results with both default and tuned parameters. Note the amount of performed iterations by this MH in a given 15 minutes — less than 100. It is affected by a several reasons. Firstly, the amount of time required to perform an internal iteration increased dramatically. Thus, even with specified 15 seconds for one task run, actually it requires much more (up to 1 minute) to accomplish a task. We have a several guesses, what may cause such a behavior. Firstly, it is a general Python performance issues, in most of the times caused by usage of global interpreter lock (GIL), which explanation is out of this thesis scope. Secondly, it may be caused by the algorithms code basis implementation in jMetalPy framework. To implement a generic termination criteria (and some other features) the authors utilized a push-observer design pattern~\cite{benitez2019jmetalpy} according to which the underlying algorithm (ES in our case) triggers its observers after each iteration. Therefore, stopping criteria may be evaluated only after finishing an iteration, which in case running ES with TSP for $7.4k$ cities, may take a while. We observed the ES algorithm termination after the first internal iteration, which causes a poor solution quality improvement. Naturally, there is also an overhead for the results sending through the network, but as Java-based ES with default parameter shows, this causes decrease only in ~100 configurations. We also eliminate the possible issue in a required time for problem loading (building distance matrix), since the worker node does it only once and stores its cached version. In any case, this issue requires deeper investigation, that we postpone to the future work. Running jMetalPy ES with tuned parameters fixes the issue with task delays and therefore, results in higher number of external iterations, but the solution improvement are still weak (see left picture on the \cref{eval:pict:bl:pla7397 intermediate}).

As in the previous case, the jMetalPy-based simulated annealing produce a good quality improvement, least depending on the hyper-parameter values. A resulting progress curve, presented in a central picture of \cref{eval:pict:bl:pla7397 intermediate} shows that SA requires more time to converge that was provided and is still far from its potential local optima.

The final evidences obtained from the jMetalPy ES show it dominance in intermediate and final results (see \cref{eval:pict:bl:pla7397 intermediate} and \cref{eval:pict:bl:pla7397 final} respectively). In this example we observe a dramatic impact of the solver parameter tuning. Using default configuration, ES is struggling in making improvements. Our guess here is the same as for the Python-based version — the number of internal iterations is extremely low a good search space traversal, but they are finishing much faster.


\paragraph{Discussion.} The observed results of meta-heuristics execution confirm the algorithm parameter setting problem importance, discussed in \cref{bg: section Parameters Setting}. An effect of proper parameter selection is different among algorithms. In our case, the performance of two out of three solvers are highly dependent on the hyper-parameter settings. Thus, an application of our generic parameter control approach to these algorithms is rather intriguing and may partially reveal the overall methodology benefits.

From the other side, we also observe the only one algorithm domination among the others with static parameters. See how all MHs were solving each TSP instance with default parameters. In each case SA outperforms two other ES. The usage of all three MHs in a selection hyper-heuristic with static (default) hyper-parameters will reveal the implemented selection HH applicability. In this case we expect to observe the results close to provided by pure SA. The other case — the application of MHs to the biggest TSP instance with tuned hyper-parameters. Here Java-based ES is the best. Thus, we expect to observe such a behavior of selection hyper-heuristic. Also, it should be rather interesting to see the impact of Python ES struggling with default parameters on HH.

%\newpage
\subsubsection{Generic Parameter Control}
As we discussed in \cref{bg: parameter control}, the goal of parameter tuning lays in adaptive changing of underlying algorithm parameters with to optimize some performance measurement. In our case, we apply the proposed in \cref{concept: conclution} methodology to set the parameters of anytime algorithms in a runtime. Here is a brief reminder: at each RL step HLH is analyzing the performance of solver with previous configurations to choose the parameters values, which hopefully lead to the higher solution quality improvements. Afterwards, we run the solver with sampled parameters for a predefined time (15 seconds) to get new evidences. 

Here we compare the performance of algorithms with statically defined default and tuned hyper-parameters to dynamically changing parameter values by means of RL control. As previously, we perform the comparison for each problem instance separately.

\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/control}}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{kroA100 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on kroA100.}
	\label{eval:pict:pc:kroA100 intermediate}
\end{figure}

Comparing to the baseline, parameter control in a small problem instance was able to reach and even outperform the results of MHs on static parameters after first 50 iterations (\cref{eval:pict:pc:kroA100 intermediate}). We may observe, that even randomly changing the parameters of heuristics (however, this random sample still emits valid configurations) in a runtime results in better solutions comparing with static parameters. It is caused by the changes in a neighborhood definition (mutation type) and traversal process (mutation probability). In most cases, given enough time the learning-based parameter assigning outperforms random allocation (\cref{eval:pict:pc:kroA100 final}).

Note the amount of configurations (iterations) performed by jMetal ES, which could be seen in \cref{eval:pict:pc:kroA100 intermediate}. According to our plan, a given time for MH run is 15 minutes, 15 seconds for running one configuration on 6 available workers. Thus, in the most optimistic case, the number of iterations should be $ \frac{15\cdot60\cdot6}{15} = 360$ but, we observe even more than 400. After an investigation, we came to conclusion that it is caused by an implementation flaw an insight of which is following. jMetal MHs provide only iteration-number-based termination criterion, which is not encapsulated how it is done in jMetalPy. For our needs we added also a time-based but did not remove the previously existing. For the iteration counter used a regular integer number, which we set up to its maximal value when using a time-based criterion. Given a specific `light' algorithm configuration (low $\mu$, $\lambda$ and mutation probability), with this OP MH is able to reach the maximal number of iteration in less than 15 seconds therefore, terminating early and triggering a new parameter control iteration. Certainly, it is our implementation bug, fix of which we postpone to the future work.

%Let us have a closer look on the jMetal ES. Two observations could be made. Firstly, the stability of results in case of random-based parameter tuning weak in comparison to model-based, which is a reasonable behavior. However, in case of jMetalPy ES, the TPE-based stability is less than random-based. This leads us to 

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{kroA100 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on kroA100.}
	\label{eval:pict:pc:kroA100 final}
\end{figure}

\newpage
\paragraph{pr439 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pr439 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on pr439.}
	\label{eval:pict:pc:pr439 intermediate}
\end{figure}

When parameter control is applied to a larger problem, it is starting to require more time for finding a better quality configurations for jMetalPy ES. In particular, only the TPE-based approach was able to reach tuned parameters results quality. The intermediate performance was reduced, since model-based tuning required more knowledge to find a good-performing settings. However, looking on the progress curves in \cref{eval:pict:pc:pr439 intermediate}, RL TPE-based control technique will probably be able to reach and outperform tuned beforehand parameters.

The case with jMetalPy SA shows (1) reduction in the final results qualities with controlled parameters (\cref{eval:pict:pc:pr439 final}) and moreover (2) unstable behavior of MH with TPE-based control (\cref{eval:pict:pc:pr439 intermediate}). As we concluded during the baseline results analysis, the parameter settings in this MH does not dramatically affect its performance therefore, the results of random-based parameter allocation are similar to model-based approaches.

On contrary, applying generic parameter control to jMetal ES MH leads to a better final results, comparing with static tuned parameters(\cref{eval:pict:pc:pr439 final}). Note, as for previous problem instance, even a random-based MH parameters allocation outperforms statically defined but, the required time to converge and a result floating increased as well. 
\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{pr439 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on pr439.}
	\label{eval:pict:pc:pr439 final}
\end{figure}

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{rat783 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on rat783.}
	\label{eval:pict:pc:rat783 intermediate}
\end{figure}

This is a problem instance, which was used to set the meta-heuristic parameters by means of off-line tuning. In this case, the parameter control in jMetalPy SA and jMetal ES did not manage to outperform the tuned parameters but only nearly reached their quality. All approaches, including the random selection resulted in a similar intermediate performance (\cref{eval:pict:pc:rat783 intermediate}).

For jMetalPy ES MH, only TPE-based parameter control produced good but rather unstable final performance, comparing to tuned hyper-parameters. The other (random- and BRR-based) approaches did not manage to select the parameters, which would perform well enough and therefore were left out of presented quality ranges in \cref{eval:pict:pc:rat783 final}.

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{rat783 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on rat783.}
	\label{eval:pict:pc:rat783 final}
\end{figure}

\newpage
\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\includesvg[width=\textwidth]{pla7397 PC progress}
	\caption{Intermediate results of meta-heuristics with parameter control on pla7397.}
	\label{eval:pict:pc:pla7397 intermediate}
\end{figure}

For the final problem instance we omit the parameter control results of jMetalPy ES since it did not manage to perform even slightest improvement in comparison to the default parameter values, presented in a baseline description. It is caused by a fact that in early stages our approach acts as a random search, since not enough evidences were obtained to build models for prediction. Thus, is case of jMetalPy ES, the MH was running with badly performing configurations and as we explained in \cref{eval: concept baseline}, did not manage to perform enough iterations to improve solution quality and was left out of this instance discussion. To resolve this issue, the time-based task termination methodology should be properly implemented in MH wrappers, but we postpone it for the future work.

As in previous cases, the least parameter-settings-sensitive jMetalPy SA shows an ability to perform well with any approach of the parameter settings (\cref{eval:pict:pc:pla7397 intermediate}). However, neither among them outperform tuned beforehand algorithm configuration in final results quality (\cref{eval:pict:pc:pla7397 final}).

As for jMetal ES, model-based approaches are outperforming the randomized parameter values allocation. Moreover, TPE model outperforms even the results of algorithm with tuned beforehand hyper-parameters.

\begin{figure}[b]
	\centering
	\includesvg[width=\textwidth]{pla7397 PC final boxplot}
	\caption{Final results of meta-heuristics with parameter control on pla7397.}
	\label{eval:pict:pc:pla7397 final}
\end{figure}

\newpage
\paragraph{Discussion.} In general, the review of meta-heuristic performance on different problem instances showed that the proposed generic parameter control approach is applicable. It is capable to improve a final algorithm performance in comparison to statically selected default parameters. Moreover, in some cases (all MHs with kroA100, jMetal ES with pr439 and pla7397) the algorithm showed better results than with tuned parameters.

Taking into account the results with random parameter allocation we conclude that learning mechanisms should and must be improved further by means of different surrogate models usage or proper optimization over surrogates. Leaving the improvement steps to future work we conclude that the proposed generic parameter control concept is able to produce better results while solving an unforeseen problem on-line.


\subsubsection{Selection Hyper-Heuristic with Static LLH Parameters}
The second mode of developed approach and at the same time a main goal of the thesis is a process of dynamic algorithm selection. It is implemented in form of described in \cref{concept: conclution} reinforcement learning-based on-line selection hyper-heuristic. In this part of evaluation we combine three available meta-heuristics with static parameter (default and tuned) into a single selection hyper-heuristic. Three approaches to select the LLH were investigated, in combination to two possible parameters settings it results in 6 combinations for each problem instance.

We present the process of problem solving in two forms. Firstly, we present the process of problem solving, distinguishing selected at each iteration LLH and the results, which it gave. For doing it, we selected only the first repetition (out of 9 available), since presenting all repetitions will not be possible to understand. Nevertheless, we present the final results of all runs in form of box-plots, comparing them to the underlying LLH performance. LLHs solely and combined into the selection hyper-heuristics are configured to use a static hyper-parameter values (HH-SP). On the left side we present final performance with the default parameter values, while on the right site — the tuned beforehand values.

\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/selection}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on kroA100 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:kroA100 intermediate}
\end{figure}
The intermediate results with random LLH allocation show us, as expected, unbiased uniform selection of LLH (codes 1.1, 1.2). We observe the same behavior with all HH-SP approaches, while they utilize LLHs with tuned parameter values (codes 1.2, 2.2, 3.2), which is motivated by an equal performance of all LLHs on this TSP instance. Thus, the point of interest here are LLHs with default parameter values (codes 2.1, 3.1 in \cref{eval:pict:hh-sp:kroA100 intermediate}). When FRAMAB is used as HLH, a preference in j.ES and py.SA appears. We observe a repetitive pattern in LLH allocation, when FRAMAB reaches a local optimum (code 2.1). It is caused by a deterministic essence of the algorithm. Being in a local optimum, FRAMAB's exploration mechanism fully guides a selection. Due to the usage of a similar time-based LLH termination, all workers are starting the next round in bunches. Thus, when a new round starts, FRAMAB operates on static information and allocates all next configurations with the same LLH. From the other perspective, when the process reaches a point, where the advantage changes towards another LLH, FRAMAB behaves inertly. One may argue this will cause a struggle in a performance, which is rather a logical conclusion. It requires a further investigation, which we are forced to postpone for the future work. In case of BRR usage, the bias dominates in j.ES however. According to presented in \cref{eval:pict:hh-sp:kroA100 final} final results statistics for this problem instance, given at least one dominating LLH (default parameters values case), even a random LLH selection could utilize to obtain a good final results.

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on kroA100 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:kroA100 final}
\end{figure}

\newpage
\paragraph{pr439 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on pr439 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pr439 intermediate}
\end{figure}
Evaluating the 439 cities instance solving process, we observe a similar behavior while using tuned hyper-parameters therefore, we do not draw our attention here: every LLH reach a local optimum in a couple of first iterations and no dominant LLH may be distinguished.

On contrary, when using the default parameter values a bias towards py.SA occurs, since it showed the best results improvement during the baseline evaluation. After reaching a local optima, FRAMAB-based HH-SP (code 2.1) frequently allocates two heuristics: py.SA and j.ES, while BRR-based (code 3.1) continues to allocate mostly py.SA (\cref{eval:pict:hh-sp:pr439 intermediate}). Comparing to the previous problem instance, BRR dramatically changed its behavior to utilize in the most cases the best performing py.SA. Thus, we can conclude the BRR HLH provides more exploitation capabilities, when compared to FRAMAB HLH.

The final result quality of all HH-SP on this problem instance are roughly similar to provided by the best performing py.SA with default parameters. Once again, all tuned LLHs separately provide a similar final result quality therefore, their combination in HH-SP leads to the same final quality (\cref{eval:pict:hh-sp:pr439 final}).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on pr439 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pr439 final}
\end{figure}

\newpage
\paragraph{rat783 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on rat783 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:rat783 intermediate}
\end{figure}
With the 783 cities instance heuristics are starting to perform in a slightly different manner even with the tuned parameters. As you remember from baseline evaluation, here the best performing was j.ES, afterwards appeared py.SA and lastly — py.ES. As a consequence, using FRAMAB HLH, HH-SP frequently utilizes j.ES (see code 2.2 in \cref{eval:pict:hh-sp:rat783 intermediate}). On contrary, BRR HLH utilizes all LLHs almost evenly. We may conclude that BRR was `confused' by performance evidences from other heuristics, since the process quickly converged into a local optima.

In the case with default parameter values, both learning models most frequently predicted the best performing py.SA, which is an expected behavior.

The final result quality of all model-based HP-SP are at least as good, as the solution quality provided by the best underlying LLH for both default and tuned parameter values (\cref{eval:pict:hh-sp:rat783 final}). Even in a case with the default parameters, a random allocation of one good performing LLH among three available (codes 1.1 and 1.2) results in a good final solution quality however, may require more time to converge (see intermediate and final performances for code 1.1).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:rat783 final}
\end{figure}

\newpage
\paragraph{pla7397 TSP instance.}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP progress}
	\caption{Intermediate performance of on-line selection hyper-heuristic with static hyper-parameters on pla7397 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-sp:pla7397 intermediate}
\end{figure}
Finally, the largest TSP instance tackled in this thesis. Our observations different MH heuristics on this problem were as following: py.ES with default parameters had the worst performance, while the tuned algorithm version was able to outperform only default parameters of j.ES. On contrary, j.ES with tuned parameters produced the best results, outperforming py.SA. The Python version of simulated annealing with both parameter settings produced average results, which were better than default version of j.ES and both py.ES. All these performance evidences are presented in \cref{eval:pict:bl:pla7397 intermediate}. Therefore, here we expected to observe a high frequency of usage: (1) py.SA for default parameter case, and (2) j.ES for tuned parameters. Naturally, it holds only for learning-based HH-SP.


\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-SP final boxplot}
	\caption{Final results of on-line selection hyper-heuristic with static hyper-parameters on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-sp:pla7397 final}
\end{figure}

The displayed chronic of LLHs allocation in \cref{eval:pict:hh-sp:pla7397 intermediate} completely match our expectations. In case of usage LLHs with default parameters, the most frequent choice of FRAMAB HLH was py.SA (code 2.1). We observe the same behavior of BRR HLH, but this model was also frequently selecting other LLHs such as py.ES(code 3.1). The results are happening to be unexpected, when comparing random allocation with BRR-based (codes 1.1 and 3.1): the frequent usage of py.ES in 1.1 `killed' the optimization process, while the same frequent MH calls in 3.1 were performing much better. Referring to the final results, presented in \cref{eval:pict:hh-sp:pla7397 final} we observe a high diverse in final result qualities when py.ES was allocated frequently (codes 1.1. and 3.1.). This is an unexpected behavior, especially for BRR HLH (code 3.1) that should be thoroughly examined, but due to the time limit we are forced to postpone the investigation for a future work.

As for the case with tuned LLHs, all selection approaches produced comparable to the best available LLHs final results. However, due to frequent usage of j.ES by FRAMAB and BRR, their final result quality slightly outperform random-based selection.


\paragraph{Discussion.} According to our observations of the developed selection hyper-heuristic performance we conclude that a proposed concept implementation performs as expected. Two utilized LLH selection approaches are performing differently when reach a local optimum. We claim the FRAMAB is a more perspective HLH, since after reaching local optima it starts to balance between previously seeing good performing LLH. In a contrary, BRR continues to select only the best performing heuristic. In case when the advantage of one LLH changes to another, BRR may require more time to learn this. Nevertheless, it is only our guesses, which need the evaluation proves.

Generally speaking, observed issues require not only a thorough investigation (pla7397 code 1.1, 3.1), but also a generic approach to handle a potential flaws in LLH performance that may cause struggling of overall HH-SP execution. The implemented system should be evaluated by means of HLH configuration influence on the performance. Also, a further investigation of adding several new LLHs should be evaluated by means of required computation effort to find good LLHs vs pure performance of these LLHs.


\subsubsection{Selection Hyper-Heuristic with Parameter Control}
The final evaluation is dedicated to performance analysis of the suggested approach of merging selection hyper-heuristic with generic parameter control in LLHs. A minimal goal of both (1) HH-SP and (2) MH-PC merge is to obtain the best algorithm (1) with tuned hyper-parameters (2) performance. In this evaluation set we follow a similar to previously used approach of intermediate results review distinguishing allocated LLH types at each iteration and comparing the final results quality with a baseline. Once again, for the intermediate results we outline only a single repetition, while the final results are compared by means of all 9 repetitions. As specified in the evaluation plan (\cref{eval: concept plan}), for the LLH selection we use three approaches: random, FRAMAB and BRR sampling, while for setting the LLH parameters we use the same random and BRR sampling, but apply TPE instead of FRAMAB.

\newpage
\paragraph{kroA100 TSP instance.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-PC progress}
	\caption{Intermediate performance of HH-PC on kroA100 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-pc:kroA100 intermediate}
\end{figure}
During the solving process a similar to HH-SP patterns of algorithm allocation may be observed for both FRAMAB-based (codes 2.4, 2.5) and BRR (codes 3.4, 3.5) HLHs. However, in this case the intermediate results are differing, since the parameter control has started to search for good LLHs settings (\cref{eval:pict:hh-pc:kroA100 intermediate}). For instance, consider the appeared `noise' in the results of both evolution strategies heuristics. It is caused by a boolean parameter \emph{elitist}, which defines the selection strategy. If the values is \texttt{false}, the outcome of heuristic run may be a dramatic decrease in solution quality (a more detailed description could be found in \cref{BG: MH Examples}). We may observe a case, when py.ES using a \texttt{elitist=true} parameter setting made an improvement in solution quality, see \cref{eval:pict:hh-pc:kroA100 intermediate}, code 3.4, around 50n iteration. TPE model (used to tune parameters in code 3.4) learned it and did not use an \texttt{elitist=false} parameter combination for this MH later in this experiment: only a j.ES-based noise appearing afterwards. Simultaneously, BRR model (used to select LLH in code 3.4) learned the improvement done by py.ES and selected it more frequently.

Since kroA100 is a relatively small TSP instance, the final performance of all approaches, including random LLH and parameter selection approach (code 1.3) are roughly the same (\cref{eval:pict:hh-pc:kroA100 final}). Nevertheless, all of them outperform a baseline, even with the tuned parameters (4.\{1,2,3\}.1 for default and 4.\{1,2,3\}.2 for tuned py.ES, py.SA, j.ES respectively in \cref{eval:pict:hh-pc:kroA100 final}).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{kroA100 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on kroA100 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:kroA100 final}
\end{figure}

\newpage
\paragraph{pr439 TSP instance.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-PC progress}
	\caption{Intermediate performance of HH-PC on pr439 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-pc:pr439 intermediate}
\end{figure}
With this TSP instance the learning process quickly converged in a local optima without a making an improvement moves. Because of this, the LLH allocation schedule, performed by FRAMAB (codes 2.4 and 2.5) is similar to one observed in HH-SP: two solvers were repeatedly executed, one is j.ES and the another is py.SA (\cref{eval:pict:hh-pc:pr439 intermediate}). In a contrary, BRR LLH selector did not manage to find an outperforming solver, therefore, was executing them uniformly (codes 3.4, 3.5), similarly to a pure random approach (code 1.3). Talking about the same distinguishable \emph{elitist} parameter, TPE-based parameter control algorithm in both cases (codes 2.4, 3.4) found out that it rarely provide a good solution quality, therefore was setting it to \texttt{elitist=false} less frequently. 

Comparing the final results quality of baseline with implemented HH-PC on this problem instance, we observe a similar to tuned MHs (codes 4.\{1,2,3\}.2) performance in all cases (\cref{eval:pict:hh-pc:pr439 final}). Once again, the purely randomized selection of both LLHs and their parameters gave a good performance (code 1.3), which may be explained by a random allocation of py.SA, which is less sensitive to parameter settings. During these calls, the algorithm managed to produce the most valuable improvements to the solution quality (see the beginning of solving process in \cref{eval:pict:hh-pc:pr439 intermediate} code 1.3).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pr439 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on pr439 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pr439 final}
\end{figure}

\newpage
\paragraph{rat783 TSP instance.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC progress}
	\caption{Intermediate performance of HH-PC on rat783 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-pc:rat783 intermediate}
\end{figure}
With the third problem instance namely the solving process is becoming more demonstrative.
Let us firstly draw attention towards HH-PC with FRAMAB for LLH and TPE for parameter sampling. At the beginning of solving process, j.ES was performing extremely well, for this reason FRAMAB was sampling it the most frequently (see beginning of experiment encoded by code 2.4 in \cref{eval:pict:hh-pc:rat783 intermediate}). When a solving process in j.ES reached its local optima, FRAMAB started to use other heuristics by means of exploration. From an absence of noise its may be seeing, that TPE has set the most perspective parameters to LLHs (codes 2.4 and 3.4), but they have reached their local optima. In such case, the parameter search should be biased towards exploitation. To do so, we may need to introduce new and different progress evidences, such as stagnation detection (which is used in EA parameter control~\cite{karafotias2014generic}). Unfortunately, due to a lack of time, we postpone this enhancement for a future work.

The final performance evidences of the proposed concept implementation, unfortunately, do not reach the desired level. More concretely, in \cref{eval:pict:hh-pc:rat783 final} we clearly see the dominating j.ES MH with tuned parameters (code 4.3.2). As we claimed in the beginning of HH-PC performance review, the expected minimal final solution quality of HH-PC should be as good as tuned j.ES. But we observe an averaged performance among all available tuned LLHs (codes 4.1.2, 4.2.2, 4.3.2 for tuned MHs vs codes 1.3, 2.4, 2.5, 3.4, 3.5).

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{rat783 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on rat783 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:rat783 final}
\end{figure}

\newpage
\paragraph{pla7397 TSP instance.}
\svgpath{{graphics/Eval/hhpc}}
\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC progress}
	\caption{Intermediate performance of HH-PC on pla7397 (single experiment).}
	\vspace{-10pt}
	\label{eval:pict:hh-pc:pla7397 intermediate}
\end{figure}

The final and largest problem instance, which should relieve the proposed and implemented concept applicability fully.

As we observed in previous experiments, py.ES with non-tuned parameters may cause a struggling in hyper-heuristic performance (see \cref{eval:pict:bl:pla7397 intermediate} and the discussion below \cref{eval:pict:pc:pla7397 intermediate}). Thus, during these benchmarks py.ES made a crucial change in the overall number of iterations where it was used many times (codes 1.3, 3.4, 3.4 in \cref{eval:pict:hh-pc:pla7397 intermediate}). While the case of fully randomized HH-PC (code 1.3) is clear, a BRR LLH selection did use this LLH frequently, since it produced a good results as a result of parameter control behavior. In a contrary, FRAMAB LLH selection in combination with TPE parameter tuning (code 2.4) managed to find good parameters for py.SA, therefore, allocated frequently. When py.SA reached its local optimum, FRAMAB triggered other LLHs usage and as a result, switched to j.ES's usage, which gave dramatic result improvements. The FRAMAB LLH selection with BRR parameter control (code 2.5) at the beginning of run was using the mixture of mainly two j.ES and py.SA, but later switched to py.SA-only mode. As we observe, this gave a fast coarse-grained solution improvement in the beginning, and stable, but rather slow improvement in a later stage.

\begin{figure}[b]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC final boxplot}
	\caption{Final results of HH-PC compared with MH on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc:pla7397 final}
\end{figure}

On the final results quality charts (\cref{eval:pict:hh-pc:pla7397 final}) we observe a dominance of FRAMAB-based LLH selection (codes 2.4, 2.5) and TPE-based parameter control (codes 2.4 and 3.4) usage. However, the results obtained with BRR-based parameter control approach (codes 2.5 and 3.5) are more stable than TPE-based (codes 2.4 and 3.4). Once again, the final results did not reach a desired performance of the best performing tuned j.ES (code 4.3.2). However, since the optimization process did not settle in a local optima, we can not doubt that HH-PC will not outperform j.ES. given more time. 

For a better intuition, let us draw the reader attention to \cref{eval:pict:hh-pc vs jES on pla7397 process}. Note how HH-PC with BRR LLH selection and TPE parameter tuning is approaching j.ES. If not the issue with a number of iterations, it would outperform the j.ES. However, in the current implementation given 15 minutes for all solvers j.ES managed to make more iterations and moved further. We postpone this discussion to the second main part of this chapter, dedicated to a coarse-grained HH-PC parameter settings influence evaluation.

\begin{figure}[t]
	\centering
	\vspace{-20pt}
	\includesvg[width=\textwidth]{pla7397 HH-PC vs jES progress}
	\caption{HH-PC and tuned jMetal ES solving process comparison on pla7397 (statistic of 9 runs).}
	\vspace{-5pt}
	\label{eval:pict:hh-pc vs jES on pla7397 process}
\end{figure}

\subsection{Conclusion on Concept Evaluation}\label{eval: concept eval conclusion}
In previously performed analysis the suggested generic parameter control (MH-PC) proved its concept, while implemented dynamic selection of heuristics with static parameters (HH-SP) performed according to our expectations. As discussed during the literature review in \cref{bg: parameter tuning,bg: hh}, each approach has a lack of another's advantageous features. Selecting good MH parameters, MH-PC will not outperform a better heuristic, while selected good algorithm with a poor parameter setting will not be able to compete with properly tuned one. Therefore, in proposed approach to merge both algorithm selection and parameter control in HH-PC we expected to observe 

We preform a comparison of HH-PC with MH-PC and HH-SP afterwards.


\section{Hyper-Heuristic with Parameter Control Settings Evaluation}\label{eval: hh-pc}
\subsection{Evaluation Plan}\label{eval: hh-pc plan}
\subsection{Evaluation Results}\label{eval: hh-pc results}



\section{Conclusion}\label{eval: conclution}
