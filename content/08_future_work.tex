\chapter{Future work}
\paragraph{add more sophisticated models}
\paragraph{dependencies / constraints in search space}
\paragraph{add new class of problem (jmetalpy easly allows it)}
\paragraph{evaluation on different types and classes}
\paragraph{adaptive time for tasks}
\paragraph{bounding LLH by number of evaluations, not time}
\paragraph{interesting direction: apply to automatic machine learning problems solving, compare to auto-sklearn.}
\paragraph{technique to optimize obtained surrogate model should be generalized}
\paragraph{investigate more deeply decoupling of data preprocessing and learning algorithm} auto-sklearn
\paragraph{influence for warm-start onto this kind of HH (by off-line learning)}
Although, the influence of meta-learning, applied in Auto-Sklearn system~\cite{feurer2015efficient} to warm-start the learning mechanism proved to worth the effort spent, as well as it was reported by developers of Selective Hyper-Heuristics with mixed type of learning~\cite{uludaug2013hybrid,}, it is intriguing to check an influence of metal-learning onto Selective Hyper-Heuristics with Parameter Control.

\paragraph{Random Forest HLH surrogate model}
% TODO: try a random forest as a 1ST level of HH, SMAC paper (short version), PAGE 7, CITES 18, 19.

\paragraph{add other learning metrics}
Inspiration could be found at:
- EAs in~\cite{karafotias2014generic}: progress stagnation, 

\todor{consider merging with conclusion, if too short}