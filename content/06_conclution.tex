\chapter{Conclusion}\label{conclusion}
Before making a final conclusion, let us briefly remind our objective.
The task of this thesis was defined as follows: using an existing parameter tuning software propose a concept to (1) perform the parameter control in meta-heuristics on a generic level and (2) making both algorithm selection and parameter control solve the optimization problem at hand.

During our research the complex objective was split into several compound tasks, which were formulated in three research questions (\cref{intro: research objective}), which we answer here explicitly.

\paragraph{RQ1} \emph{Is it possible to perform the algorithm configuration at runtime on a generic level?}
	
During the review of dedicated to parameter setting problem studies, we found that most of the generic approaches are related to parameter tuning. In other words, the search of a proper configuration is made at design time. The idea of adapting algorithm parameters arose in field of evolutionary algorithms and spread into other meta-heuristics, however, in algorithm-dependent manner. \todoy{is it needed?}

The proposed in this thesis generic parameter control approach relies on two aspects. Firstly, it should be possible to evaluate the performance of system under control with specified configuration at any time. Ideally, is to limit the system run with specified beforehand budget (number of iterations, wall-clock time, etc). Secondly, it should be possible to change the target algorithm configuration and proceed with execution basing on previously obtained results. We use the reinforcement learning methodologies to traverse the parameter space evaluating the performance of unforeseen configurations iteratively, while solving the problem at hand. The proposed concept of generic parameter control was examined in \cref{eval:1:PC} with three meta-heuristics: two Python-based algorithms, namely, simulated annealing and evolution strategy and one Java-based evolution strategy. The proposed approach revealed its applicability by reaching, and in some cases even outperforming the results of tuned in offline algorithm parameters. Please note, the use-cases of our concept are defined by the algorithms, which execution time is much larger than parameter control routines (see \cref{concept:parameter control}).

\paragraph{RQ2} \emph{Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?}
	
Our idea of merging those two problems lays in treating the algorithm type as a regular categorical parameter in the search space. By utilizing the parent-child relationships we define dependent parameters in such search space and perform the selection by means of firstly sampling the independent parameters and hiding the children, and secondly fixing the selected for parents values and exposing the \emph{activated} children parameters to proceed with prediction. The proposed step-wise process  of configuration construction provides a possibility to utilize a wide range of surrogate models for learning the dependencies among parameter values on each level in isolation. The requirements and use-cases of the proposed approach remain the same as for the defined above generic parameter control technique.
	
\paragraph{RQ3} \emph{What is the effect of selecting and adapting algorithms while solving an optimization problem?}

According to our expectations, the proposed approach should expose the performance comparable to the best among available for selection algorithms with tuned parameters. The performed in \cref{eval:1:hh-pc} evaluation and analysis of the simultaneous runtime algorithm selection and parameter control revealed its applicability. More concretely, given the same wall-clock time and environment setup, HH-PC was able to produce a better results than the best underlying tuned meta-heuristics for kroA100 TSP instance. For slightly larger pr439 example, HH-PC results were comparable with the tuned in offline meta-heuristics performance. With rat783 TSP instance HH-PC only in some cases reached the best available solver, used in isolation with tuned parameters. The experiments with pla7397 TSP instance shown an increasing gap between the HH-PC final performance and the best tuned meta-heuristic. However, analyzing the system and meta-heuristics behavior we found several issues not only in our implementation, but also in one of the underlying heuristics. Our evaluation of the developed HH-PC parameters influence in \cref{eval:2} showed that: (1) we did not use the best-performing settings for the used surrogate models; (2) RL routines configuration was not correct by means of LLH execution budget (task time), number of transferred solutions and forgetting mechanism. Therefore, our conclusion on the outperforming results of merging both problems for solving at runtime cannot be confidently assured yet. Nevertheless, the positive effect was definitely obtained even with the largest TSP instance: our approach is doing worse than the best tuned meta-heuristic, however, dramatically outperforms randomized LLH and its parameters allocation, which is the only available by this time online approach.

\paragraph{}
An explicit list of this thesis contributions is following:
\begin{enumerate}
	\item The concept of reinforcement learning-based generic parameter control in meta-heuristics was proposed. Empirical evaluation on three meta-heuristic implementations proved its ability to reach the performance of tuned in offline parameters.

	\item We proposed the concept of reinforcement learning-based online selection hyper-heursitic with parameter control in low-level meta-heuristics. The experiments demonstrated its applicability, however, the concept requires more thorough evaluation.

	\item The usability of an existing SPL for parameter tuning BRISEv2 was extended with aforementioned cases without losing the flexibility of usage a wide range of learning models. Also, the concept of data preprocessing was encapsulated.
\end{enumerate}

We consider the thesis task to be accomplished and the proposed reinforcement learning-based generic parameter control and algorithm selection approaches to be useful for solving the optimization problems with help of meta-heuristics.