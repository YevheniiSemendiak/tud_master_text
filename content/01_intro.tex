\chapter{Introduction}\label{intro}

\section{Motivation}
Traditionally, optimization problem (OP) in general and combinatorial OP in particular may be tackled by a number of different approaches, among which \emph{exact solvers}, \emph{approximate solvers} and \emph{heuristics} are commonly highlighted~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}. When it comes to heuristic solvers, their performance is defined by the provided strength in exploration and exploitation (EvE) and in the exposed balance between them. The EvE balance may be controlled on a several levels. Firstly, one could try to set a proper values of the exposed by an algorithm \emph{hyper-parameters}. This process is formalized under the notion of \emph{parameter settings problem} (PSP), which resolution can be done before running the algorithm (design time), or while it solves the OP (runtime). The former approach is also called \emph{parameter tuning} and can be tackled by numerous universal tuning systems ~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. A key assumption of this software is an expensiveness of the target system evaluation. It is tackled by the creation of a surrogate learning model, which simulates direct evaluations. The later approach, namely, \emph{parameter control} originally was introduced for evolutionary algorithms~\cite{karafotias2014parameter} and nowadays appears in an \emph{algorithm-dependent} manner. However, even a proper parameter setting may not lead to the best results for the problem at hand. This issue was formalized by the \emph{no-free-lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which states that ``all search algorithms have the same average performance over all possible optimization problems''. The second way for reaching the EvE balance is a proper algorithm selection. It resolves the consequence of NFLT and was formalized as \emph{algorithm selection problem} (ASP). The commonly used approach for ASP solving is the hyper-heuristic utilization. Depending on the learning time, they may perform low-level heuristic selection before actual problem solving, or at the runtime~\cite{burke2019classification}. To operate in on-line, hyper-heuristics often utilize a reinforcement learning (RL) techniques~\cite{moriarty1999evolutionary,mcclymont2011markov}, while for the design time a regular parameter tuning could be used.

The research does not stand still and nowadays the attempts of merging ASP and PSP into the united APSP are actively making. For instance, in an \emph{automatic machine learning} field such union was formalized as the \emph{combined algorithm selection and hyper-parameter optimization} problem (CASH)~\cite{thornton2013auto}, while for heuristics an explicit studies of APSP merging and solving at the runtime were not found. To tackle ML CASH problem several frameworks based on the existing parameter tuning systems were created~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}. However, those solutions are not applicable in case of heuristics, since they are (1) purely related to ML field and (2) acting at the design time due to ML techniques specifics. Naturally, to solve APSP for heuristics in off-line one may try to follow the ML approach of joining ASP and PSP into united APSP search space and utilizing the same parameter tuning systems. An important characteristic of such spaces appears, prohibiting treating them as a \emph{flat} structures. Since one solver type requires only its specific parameters and prohibits the other, a resulting configuration space happen to be `sparse'. Even if the surrogate models of the reviewed tuning systems treat the aforementioned dependencies properly (still, with help of some tricks), the proposed surrogate optimization techniques in an extremely sparse spaces may straggle while searching a good quality configuration. However, when it comes to runtime, the aforementioned struggling may play a settle role and, what is the most important, the universal technique for parameter control in heuristics was not proposed yet.

\section{Research objective}\label{intro: research objective}
At this point, we would like to define the \emph{goal of this thesis}. Basing on the existing parameter tuning software, use it to (1) perform the parameter control in meta-heuristics on the generic level and (2) tackle both ASP and PSP while solving the optimization problem at hand. In other words, to propose an \emph{on-line} approach for solving \emph{both} PSP and ASP problems for heuristic solvers.

In order to fulfill the defined above goal, the following \textbf{research questions} should be answered:
\begin{itemize}
	\item \textbf{RQ 1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	
	\item \textbf{RQ 2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	
	\item \textbf{RQ 3} What is the effect of selecting and adapting algorithm while solving an optimization problem?
\end{itemize}


\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP into a single problem. For doing so, we firstly introduce a generic runtime PSP solution; secondly we propose joining several PSPs search spaces into united APSP on an algorithm type.

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and prediction process. For the APSP representation we propose the usage of data structure, similar to feature trees from software product lines field. Doing so we treat solver type and its hyper-parameters uniformly as a regular parameter, however of the different types (categorical vs numerical). The dependencies between parameters in such search space are explicitly handled in form of parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level remain (categorical) parameter defining algorithm type, and on the level(s) below a respective hyper-parameters (categorical and numerical). The prediction process is made level-by-level. Thus, in the united APSP we firstly build a surrogate model for the algorithm type prediction. Afterwards, when the solver type is selected, we filter available information to operate on only relevant to the selected algorithm type data. With this filtered data we build a surrogate for the second level and predict the parameters for selected algorithm. The dependencies among algorithm-related parameters are also treated in form of parent-child relationship, therefore, we proceed the level-wise prediction proceeds until obtaining a completed configuration. In our work we utilize RL techniques solve the underlying OP iteratively on-line. On each step, according to available performance evidences for current optimization session, we select among any-time meta-heuristics (portfolio of algorithms), predict its best-performing hyper-parameters all by means of created for each layer surrogate models. After constructing such configuration, we continue solving the underlying OP to obtain new evidences for the next iteration.

The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader background knowledge in the field of optimization problems, solver types focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find the description of the proposed approach for generic parameter control and APSP problem unification. The structural and functional requirements for the search space representation and the sampling process are also located there. \cref{impl} is dedicated to the review of implementation details, among which a code basis selection, aforementioned requirements realization and a work-flow representation. The evaluation results of the proposed concept and their discussion could be found in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
