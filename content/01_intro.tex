\chapter{Introduction}\label{intro}

\section{Motivation}
Traditionally, optimization problem (OP) in general and combinatorial OP in particular may be tackled by a number of different approaches. Besides an exhaustive brute-force search (comparison of all possible solutions) a more convenient methodologies do exist. Most of the frequently used approaches can be grouped into three large families~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}: \emph{exact solvers}, \emph{approximate solvers} and \emph{heuristics}. The first two among them are build on mathematical proofs and provide a guarantee on the solution quality, while the later is often based on the usage of domain knowledge and stochastic processes to guide the search. 

Both families have their pros and cons, which are crucial when the methodology selection arise. For instance, the exact or approximate solver creation is often an expensive and complicated process, which in most cases is firstly arises in field of theoretical computer science and only afterwards, after reviewing and accepting by the research community may be widely implemented in form of software~\cite{woeginger2003exact,roubivcek2011relaxation}. The drawback of expensive algorithm creation and hardness in application is payed-off by a provided solution quality. On a contrary, the third family, namely, the heuristic solver can be viewed as an approach for some practical method utilization that does not provide any guarantees on the solution quality. However, the advantages of this family is the simplicity of application and scalability to larger problem sizes~\cite{festa2014brief}. Taking into account strengths and drawbacks of aforementioned optimizer families we consider the usage of heuristic algorithm in this thesis.

With respect to implied domain knowledge dependencies and availability of generic search guidance mechanisms for heuristics could be divided into three types: \emph{simple-}, \emph{meta-} and \emph{hyper-heuristics}. Scientists in this field distinguish two crucial characteristics of performed by heuristics actions: \emph{exploration} and \emph{exploitation}. The former could be explained as an investigation process in unforeseen search space zones in order to find a new and hopefully better solution, while the later characterizes a previously obtained information usage for currently available solution improvement. Since the provided budget for the optimization is often strictly limited, the exploration vs exploitation (EvE) dilemma arises. For finding solutions with a good quality and as a consequence, to be broadly applicable, the heuristic solver should supply a sufficient EvE balance, which depends on the underlying OP.

The EvE balance is controlled on a several levels. Firstly, the majority of algorithms is configurable by means of exposed parameters, which are also often called \emph{hyper-parameters}. The proper selection of algorithm parameters itself is an OP and named \emph{parameter settings problem} (PSP). Resolution of this problem often requires a considerable effort and has a great influence on the algorithm performance~\cite{lavesson2006quantifying}. With respect to tackling time, PSP resolution may be performed \emph{before} running the algorithm (design time), or \emph{while} it solves the OP (runtime). The former approach is often called \emph{parameter tuning}, where numerous systems was already created to solve the problem on a generic level~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. On a contrary, the later approach, namely, \emph{parameter control} was introduced by the research community of evolutionary algorithms~\cite{karafotias2014parameter}.

Nevertheless, even a proper parameter settings does not lead to the algorithm instantiation, which is the best among available. This issue was formalized in the \emph{no free lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which roughly states that ``all search algorithms have the same performance, when their results are averaged over all possible optimization problems''. However, the second approach, which resolves the consequence of NFLT and affects the aforementioned EvE balance is the algorithm selection itself. Naturally, some algorithms are better suited for one OP type, while the other are designed for another OPs. The algorithm selection problem (ASP), similarly to PSP, may be resolved at design-time, or at runtime. One of the most common approach for ASP solving is the usage of hyper-heuristics, which, depending on the learning-time, may be on-line and off-line~\cite{burke2019classification}. 

Up until now, we discussed the PSP and ASP problems separately, however, the research does not stand still and nowadays the attempts to merge the two problems are actively making. Concretely, in the \emph{automatic machine learning} field, such problem was formalized as \emph{combined algorithm selection and parameter setting} (CASH) problem in~\cite{thornton2013auto}. Several ML-related frameworks were created to resolve it~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}, however, (1) they are purely related to ML field and (2) they hardly rely on the off-line learning approaches.


\section{Research objective}\label{intro: research objective}
At this point, we would like to define the \emph{goal of this thesis}. Basing on the existing software for parameter tuning, we extend it with the (existing in hyper-heuristics) approach of tackling ASP in runtime and merge with generic way to resolve PSP. In other words, to propose an \emph{on-line} approach for solving \emph{both} PSP and ASP problems for heuristic solvers.

In order to fulfill the defined above goal, the following \textbf{research questions} should be answered:
\begin{itemize}
	\item \textbf{RQ 1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	
	\item \textbf{RQ 2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	
	\item \textbf{RQ 3} What is the effect of selecting and adapting algorithm while solving an optimization problem?
\end{itemize}


\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP problems into a single search problem. For doing so, we propose joining the search spaces of separate PSPs into one APSP including also an algorithm type. Important characteristic of such space appears that prohibits treating it as a \emph{flat} structure:  configurations, are formed from all underlying parameters from all algorithms. However, since one solver type requires its specific parameters, but prohibits parameters of other algorithms, the resulting configuration will happen to be sparse. Later, to perform a search of promising settings, scientists utilize knowledge from configurations to construct surrogate models. Such surrogates are crucially required, since the evaluation of each potential configuration is a costly task. In the reviewed systems aforementioned dependencies are treated properly and surrogate models are able, with help of some tricks, learn such dependencies. But, the proposed methodology for sampling parameter values will straggle in an extremely sparse search spaces by means of configuration quality and sampling performance.

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and prediction process. For the search space representation we propose the usage of data structure, similar to feature trees from software product lines field. Doing so we treat solver type and its hyper-parameters uniformly as a regular parameter. The dependencies between parameters in such search space are explicitly handled in form of parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level remain algorithm type parameter, and on the level(s) below a respective hyper-parameters. The prediction process in such search space is made level-by-level. Thus, in the united APSP, firstly, we build a surrogate models for the algorithm type prediction. Afterwards, when the algorithm type is selected, we filter available information to remain only relevant to the selected algorithm type data. Using this filtered data we build surrogate for the second level and predict the parameters for selected algorithm type. The dependencies among parameters are also treated in form of parent-child relationship, therefore, this level-wise prediction process proceeds, until we obtain a completed configuration.

One among commonly used approaches for on-line selection hyper-heuristic is the usage of reinforcement learning (RL) approaches~\cite{moriarty1999evolutionary,mcclymont2011markov}. In our work we utilize such RL technique and perform the problem solving iteratively, on each step selecting among underlying any-time meta-heuristics (portfolio of algorithms) and setting its hyper-parameters according to available performance evidences for current optimization session.

The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader background knowledge in the field of optimization problems, solver types focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find the detailed conceptual description of the proposed approach for unification ASP and PSP problems. The structural and functional requirements for the search space representation sampling process are located there. \cref{impl} is dedicated to an implementation details review, among which a code basis selection, aforementioned requirements realization and a work-flow representation. The evaluation results and their discussion could be found in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
