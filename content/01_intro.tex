\chapter{Introduction}\label{intro}

\section{Motivation}
Traditionally, optimization problem (OP) in general, and combinatorial OP in particular, may be tackled by a number of different approaches, namely, \emph{exact solvers}, \emph{approximate solvers} and \emph{heuristics}~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}. When it comes to heuristic solvers, their performance is defined by the provided strength in exploration and exploitation (EvE) and in the exposed balance between them. The EvE balance may be controlled on a several levels. Firstly, one could try to set a proper values of the exposed by an algorithm \emph{hyper-parameters}. This process is formalized under the notion of \emph{parameter settings problem} (PSP), which resolution can be done before running the algorithm (design time), or while it solves the OP (runtime). The former approach is also called \emph{parameter tuning} and can be tackled by numerous universal tuning systems ~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. A key assumption of this software is an expensiveness of the target system evaluation. It is tackled by the creation of a surrogate learning model, which simulates direct evaluations. The later approach, namely, \emph{parameter control} originally was introduced for evolutionary algorithms~\cite{karafotias2014parameter} and nowadays appears in an \emph{algorithm-dependent} manner. However, even a proper parameter setting may not lead to the best results for the problem at hand. This issue was formalized by the \emph{no-free-lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which states that ``all search algorithms have the same average performance over all possible optimization problems''. The second way for reaching the EvE balance is a proper algorithm selection. It resolves the consequence of NFLT and was formalized as \emph{algorithm selection problem} (ASP). The commonly used approach for ASP solving is the hyper-heuristic utilization. Depending on the learning time, they may perform low-level heuristic selection before actual problem solving, or at the runtime~\cite{burke2019classification}. To operate in on-line, hyper-heuristics often utilize a reinforcement learning (RL) techniques~\cite{moriarty1999evolutionary,mcclymont2011markov}, while for the design time a regular parameter tuning could be used.

The research is not at a standstill and nowadays the researchers are actively attempting to merge ASP and PSP into the united APSP. For instance, in an \emph{automatic machine learning} field such combination was formalized as the \emph{combined algorithm selection and hyper-parameter optimization} problem (CASH)~\cite{thornton2013auto}, while for heuristics explicit studies of APSP merging and solving at the runtime were not found. To tackle ML CASH problem several frameworks based on the existing parameter tuning systems were created~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}. However, those solutions are not applicable in case of heuristics, since they are (1) purely related to ML field and (2) acting at the design time due to ML techniques peculiarities. Naturally, to solve APSP for heuristics off-line, one may try to follow the ML approach of joining ASP and PSP into united APSP search space and utilizing the same parameter tuning systems. An important characteristic of such spaces appears when one prohibits treating them as a \emph{flat} structures. Since one solver type requires only its specific parameters and prohibits the other, a resulting configuration space happen to be `sparse'. Even if the surrogate models of the reviewed tuning systems treat the aforementioned dependencies properly (still, with the help of some tricks), the proposed surrogate optimization techniques in extremely sparse spaces may straggle while searching a good quality configuration. However, when it comes to runtime, the aforementioned struggling may play a crucial role and most importantly, the universal technique for parameter control in heuristics has not yet been proposed.

\section{Research objective}\label{intro: research objective}
At this point, we would like to define the \emph{goal of this thesis}. Basing on the existing parameter tuning software, use it to (1) perform the parameter control in meta-heuristics on a generic level and (2) tackle both ASP and PSP while solving the optimization problem at hand. In other words, to propose an \emph{on-line} approach for solving \emph{both} PSP and ASP problems for heuristic solvers.

In order to fulfill the goal defined above, the following \textbf{research questions} should be answered:
\begin{itemize}
	\item \textbf{RQ 1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	
	\item \textbf{RQ 2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	
	\item \textbf{RQ 3} What is the effect of selecting and adapting algorithm while solving an optimization problem?
\end{itemize}


\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP into a single problem. In order to do so, we firstly introduce a generic runtime PSP solution; secondly, we suggest joining several PSPs search spaces into united APSP on an algorithm type.

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and prediction process. For the APSP representation we suggest using of data structure, similar to feature trees from software product lines field. Doing so we treat solver type and its hyper-parameters uniformly as a regular parameter, but of the different types (categorical vs numerical). The dependencies between parameters in such search space are explicitly handled in form of parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level remains (categorical) parameter defining algorithm type, and on the level(s) below a respective hyper-parameters (categorical and numerical). The prediction process is made level-by-level. Therefore, in the united APSP we firstly build a surrogate model for the algorithm type prediction. Afterwards, when the solver type is selected, we filter available information to operate on only relevant to the selected algorithm type data. With this filtered data we build a surrogate for the second level and predict the parameters for selected algorithm. The dependencies among algorithm-related parameters are also treated in form of parent-child relationship, therefore, we proceed the level-wise prediction process until obtaining a completed configuration. In our work we utilize RL techniques solve the underlying OP iteratively on-line. At each step, according to available performance evidences for current optimization session, we select among any-time meta-heuristics (portfolio of algorithms), predict its best-performing hyper-parameters all by means of created for each layer surrogate models. After constructing such configuration, we continue solving the underlying OP to obtain new evidences for the next iteration.

The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader's background knowledge in the field of optimization problems and solver types, focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find the description of the proposed approach for generic parameter control and APSP problem unification. There we also present both structural and functional requirements. \cref{impl} is dedicated to the review of implementation details, including a code basis selection, aforementioned requirements realization and a work-flow representation. The evaluation results of the proposed concept and their discussion could be found in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
