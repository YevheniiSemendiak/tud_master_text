\chapter{Introduction}\label{intro}

\section{Motivation}
Heuristic-based optimization is a popular research area. Various optimization problems (OPs) are defined and can be tackled by heuristic algorithms~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}. Unfortunately, an ideal algorithm that can solve every OP does not and cannot exist. This issue was formalized by the \emph{no-free-lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which states that ``all search algorithms have the same average performance over all possible optimization problems''. Heuristic solver acts by means of \emph{exploration} (effort diversification over a search space) and \emph{exploitation} (effort intensification in a promising area) operations. The success of heuristic on the problem at hand is defined by the exposed strength of both operations (E\&E) and the provided balance between them (EvE). Both E\&E and EvE characteristics can be controlled in several ways.

Firstly, one could try to set proper values of hyper-parameters exposed by the algorithm. This process is formalized under the notion of \emph{parameter settings problem} (PSP), whose resolution can be done before running the algorithm (design time), or while it solves the OP (runtime). The former approach is also called \emph{parameter tuning} and can be tackled by numerous universal tuning systems ~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. A key assumption of this software is an expensive evaluation of the target system in terms of computational resources. High expensiveness is tackled by a surrogate learning model creation, which is then used to simulate the direct evaluations. The latter approach called \emph{parameter control} was originally introduced for evolutionary algorithms~\cite{karafotias2014parameter} and nowadays appears in an \emph{algorithm-dependent} manner. However, even a proper parameter setting may not lead to the best results for the problem at hand. 

Secondly, one could try to select a proper algorithm. It was formalized as the \emph{algorithm selection problem} (ASP) and defined as a process of searching for an appropriate solver for the problem at hand. ASP resolves the direct consequence of NFLT, which states that a single algorithm cannot be used to tackle various problems. Hyper-heuristics are commonly used for solving ASPs. They may perform low-level heuristic selection before solving~\cite{burke2019classification} the actual problem, or at runtime~\cite{burke2019classification}. To operate online, hyper-heuristics often utilize reinforcement learning (RL) techniques~\cite{moriarty1999evolutionary,mcclymont2011markov}, while for design time, a regular parameter tuning could be used.

The research has not been standing at a standstill and nowadays the researchers are actively attempting to merge ASP and PSP into a united \emph{algorithm selection and parameter setting problem} (APSP). For instance, in automatic machine learning such combination was formalized as the \emph{combined algorithm selection and hyper-parameter optimization} problem (CASH)~\cite{thornton2013auto}, while for heuristics the explicit studies of APSP merging and solving at runtime were not found. To tackle ML CASH problem several frameworks based on the existing parameter tuning systems were created~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}. However, those solutions are not applicable in case of heuristics, since they are (1) purely related to ML field and (2) acting at design time due to ML techniques nature. One may follow the ML approach of the united APSP search space definition and solving for heuristics, but it is applicable \emph{only} at design time. Nevertheless, when it comes to runtime, it turns out that the universal technique for setting the parameters online (parameter control) in heuristics has not yet been proposed. It is essential, since the generic approach to tackle PSP is one of two required methodologies for solving heuristic APSP at runtime. The other building block (ASP) is already available in online hyper-heuristics.

\section{Research objective}\label{intro: research objective}
The goal of this thesis is to improve the quality of online heuristic-based optimization. The research objective is to determine whether it is possible to solve both PSP and ASP, while solving the OP. In order to reach the research objective we need to answer the following RQs:
\begin{itemize}%[noitemsep]
	\item \textbf{RQ1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	\item \textbf{RQ2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	\item \textbf{RQ3} What is the effect of selecting and adapting algorithms while solving an optimization problem?
\end{itemize}

\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP into a single problem. In order to do so, we firstly introduce a generic runtime PSP solution; secondly, we suggest joining several PSPs search spaces into a united APSP. The consequence of merging several PSPs into a single APSP is the appearance of \emph{sparse} search spaces, where the percentage of properly defined configurations is low due to requiring and prohibiting dependencies among parameters (for instance, each algorithm has its own required set of parameters).

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and sampling process. For the APSP representation we suggest using a data structure, similar to feature trees from software product lines field. By doing so, we treat a solver type and its hyper-parameters uniformly. The dependencies between parameters are explicitly handled in the form of parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level the algorithm type is defined, and on the level(s) below its respective hyper-parameters are specified. The prediction process is made sequentially for each level, utilizing the available performance evidence in a form of already tried configurations and respective improvements. Therefore, in the united APSP we firstly build a surrogate model for the algorithm type prediction. Afterwards, when the solver type is selected, we filter the performance evidence to operate on data, which is relevant to the selected algorithm type. With this filtered data we build a surrogate for the second level and predict the parameters values on second level. This level-wise process continues until obtaining a completed configuration. Next, we continue solving the underlying OP with the defined algorithm type and the predicted configuration to obtain new evidence and repeat configuration prediction process. This reinforcement learning technique enables us to solve the APSP online, while iteratively tackling the OP at hand. The proposed concept evaluation showed that: (1) applying the generic parameter control to each among the reviewed meta-heuristics results in the solution quality, comparable and in some cases even outperforming the quality of tuned in offline parameters; (2) our APSP tackling approach is preferable in cases, when the heuristics dominance and their parameters are unknown beforehand.

\paragraph{}
The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader's background knowledge in the field of optimization problems and solver types, focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find a description of the proposed approach for generic parameter control and APSP problem unification. There we also present both structural and functional requirements for system components. \cref{impl} is dedicated to the review of implementation details, including a code basis selection, the aforementioned requirements realization and the developed system workflow representation. We evaluate the proposed concept and discuss the results in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
