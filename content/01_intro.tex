\chapter{Introduction}\label{intro}

\section{Motivation}
Traditionally, optimization problem (OP) in general and combinatorial OP in particular may be tackled by a number of different approaches. Besides an exhaustive brute-force search (comparison of all possible solutions) a more convenient methodologies do exist. Most of the frequently used approaches can be grouped into three large families~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}: \emph{exact solvers}, \emph{approximate solvers} and \emph{heuristics}. The first two among them are build on mathematical proofs and provide a guarantee on the solution quality, while the later is often based on the usage of domain knowledge and stochastic processes to guide the search. When it comes to heuristic solvers, their performance is defined by the provided strength in exploration and exploitation (EvE) and in the exposed balance between them. 

The EvE balance is controlled on a several levels. Firstly, the majority of algorithms is configurable by means of their exposed parameters, which are also often called \emph{hyper-parameters}. Their proper selection is also an OP and named \emph{parameter settings problem} (PSP). Solving this OP often requires a considerable effort and has a great influence on heuristic performance~\cite{lavesson2006quantifying}. The resolution may be done \emph{before} running the algorithm (design time), or \emph{while} it solves the OP (runtime). The former approach is also called \emph{parameter tuning}, for which numerous systems were created to solve the problem on a generic level~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. A key assumption of the parameter tuning systems is an expansiveness of target system evaluation, therefore, they build surrogate models to simulate direct evaluations. The later approach, namely, \emph{parameter control} originally was introduced by the research community of evolutionary algorithms~\cite{karafotias2014parameter} and nowadays is spread to other heuristic solvers, but in an \emph{algorithm-dependent} manner. Unfortunately, even a proper parameter setting may not lead to the best performing algorithm for problem at hand. This issue was formalized by the \emph{no-free-lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which states that ``all search algorithms have the same performance, when their results are averaged over all possible optimization problems''. Therefore, the second approach for reaching EvE balance, which resolves the consequence of NFLT is the problem of a proper algorithm selection (ASP). The commonly used approach for ASP solving is the hyper-heuristics utilization. Depending on the learning time, they may perform low-level heuristic selection at the design- or runtime~\cite{burke2019classification}. One among commonly used approaches for on-line selection hyper-heuristic is the usage of reinforcement learning (RL) approaches~\cite{moriarty1999evolutionary,mcclymont2011markov}, while in the design time a regular parameter tuning techniques could be used.

The research does not stand still and nowadays the attempts to merge ASP and PSP problems are actively making. Concretely, in an \emph{automatic machine learning} field, such problem was formalized as the \emph{combined algorithm selection and parameter setting} problem (CASH)~\cite{thornton2013auto}. Several frameworks were created to resolve ML CASH problem~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}. The majority of them are based on the existing parameter tuning systems, but those frameworks are (1) purely related to ML field and (2) acting at the design time due to ML techniques specifics. As for heuristics, an explicit studies for ASP and PSP merging in runtime were not found. One may follow the ML approach of joining ASP and PSP into united search space and utilizing the same parameter tuning systems. However, an important characteristic of such spaces appears that prohibits treating them as a \emph{flat} structure: they comprise all underlying algorithms hyper-parameters. Since one solver type requires only its specific parameters and prohibits (or ignores) the parameters of other algorithms, the resulting configuration space will happen to be `sparse'. In the reviewed systems aforementioned dependencies are treated properly and surrogate models are able, with help of some tricks, learn them. But, the proposed surrogate optimization techniques in an extremely sparse spaces will straggle while searching a good quality configuration by means of sampling performance. Moreover, the parameter adaptation at the runtime were not introduced on a generic level yet. 


\section{Research objective}\label{intro: research objective}
At this point, we would like to define the \emph{goal of this thesis}. Basing on the existing parameter tuning software, use it to (1) perform the parameter control in meta-heuristics on the generic level and (2) tackle both ASP and PSP while solving the optimization problem at hand. In other words, to propose an \emph{on-line} approach for solving \emph{both} PSP and ASP problems for heuristic solvers.

In order to fulfill the defined above goal, the following \textbf{research questions} should be answered:
\begin{itemize}
	\item \textbf{RQ 1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	
	\item \textbf{RQ 2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	
	\item \textbf{RQ 3} What is the effect of selecting and adapting algorithm while solving an optimization problem?
\end{itemize}


\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP into a single problem. For doing so, we firstly introduce a generic runtime PSP solution; secondly we propose joining several PSPs search spaces into united APSP on an algorithm type.

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and prediction process. For the APSP representation we propose the usage of data structure, similar to feature trees from software product lines field. Doing so we treat solver type and its hyper-parameters uniformly as a regular parameter, however of the different types (categorical vs numerical). The dependencies between parameters in such search space are explicitly handled in form of parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level remain (categorical) parameter defining algorithm type, and on the level(s) below a respective hyper-parameters (categorical and numerical). The prediction process is made level-by-level. Thus, in the united APSP we firstly build a surrogate model for the algorithm type prediction. Afterwards, when the solver type is selected, we filter available information to operate on only relevant to the selected algorithm type data. With this filtered data we build a surrogate for the second level and predict the parameters for selected algorithm. The dependencies among algorithm-related parameters are also treated in form of parent-child relationship, therefore, we proceed the level-wise prediction proceeds until obtaining a completed configuration. In our work we utilize RL techniques solve the underlying OP iteratively on-line. On each step, according to available performance evidences for current optimization session, we select among any-time meta-heuristics (portfolio of algorithms), predict its best-performing hyper-parameters all by means of created for each layer surrogate models. After constructing such configuration, we continue solving the underlying OP to obtain new evidences for the next iteration.

The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader background knowledge in the field of optimization problems, solver types focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find the description of the proposed approach for generic parameter control and APSP problem unification. The structural and functional requirements for the search space representation and the sampling process are also located there. \cref{impl} is dedicated to the review of implementation details, among which a code basis selection, aforementioned requirements realization and a work-flow representation. The evaluation results of the proposed concept and their discussion could be found in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
