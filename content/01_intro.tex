\chapter{Introduction}\label{intro}

\section{Motivation}
Heuristic-based optimization is a popular research area~\cite{junger2003combinatorial,biegler2004retrospective,festa2014brief}. Numbers of optimization problems are defined, but an ideal approach for their struggling does not exist. This issue was formalized by the \emph{no-free-lunch theorem for optimization} (NFLT)~\cite{wolpert1997no}, which states that ``all search algorithms have the same average performance over all possible optimization problems''. The success of heuristic solver on problem at hand is defined by the exposed strength in \emph{exploration} (effort diversification over a search space) and \emph{exploitation} (effort intensification on a promising area) and balance between them. This EvE balance may be controlled on a several levels. 

Firstly, one could try to set proper values of hyper-parameters, exposed by the algorithm. This process is formalized under the notion of \emph{parameter settings problem} (PSP), whose resolution can be done before running the algorithm (design time), or while it solves the OP (runtime). The former approach is also called \emph{parameter tuning} and can be tackled by numerous universal tuning systems ~\cite{hutter2009paramils,hutter2011sequential,lopez2016irace,falkner2018bohb,brise2spl}. A key assumption of this software is an expensiveness of the target system evaluation in terms of computational resources. Expensiveness is tackled creating a surrogate learning model, which simulates direct evaluations. The latter approach \emph{parameter control} was originally introduced for evolutionary algorithms~\cite{karafotias2014parameter} and nowadays appears in an \emph{algorithm-dependent} manner. However, even a proper parameter setting may not lead to the best results for the problem at hand. 

The second way for reaching the EvE balance is a proper algorithm selection. It resolves the direct consequence of NFLT by searching appropriate solver for problem at hand and was formalized as the \emph{algorithm selection problem} (ASP). Hyper-heuristics are commonly used for solving ASPs. They may perform low-level heuristic selection before solving the actual problem, or at runtime~\cite{burke2019classification}. To operate on-line, hyper-heuristics often utilize reinforcement learning (RL) techniques~\cite{moriarty1999evolutionary,mcclymont2011markov}, while for design time, a regular parameter tuning could be used.

The research is not at a standstill and nowadays the researchers are actively attempting to merge ASP and PSP into the united \emph{algorithm selection and parameter setting problem} (APSP). For instance, in automatic machine learning such combination was formalized as the \emph{combined algorithm selection and hyper-parameter optimization} problem (CASH)~\cite{thornton2013auto}, while for heuristics the explicit studies of APSP merging and solving at runtime were not found. To tackle ML CASH problem several frameworks based on the existing parameter tuning systems were created~\cite{thornton2013auto,feurer2015efficient,olson2019tpot}. However, those solutions are not applicable in case of heuristics, since they are (1) purely related to ML field and (2) acting at design time due to ML techniques nature. One may follow the ML approach of the united APSP search space definition and solving for heuristics, but it is applicable \emph{only} at design time. However, when it comes to runtime, it turns out that the universal technique for setting the parameters on-line (parameter control) in heuristics has not yet been proposed. It is essential, since this (PSP) is one of two required methodologies for solving heuristic APSP at runtime. The other building block (ASP) is already available in on-line hyper-heuristics.

\section{Research objective}\label{intro: research objective}
The goal of this thesis is to basing on the existing parameter tuning software, propose an \emph{on-line} approach for solving \emph{both} PSP and ASP problems for heuristic solvers.

In order to fulfill the goal defined above, the following \textbf{research questions} should be answered:
\begin{itemize}
	\item \textbf{RQ1} Is it possible to perform the algorithm configuration at runtime on a generic level?
	
	\item \textbf{RQ2} Is it possible to simultaneously perform algorithm selection and parameters adaptation while solving an optimization problem?
	
	\item \textbf{RQ3} What is the effect of selecting and adapting algorithm while solving an optimization problem?
\end{itemize}


\section{Solution overview}
In this thesis we propose the unification of both ASP and PSP into a single problem. In order to do so, we firstly introduce a generic runtime PSP solution; secondly, we suggest joining several PSPs search spaces into a united APSP.

To overcome the sparseness issue we propose a complex solution, which is spread in both search space structure and prediction process. For the APSP representation we suggest using of a data structure, similar to feature trees from software product lines field. Doing so we treat a solver type and its hyper-parameters uniformly as a regular parameter in a search space. The dependencies between parameters are explicitly handled in form of a parent-child relationship. As a result, the search space could be viewed as a layered structure, where on the first level remains (categorical) parameter defining the algorithm type, and on the level(s) below its respective hyper-parameters (categorical and numerical). The prediction process is made sequentially for each level, utilizing the available performance evidences in form of already tried configurations in this optimization session. Therefore, in the united APSP we firstly build a surrogate model for the algorithm type prediction. Afterwards, when the solver type is selected, we filter the performance evidences to operate on data, which is relevant to the selected algorithm type. With this filtered data we build a surrogate for the second level and predict the selected algorithm parameters. Dependencies among algorithm-related parameters are also treated in form of the parent-child relationship, therefore, we proceed the level-wise prediction process until obtaining a completed configuration. Next, we continue solving the underlying OP with the defined algorithm type and its parameters to obtain new evidence and repeat configuration prediction process. This reinforcement learning techniques enables us to solve the APSP on-line, while iteratively tackling the OP at hand. 

The structure of this thesis is organized as follows. Firstly, in \cref{bg} we refresh the reader's background knowledge in the field of optimization problems and solver types, focusing on heuristics. We also review the parameter setting and the available solutions for this problem. In \cref{Concept description} one will find a description of the proposed approach for generic parameter control and APSP problem unification. There we also present both structural and functional requirements for system components. \cref{impl} is dedicated to the review of implementation details, including a code basis selection, aforementioned requirements realization and the developed system workflow representation. We evaluate the proposed concept and discuss the results in \cref{eval}. \cref{conclusion} concludes the thesis and \cref{future work} describes the future work.
