\chapter{Implementation Details}
In this Chapter we dive into the development description of listed in the \cref{Concept description} requirements.
 
The best practice in software engineering is to minimize an implementation effort reusing the already existing and well-tested code. With this idea in mind, we use one of the reviewed open-source parameter tuning frameworks (\cref{bg: parameter tuning}) as the code basis for the high level heuristic (HLH) in desired hyper-heuristic. We also reuse the existing low level heuristics (LLH) implementation in other frameworks. While we use LLH-basis almost out of box, the HLH-basis requires more changes.

In the \cref{impl:hlh code basis section} we analyze the parameter tuning frameworks from a perspective of needed effort to fulfill the HLH requirements, listed in the \cref{concept:prediction}. In a conclusion (\cref{impl:hlh code basis conclusion}) we outline the adaptations, needed to be done in the selected code basis. Afterwards, we split the former HLH implementation description into two logical parts, as we have done for the concept description. In the \cref{impl: search space} we discuss the search space development, while the \ref{impl: prediction logic} is dedicated to the prediction process.

Finally, in the \cref{impl: LLH} we choose the LLH code basis for, present a set of reused meta-heuristics and their adaptations to fulfill our requirements listed in the \cref{concept: llh} as well.


\section{Hyper-Heuristics Code Base Selection}\label{impl:hlh code basis section}
To start with the framework analysis, let us firstly outline the important characteristics from the implementation perspective.

The first two crucial criteria are the framework \emph{variability} and \emph{extensibility}. The desired HLH should be easily variable in terms of replacing such functionality as the learning model or the termination criteria by a new one. We may need to use different models to select the LLH and control the parameters therefore, the code basis should be extensible in terms of usage different model for each prediction level (see \cref{concept:pict:Level-wise prediction process}).

The next is a \emph{support for on-line optimization}. This is a bit complex characteristic of system that we are willing to distinguish. As it turns out, many parameter tuning systems require full evaluation of target system for configuration comparison. However, in case of desired hyper-heuristic, we treat the configuration evaluation as a trial to solve the problem in hand using particular LLH with tuned parameter. It implies an important ability of the optimization problem (OP) solving in a set-wise manner (\cref{concept:parameter control}).

The next characteristic is the support of \emph{conditional parameters}. Since, it is a complex feature, which lays not only in the search space, but also in the prediction process, we pay attention to both of them separately.

\subsection{Parameter Tuning Frameworks Analysis}\label{impl: Parameter Tuning Frameworks Analysis}
\paragraph{SMACv3}
The implementation of Sequential Model-based Algorithm Configuration framework is available on-line under the BSD-3 license.

The idea of SMACv3 lays in ROAR mechanism enhancement with the model-based sampling algorithm.
As we mention in \cref{bg: smac}, underlying surrogate model are one of the random forest or Gaussian process kernel density estimator. These models could fit complex dependencies among parameters in the search spaces. To choose a next configuration the \emph{one-exchange} neighborhood of best found so far is traversed, using the surrogate models and the expected improvement estimations.
The learning capabilities in surrogates are great, and the usage of expected improvement guarantees converging to the global optimum given enough time as well. However, the major drawback in this system is lack of ability to include the conditional dependencies between parameters into the sampling process. In fact, used here search space representation framework ConfigSpace~\cite{configspace} is able to specify the dependencies among hyper-parameters.
However, to the best of our knowledge, the one-exchange neighborhood used as sample mechanism in SMACv3, is unaware of the parameter dependencies therefore, violates them during the configuration sampling resulting in illegal parameter combination. Those cases are rejected by the ConfigSpace, but we believe that in case of `sparse' search spaces this approach could lead to ineffective sampling and struggling in system predictive abilities. Unfortunately, we did not find any officially published empirical studies of such cases and can only make guesses based on own intuition but, the SMACv3 developers advises for operation in such cases~\footnote{Visit GitHub repository of SMACv3 for more info~\url{github.com/automl/SMAC3/issues/403}} could serve as an evidence to correctness of our assumptions. One of the possible solutions here could be the usage of a conditional-aware one-exchange neighborhood definition for sampling process.

The ROAR mechanism is a derivative from the FocusedILS algorithm (solver in the parameter tuning framework ParamILS~\cite{hutter2009paramils}) where each evaluation of a new candidate solution on problem instance performed sequentially. Since the ROAR evaluation strategy is also used in SMACv3, we expect it to require much effort to enable system solving the problem on-line.

% TODO: maybe more here, since the motivation is not clear...

\paragraph{IRACE}
The IRACE framework implements the iterated racing algorithm to evaluate the set of configurations during the parameter tuning session(\cref{bg: irace}). It is distributed under the GNU General Public License and the source code is available on-line.

As the surrogate models, IRACE uses the probability distributions of those parameter values, which shown to be good during racing step. The prediction process is defined as the step-wise sampling in previously built distributions. It elegantly handles the conditions among parameters and illuminates the possibility of invalid configuration appearance.
The framework completely relies on the racing algorithm for parallel evaluations and on \emph{Friedman test}~\cite{conover1980practical} or alternatively \emph{paired t-test} for statistical analysis of racing configurations. Thus, we found it to be static in terms of variability and extensibility on the learning mechanisms.
In terms of parallel evaluations, the algorithm utilizes all available resources at the beginning of each racing step, but as the process continues, fewer evaluations are executed in parallel those available resources are idling and not utilized optimally at all steps of IRACE execution.

As for the on-line problem solving, let us discuss the racing algorithm. As we described in \cref{bg: irace}, this step is executed with a (1) set of TS configurations under evaluation and (2) a benchmark set of optimization problems. Then, the TS starts to solve the problem set under each configuration, while racing algorithm drops the worst-performing configurations. It is possible to use a single problem instance however, divided into parts (from the perspective of TS allowed running time) instead of using a benchmark set. Doing so, it will be possible to adapt system for on-line problem solving cheaply however, the granularity of parameter (and LLH type as well) control will be reduced. The reason for such reduce is the amount of information obtained from race: only the best configurations are returned, leaving the performance evidences of others behind, which may used to create a more precise surrogate models. The only possible way to deal with this is to leave a racing algorithm and use the reinforcement learning.

\paragraph{HpBandSter}
As we discussed in the \cref{bg: bohb}, HpBandSter is an implementation of BOHB algorithm, which turns to be a hybridization of Hyperband and Bayesian Optimization approaches.

A role of Hyperband in this duet is the configuration evaluation and comparison, while the Bayesian Tree Parzen Estimator (TPE) suggests the which configuration to evaluate next. The idea behind this combination is to eliminate the weak sides of each algorithm with the strengths of other. For instance, in original Hyperband the configuration selection is made uniform at random, which results in a slow converge of optimization process. As for the BO TPE, a drawback lays in configuration evaluation, which does not take into account an early evidences about the TS performance. Thus, even when the proposed configuration results in poor intermediate TS performance (which may be an evidence of a weak final performance), BO still continues TS execution. This motivated authors to merge those two algorithms and create one with strong anytime (HB) and final (BO) performance, which will effectively use available computational resources in parallel (HB) and scalable learning mechanisms (BO).

Let us discuss the process of conditions between hyper-parameters handling. SMACv3 and HpBandSter as well, uses ConfigSpace framework for search space representation. As we discussed in SMACv3 description above, ConfigSpace naturally allows to encode the dependencies and conditions among parameters. The TPE learning models are also able to somehow fit these dependencies by using the \emph{impuration} mechanism\cite{levesque2017bayesian}. In short, when fitting the surrogate models, inactive parameters (turned off by means of dependencies) are treated with their default values. Later, while building the surrogate models those default parameter values are ignored therefore, the probability densities still represent a proper parameter values distributions. However, consider an appearance of two configurations sets: $C_1$ and $C_2$, such that some parameter $P_i$ is forbidden in $C_1$, but required in $C_2$ and in a contrary the other parameter $P_j$ is required in $C_1$, but forbidden in $C_2$. If these configuration families are turn to be superior, this will bias the densities towards $P_i$ and $P_j$ values. As a consequence, the proposed prediction mechanism will sample non-default parameter values for both $P_i$ and $P_j$, which results in the configurations with violated parameter dependencies. The more `sparse' search space, the more harming an effect will be from a prediction performance perspective. A possible treatment here is to change the sampling process, introducing an intermediate layer to perform a prediction in level-wise approach, suggested in the \cref{concept:prediction}.


\paragraph{BRISEv2}
BRISEv2 is a software product line (SPL), created with aim in solving the expensive optimization problems in general and for the parameter tuning in particular (\cref{bg: brise}).

The advantage of BRISEv2 over other systems comes from its modular design of \emph{main-node}. It is a set of cooperating core entities (Experiment, Search Space and Configuration) with other non-core entities exposed to user for variability. The prediction models, termination criteria, outliers detectors, repetition strategies, etc. are representatives of these non-core and variable components. A number of implementations are provided out-of-box for all variable entities, but we focus our attention to implemented sampling process. The reason of such greedy review is that the underlying search space representation is carried out by the same ConfigSpace, while the provided surrogate models are ridge regression model with polynomial features and Bayesian Tree Parzen Estimator (TPE). We are not going to repeat ourselves reviewing the ConfigSpace + TPE combination, but we have to put a few words about the ridge regression. 

Ridge is the machine learning linear regression model with parameters regularization~\cite{hoerl1970ridge}. Since, it is a linear model, its ability to fit a `sparce' search spaces is poor therefore, the machine learning community are suggested to treat such cases with \emph{conditional linear regression}~\cite{DBLP:journals/corr/abs-1806-02326}. The underlying idea is to split the search space into sub-search spaces and build a separate regression model, but to the best of our knowledge, this approach is not built in into the underlying ridge regression model.

As for the support of on-line problem solving, the routine of optimization process, implemented in BRISEv2 is nothing else, but the reinforcement learning approach. After each new obtained evidence, a `fresh' surrogate model is built to react on the learning process by predicting of new configuration. Which makes it easy to embed the on-line parameter tuning approach, presented in \cref{concept:parameter control}.


\subsection{Conclusion on Code Base}\label{impl:hlh code basis conclusion}
The most among reviewed parameter tuning systems share the same SMBO approach for problem solving. They utilize a rather similar approaches for building the surrogate models and making the predictions however, the different architecture is implemented.

To sum um our review, we use a term \emph{quality} to aggregate both the provided  out-of-the-box desired characteristic support and the required effort to adapt it, if necessary. We aggregate the reviewed characteristics quality in each software framework into \cref{iml: table code basis selection} for the visual representation.
The quality estimates are quantized into three ordinal values:
\begin{enumerate}
	\item \textbf{Poor} quality, which implies the weak characteristic support and much effort required to provide it.
	
	\item \textbf{Average} quality, which implies the weak support, but requires small amount of effort to provide it.
	
	\item \textbf{Good} quality, which implies a great out-of-the-box characteristic support, and requires minor or no changes at all.
\end{enumerate}

\begin{table}[h!]
	\centering
	\begin{tabular}{l||cccc}
		\textbf{Characteristic} & \textbf{SMACv3}& \textbf{IRACE} & \textbf{HpBandSter} & \textbf{BRISEv2} \\
		\hline
		\hline
		Variability \& extensibility & \cellcolor{yellow!25}Average & \cellcolor{red!25}Poor & \cellcolor{yellow!25}Average & \cellcolor{green!25}Good \\
	
		On-line optimization & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average \\
	
		Conditional parameters & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor \\
	\end{tabular}
	\caption{Code basis candidate systems characteristics.}
	\label{iml: table code basis selection}
\end{table}

Among the reviewed software systems, the majority were created as an implementation of some concrete algorithm (or a combination of algorithms). It results in the reduction of system flexibility. It turns out that there are two most promising candidates for hyper-heuristic with parameter control creation: IRACE and BRISEv2. They both require much adaptation and preparation steps, which we will discuss in upcoming sections, but still less in comparison to SMACv3 and HpBandSter. Among such features as proper support of conditional parameters vs variability and extensibility, the former plays a settle role therefore, we choose BRISEv2 as the code basis.

\section{Search Space}\label{impl: search space}
Previously, in the \cref{concept:search space} we presented a set of structural requirements for the search space representation: parent-child relationship should be presented explicitly, supporting different parameter types in a values-specific approach. To support the prediction process in the \cref{concept:prediction} we listed functional requirements in form of mechanisms: data filtering, sampling propagation, parameter description and a configuration verification.

In this section we (1) analyze the available ConfigSpace framework, how it fits to our requirements and (2) decide whether to use it or to set it aside for making own search space representation implementation.

\subsection{Base Version Description}
From the structural point of view, in ConfigSpace\footnote{ConfigSpace GitHub repository: \url{github.com/automl/ConfigSpace}} the parameters coupling is made implying parent-child relationship, which fit into our requirements. The parameter types suite the most of use-cases and the value-specific dependencies are supported as well. Therefore, from the structural requirements S.R.1, S.R.2 and S.R.3 (\cref{concept:search space}) are perfectly met.

When it comes to the functional requirements (\cref{concept:prediction}), ConfigSpace samples the random configurations in a completion approach. In other words, there is no step-wise configuration creation, only a final and valid ones are produced. Thus, there is no way to expose the underlying dependencies among parameters for the prediction models, except of carrying them on a side and evaluating manually. As a consequence the data filtering mechanism should be implemented on a side and the sampling propagation as well. The framework exposes an ability to validate a created configuration, which turn to be also a proprietary class. As for the parameter description, the amount of exposed knowledge is satisfying. Here we conclude that all functional requirements, except S.F.R.4 are not met.

As the conclusion, we decided to set aside the $3rd$ party ConfigSpace framework because: (1) it still requires much adaptation and implies its usage as a core entity in BRISEv2, (2) it implies replacement of the other core entity — Configuration, which turns to be costly and (3) it obligates us to use a third proprietary entity — Hyperparameter. 

\subsection{Search Space Implementation}\label{impl: search space impl}
From the structural requirements of search space we know that the parameters should be treated uniformly. The desired \emph{feature tree} shape is perfectly handled by \emph{composite} design pattern. With this idea in mind, we construct the search space as a composite \emph{Hyperparameter} object with four possible hyper-parameter types: integer and float as numerical, nominal and ordinal as categorical. This fulfilling S.R.2, specified in \cref{concept:search space}. The implemented class diagram could be found in Appendix.
\todoy{add class diagram in appendix}

In the code snippets provided through the explanation we highlight an implemented method  signature, which fulfills one of our requirements, specified in \cref{Concept description}.

\paragraph{Search space construction.} The S.R.3 implementation is performed by means of $add_child_hyperparameter$ method in Hyperparameter class (\cref{impl: S.R.1 listing}). It should be called on a parent hyper-parameter object, specifying the activation value(s) ($activation_categories$ argument) of parent hyper-parameter which will expose child. 

\begin{lstlisting}[language=Python, caption=S.R.1 implementation., label=impl: S.R.1 listing]
class Hyperparameter:
	...
	def add_child_hyperparameter(self, other: Hyperparameter, activation_categories: Iterable[_CATEGORY]) -> Hyperparameter: pass
	...
\end{lstlisting}

Note, currently we support a composite construct only by means of categorical parameter type therefore, requiring a list of activation categories and postponing composition on numerical ranges enhancement for the future work, since our current needs do not include the composition on numerical ranges.

\paragraph{Search space role in prediction.}
Imagine a number of configurations were already evaluated. For making the prediction in tiered approach, the parameter values on current level should be selected. Thus, first we filter data which fits to this level by means of search space S.F.R.1. Thereby, filter accepts the already chosen parameter values and iterates over the available configurations. It is implemented in form of hyper-parameter instance method in \cref{impl: S.F.R.1 listing}.
An intent of this method is to derive whether the already chosen parameter values ($base\_values$) form a sub-feature-tree of the parameter values under comparison ($target\_values$). The outcome of this method is a decision, should this particular configuration be included into the dataset or not. For instance, if the selected LLH type in $base\_values$ is not the same as one in $target\_values$, the result will be false.

\begin{lstlisting}[language=Python, caption=S.F.R.1 implementation., label=impl: S.F.R.1 listing]
class Hyperparameter:
	...
    def are_siblings(self, base_values: MutableMapping, target_values: MutableMapping) -> bool: pass
	...
\end{lstlisting}

After filtering data, the time comes for predict a parameter values. For doing so, the search space, must expose parameters on the current level be means of S.F.R.2. Since we always interact with a search space root object, the call to $generate$ method is executed recursively (\cref{impl: S.F.R.2 listing}). If a callee finds itself in $values$, it redirects a call to \textit{activated} children. If it does not, it adds itself as a $parameter\ name \rightarrow random\ parameter\ value$ to the $values$.

\begin{lstlisting}[language=Python, caption=S.F.R.2 implementation., label=impl: S.F.R.2 listing]
class Hyperparameter:
	...
    def generate(self, values: MutableMapping) -> None: pass
	...
\end{lstlisting}

Randomly sampled for current level values are then used for getting a description and (1) cutting-off the data from levels above and below, (2) building the surrogate models and (3) making the prediction.

To build the surrogate models we require an available data (parameters) description. Thus, S.F.R.3 implementation is performed in method $describe$ (\cref{impl: S.F.R.3 listing}). This is once again a recursive method, which terminates when parameter can not find activated children or himself in the provided $values$. 
This description contains a mapping from parameter name to its type and range of possible values.

\begin{lstlisting}[language=Python, caption=S.F.R.3 implementation., label=impl: S.F.R.3 listing]
class Hyperparameter:
	...
    def describe(self, values: MutableMapping) -> Description: pass
	...
\end{lstlisting}

Later, this description is used by prediction models for building surrogates and making the predictions, which will replace a randomly sampled parameter values in $generate$ method.

The described above process is controlled by S.F.R.4., implemented as method $validate$ (\cref{impl: S.F.R.4 listing}). The control occurs twice. Firstly, before starting a new loop of $filter \rightarrow propagate \rightarrow describe \rightarrow predict$ to check whether the construction process is finished (deep validation). Secondly, after making the prediction by models (flat validation). In the later case, if the conditions are violated, the predicted values are discarded and sampled randomly. Since the sampling process implemented in hyper-parameters guarantees to provide valid parameter values, after maximally $N$ mentioned above loops, we derive a new and valid configuration, where $N$ is a maximal depth in the defined search space.

\begin{lstlisting}[language=Python, caption=S.F.R.4 implementation., label=impl: S.F.R.4 listing]
class Hyperparameter:
	...
	def validate(self, values: MutableMapping, recursive: bool) -> bool: pass
	...
\end{lstlisting}


\section{Prediction Process}\label{impl: prediction logic}
The next step is an investigation and planning of prediction logic adaptation.
In the \cref{impl: Parameter Tuning Frameworks Analysis} we learned that BRISEv2 provides two learning models: Bayesian TPE and ridge linear regression. Both approaches could be used within a tiered parameter values sampling however, it should be generalized.

P.F.R.1 implies the addition of entity, which will encapsulate the prediction process, described in the \cref{impl: search space impl}. According to P.F.R.3, this entity may also be responsible for the forgetting strategy. Both requirements are not available in BRISEv2 yet therefore, we will implement them from scratch.

As for P.F.R.2, the current implementation of BRISEv2 already provides some level of model unification behind a required interface, which, however, is too coarse-grained and implies binding of tree logical steps: data preprocessing, surrogate models creation and optimization of surrogates to predict a next configuration.

The following parts of prediction logic description are dedicated to (1) P.F.R.1 + P.F.R.3 implementation in form of $Predictor$ entity and P.F.R.2 in form of decoupled data preprocessing mechanism and the prediction models. Note, the current implementation does not solve the problem of binding surrogate models creation and optimization over them. We postpone this to the future work implementing a simple random search over the surrogate models.

\subsection{Predictor Entity}
In addition to previously listed logic during the search space description, a role of predictor is also to decouple a learning models from (1) feature model shape of search space and (2) other core entities such as Configuration. In addition to static search space, the input to predictor is the available in a moment data (evaluated configurations), while the desired output is a configuration. \cref{impl: P.F.R.1 + P.F.R.3 implementation pseudocode} provides a pseudo-code of predictor implementation.

To implement the information forgetting mechanism, we refer the idea of sliding window, mentioned in~\cite{ferreira2017multi}. According to this, predictor should use specified in a settings number of the latest configurations as information for surrogate models creation. We modify this logic, allowing user to specify not only a static number, but also a percentage of the latest configurations (line 3). This fulfills the P.F.R.3 however, more exotic approaches may arise such as the statistical analysis to estimate a required configuration number or the other type of meta-learning.

The next question is decoupling of prediction models from the search space structure by means off fulfilling P.F.R.1. As we discussed in the \cref{impl: search space impl}, to predict a parameter values on each level, the models should be built on only related to this level information. For doing so, after filtering the data (line 6), we propagate the prediction from previous level to current (line 7), remove parameters values from level above and derive a description for current level parameters (line 9). Independently, we instantiate a specified in predictor settings prediction model for this level and fit it with the related information (lines 11-13). Afterwards, we make a prediction and check if it not validates a search space boundaries (lines 16-17). If either model can not properly fit the data, or the prediction is invalid, we sample the parameter values randomly (lines 18-20).

\begin{code}[language=Python, caption=P.F.R.1 + P.F.R.3 implementation pseudo-code., label=impl: P.F.R.1 + P.F.R.3 implementation pseudocode]
class Predictor:
	def predict(measured_configurations):
		level_configurations = trim_in_window(measured_configurations)
		prediction = Mapping()
		while not search_space.validate(prediction, recursive=True):
			level_configurations = filter(search_space.are_siblings(prediction, x), level_configurations)
			randomly_generated = search_space.generate(prediction)
			full_description = search_space.describe(randomly_generated)
			level_description = trim_previous_levels(description, prediction)
			
			data = trim_accodring_to_description(level_configurations, level_description)
			model = get_current_level_model()
			model.build(data, level_description)
			
			if model.is_built():
				level_prediction = model.predict()
				if not search_space.validate(level_prediction, recursive=False):
					level_prediction = randomly_generated
			else:
				level_prediction = randomly_generated
		
		return Configuration(prediction)
\end{code}

For the sake of simplicity we omit some minor implementation details and provide the description of (1) available models and (2) data preprocessing in a Sections below.

\subsection{Data Preprocessing}\label{impl: preprocessing}
Data preprocessing may be split into two concepts: an obligatory data encoding and optionally data transformation. The first is required to make the underlying model capable with provided data. Imagine the parameters values to be a simple string. Having a surrogate model, which is constructed as probability densities of parameter values, one should first derive a numeric data for those string parameter values. The second concept is applied on a data, which is already suitable. This is usually done to improve an available surrogate model performance. An example of former could be simple indexing of all possible string values, which results in replacement of strings by their indexes during the data preprocessing. On a contrary, the later case may be presented by an addition of polynomial features to already available data with aim to improve surrogate models preciseness by learning more complex dependencies.

The decision of which encoding to use is strictly defined by the learning model, while the decision on data transformation is carried out by user and depends on the concrete use-case.

In all reviewed parameter tuning systems, the data preprocessing is implemented only by means of an obligatory for underlying learning models encoding and omitting the possible data transformation. In most cases, the encoding is performed in simple label into integer numbers. Being the easiest approach, this encoding may introduce a non-existing patterns in nominal data. For instance, having 3 possible LLH types genetic algorithm, simulated annealing and evolution strategy, the label encoding will encode such parameter values to numbers 0, 1 and 2 respectively. When the such data is passed to surrogate for learning, the model may interpret that GA is closer to SA than to ES in the search space. To prevent this, the other preprocessing type should be used for instance, binary encoding.

In any case, the intent of this discussion is to provide the reader an insight of data preprocessing importance, but the discussion of possible cases and their influence are out of this thesis scope. Here instead we decided to gain a certain level of flexibility by providing a uniformed wrapper for the preprocessing routines, implemented in Scikit-learn machine learning framework~\cite{scikit-learn}.

We omit the details of wrapper implementation since it is a single class, that is instantiated with provided scikit-learn preprocessing unit. The wrapper is executed each time before the actual surrogate performs learning and after making the prediction to inverse the transformation.


\subsection{Prediction Models}\label{impl: prediction models}
As a derivative from predictor implementation, the underlying prediction models should implement unified interface and behavior. Due to predictor, the models are acting on a `flat' search space levels. This enables us to use a vast range of possible surrogates for instance, linear regression models. In fact, the previously used in BRISEv2 ridge regression with polynomial features is nothing else, but a combination of data preprocessing from the Section above and the ridge regression model from scikit-learn. Below we discuss an implementation of a unified wrapper for scikit-learn linear models.

As a step further, we also add the implementation of Multi Armed Bandit (MAB) selection strategy proposed in~\cite{auer2002finite}. This is motivated by a great reported performance of the selective hyper-heuristics, with MAB as HLH. However, it is worth to mention that MAB is applicable only to categorical parameters types.

The previously available in BRISEv2 Bayesian Tree Parzen Estimator should be decoupled from the data preprocessing logic however, no other major changes except refactoring are required. Thus, there is no reasons to present TPE implementation here.

\paragraph{Scikit-learn linear models wrapper.}
Scikit-learn is one among most popular open-source machine learning frameworks. As a consequence of flexible decisions ($H<=T$ framework design patter), the scikit-learn often plays a central role providing many implementations of building blocks for machine learning pipelines. This advantages in combination with a comprehensive documentation result into a large and active framework community~\footnote{Scikit-learn GitHub repository~\url{github.com/scikit-learn/scikit-learn}}.

All available in scikit-learn linear regressors implement the same interface and usage routines. Firstly, before usage a regression model should be trained on a data providing separately \emph{features} and \emph{labels}. Afterwards, one may request a prediction for unforeseen features and surrogate will produce a corresponding label according to learned dependencies. This implies that to obtain a prediction of the best parameter combination, one should solve the same optimization problem. However, the crucial advantage of surrogate models is the reduced estimation cost.

To reuse an available in scikit-learn surrogate models we create wrapper as an implementation of unified $Model$ interface, which is required in $Predictor$. The pseudo-code of this wrapper is presented on the \cref{impl: sklearn model wrapper pseudo-code}.

\begin{code}[language=Python, caption=Scikit-learn linear model wrapper pseudo-code., label=impl: sklearn model wrapper pseudo-code]
class SklearnModelWrapper(Model):
	def build_model(features, labels, features_description):
		features_preprocessors, labels_preprocessors = build_preprocessors()
		transformed_features = features_preprocessors.transform(features)
	 	transformed_labels = labels_preprocessors.transform(labels)
	 	
	 	score = cross_validation(model, transformed_features, transformed_labels)
	 	if score > threshold:
	 		model.fit(transformed_features, transformed_labels)
	 		model_is_built = True
	 	else:
	 		model_is_built = False
	 	return model_is_built
	 
	 def predict():
	 	features = random_sample(features_description)
	 	features_transformed = features_preprocessors.transform(features)
	 	
	 	labels_predicted_transfored = model.predict(features_transformed)
	 	labels_predicted = labels_preprocessors.inverse_transform(labels_predicted_transfored)
	 	
	 	features_transformed_chosen = select_by_labels(features_transformed, labels_predicted)
	 	prediction = features_preprocessors.inverse_transform(features_transformed_chosen)
	 	
		return prediction
\end{code}

During the model creation, we firstly instantiate features and labels preprocessors, and transform the input data (lines 3-5). The underlying process of model building includes also a verification step, which is performed by means of cross-validation: splitting the set of data onto \textit{k} folds, training \textit{k} times model each time excluding one fold for validation and using the others for training (line 7). If the obtained score less than predefined threshold — the model is considered to be not accurate enough therefore, we reject it (line 12), forcing the predictor to use the random parameter values sampling. However, if the model is able to perform well, we train it on an entire dataset and store for further usage (lines 8-10).

Later, during the prediction (if the model was built successfully) we firstly sample features from this level and transform them to fit into created previously model (lines 16-17). Afterwards, we use a regression model to make a prediction for sampled features and transform those predictions back into original labels (lines 19-20). Finally, we select the best feature by means of predicted labels, transform it and return (lines 22-23).

\subsubsection{Multi Armed Bandit}
Originally, the Multi Armed Bandit (MAB) problem were introduced by~\cite{robbins1952some} and defined as: for given set of choices $c_i$ with unknown stochastic reward values $r_i$, which are distributed normally with variance $v_i$, the goal is to maximize the accumulated reward, sequentially selecting one among available choices $c_i$. The problem derived its name as an analogy to one-hand slot machines in casino and comprises well-known exploration vs exploitation dilemma.

In most of the times, MAB is solved by RL approaches, where before performing each new step, the already available evidences are analyzed. The authors in~\cite{auer2002finite} propose the Upper Confidence Bound algorithm, which proposes an intuitive solution: in iteration $k$, among available choices select one with a maximal UCB value. The category UCB values is calculated according to the \cref{impl: ucb formula}, where first component $Q$ is a quality of category under evaluation and represents the exploitation portion of UCB, while the second component estimates the exploration portion with $C$ as a balancing coefficient. 

\begin{equation}
UCB = Q + C \cdot \sqrt{\frac{2 \log \sum_{1}^{i} n_k^i}{n_k}}
\label{impl: ucb formula}
\end{equation}

In this work we implement Fitness-Rate-Average based MAB (FRAMAB) with two reasons: (1) it is an intuitive and robust approach, (2) according to benchmarks in~\cite{ferreira2017multi}, it outperforms the other. In FRAMAB, $n_k^i$ denotes the overall number of categories, while $n_k$ is a number of times the category under evaluation was selected. The quality estimation here is the average improvement, obtained by the underlying category.

As for the balancing coefficient $C$, the authors in~\cite{ferreira2017multi} were evaluating a range of values $10^(-4)...10^(-1)$ and the dominance for various problem types were different. In addition to the statically used $C$ value, we add a mechanism for proper $C$ estimation by means of standard deviation among improvement. The motivation is as follows: if the deviation is high, there exist an uncertainty in category domination, therefore we encourage the exploration portion of UCB values. Since the implementation straightly repeats provided above algorithm description, we do not provide a pseudo-code.


\section{Low Level Heuristics}\label{impl: LLH}
When the HLH is ready to solve the problem, time comes to provide the tools for solving. A role of LLH in our selection hyper-heuristic (HH) may play a single heuristic algorithm or even a meta-heuristic (MH). As we discussed in \cref{bg: mh}, the present time in MH research is referred as the framework growth time. Therefore, we wish not only to reuse a single heuristic among available in framework, but be able to instantiate a set of underlying heuristics. Therefore, in this section we present a review of several toolboxes (meta-heuristic frameworks) with an intent to select the best suited one, implement a facade for framework usage and use it as LLHs in our hyper-heuristic.

Before diving into description of available frameworks we briefly refresh the LLHs characteristics, with respect to which we analyze each framework:
\begin{enumerate}
	\item \textbf{Set of meta-heuristics}, which we will be able to use as LLH in our HH.
	
	\item \textbf{Exposed hyper-parameters}, which are required for LLH (MH) tuning. We point it out explicitly, since it happens that the parameters of an algorithm are exposed not fully.
	
	\item \textbf{Set of supported optimization problems} among which required in our case TSP. The wider this set, the more use-cases developed HH is able to tackle.
	
	\item \textbf{Warm-startup} is required to continue the problem solving from previously reached solution. Therefore, the underlying LLH should not only to report finally found solution(s), but also to accept them as starting points. Note, by plural form we refer to population-based MHs.
	
	\item \textbf{Termination criteria} characteristic is reviewed by means of providing the wall-clock time or number of target system evaluations criteria, which are required to control the intermediate results of optimization process by HH.
\end{enumerate}


\subsection{Low Level Heuristics Code Base Selection}\label{implementation:llh code basis selection}
As candidates, we select following open-source frameworks: Solid~\footnote{Solid GitHub repository~\url{github.com/100/Solid}}, mlrose~\footnote{mlrose GitHub repository~\url{github.com/gkhayes/mlrose}}, pyTSP~\footnote{pyTSP GitHub repository~\url{github.com/afourmy/pyTSP}}, LocalSolver~\footnote{LocalSolver website~\url{localsolver.com}}, jMetal~\footnote{jMetal GitHub repository~\url{github.com/jMetal/jMetal}} and jMetalPy~\footnote{jMetalPy GitHub repository~\url{github.com/jMetal/jMetalPy}}.

\paragraph{Solid.} A framework for gradient-free optimization. It comprises basic versions of a wide MH range with exposed hyper-parameters: genetic algorithm, evolutionary algorithm, simulated annealing, particle swarm optimization, tabu search, harmony search and stochastic hill climbing. The support of warm-startup is not provided and requires changes in every algorithm as a consequence of single base class absence. As for the budget, algorithms in this framework support several types such as the maximal number of evaluations and termination after reaching the desired quality. Once again, to add a new termination criteria, one should modify the core of all algorithms. The framework does not provide the problem instances therefore, to use it one will need to carry out not only a domain-specific framework adaptation, but also the problem description according to this adaptation.

\paragraph{mlrose.} A framework with implementation of various well-known stochastic optimization algorithms such as: na\'ive and randomized hill climbing, simulated annealing, genetic algorithm and mutual-information-maximizing input clustering (MIMIC) algorithm. Each listed solver is implemented as a separate function with exposed set of hyper-parameters and possibility to control an initial state, which is handy to continue with the problem solving. With respect to implemented OPs, the framework comprises a large set of different types: one max, flip-flop, four and six peaks, continuous peaks, knapsack, traveling salesman, n-queens and max-k color optimization problems. As for the proposed termination criteria, the only one criterion controlling the number of TS evaluations fits our needs. As in the previous framework, here the algorithms are not sharing the same code basis therefore, it may require much effort for their adaptation.

\paragraph{pyTSP.} A system, specially designed to tackle a traveling salesman optimization problem. Together with visualization techniques, it also provides a bunch of algorithms. Here they are divided into four groups: (1) construction heuristics with the nearest neighbor, the nearest insertion, the farthest insertion and the cheapest insertion algorithms, (2) a linear programming algorithm, (3) simple heuristics among which a pairwise exchange, which is also known as 2-opt, a node insertion and an edge insertion, and finally (4) a meta-heuristics, represented by the genetic algorithm. As one may expect, the only supported problem type is the TSP. Other drawbacks of this framework is partial hard-coding of hyper-parameters and absence of exposed termination criteria. Also, as one may expect, the construction heuristics are not expose the possibility to seed them. Talking about the other algorithms, the functionality to specify the for initial solution is present however, not in all algorithms.

\paragraph{LocalSolver.} A commercial optimization tool with free academic license. It is implemented in C++, but an API is exposed to such programming languages as Python, C++, Java and C\#. The software is a local search programming paradigm implementation~\cite{benoist2010toward,benoist2011localsolver} therefore, the algorithm itself and its parameters are not exposed. From the user side, it is required to provide a solver-specific problem description. Thanks to a detailed documentation, the desired TSP could be found among the number of other problem examples. Two possible termination criteria are exposed: a wall-clock time and a number of solver iterations. Also, the framework supports a possibility of setting an initial solution for the solver therefore, it looks like a good candidate for the LLH. However, our trial to use the tool showed up that the provided academic license could not be easily used within BRISEv2 containerized architecture. A possible work-around is to deploy a license server on a host machine and configure workers to register themselves, but we found this to be expensive from the perspective of configuration one-time solution.

\paragraph{jMetalPy.} An open-source meta-heuristic framework for multi- and single-objective optimizations. Among the provided single-objective algorithm one will find genetic algorithm, evolutionary strategy, local search (hill climber) and simulated annealing. Even if the list of proposed heuristics is not the largest in comparison to other reviewed frameworks, every implemented algorithm exposes its hyper-parameters for tuning, and we found the code well-structured. A functionality for warming-up the solving process by already obtained solutions is available out-of-the-box and various termination criteria are ready as well (among which the time-based and the number-of-evaluations-based). The list of supported single-objective optimization problems consists of knapsack, traveling salesman and four synthetic problems: one max, sphere, Rastrigin and subset sum.

\paragraph{jMetal.} A meta-heuristic framework implemented in Java is an alternative to reviewed above Python-based jMetalPy. This framework also provides meta-heuristics for multi- and single-objective OP. For later, jMetal developers implemented following algorithms: evolution strategy, covariance matrix adaptation evolution strategy (CMA-ES), genetic algorithm, particle swarm optimization (PSO), differential evolution and coral reef optimization. It is worth to mention that among this list, not all algorithms are universally applicable to wide range of OPs. More concretely, CMA-ES, PSO and differential evolution can be applied only to OPs with continuous numeric input such as synthetic mathematical problems. In contrast to his implemented in Python brother, jMetal supports only based on number of evaluations termination criteria, and do not support algorithm warm-startup at all.

To sum up our discussion, we aggregate the desired characteristics in Table, similar to \cref{iml: table code basis selection} presented during the HLH code basis selection. Once again, the characteristics qualities are scored into three ordinal values: poor, average and good.

\cellcolor{red!25}Poor
\cellcolor{yellow!25}Average
\cellcolor{green!25}Good

\begin{table}[h!]
	\centering
	\begin{tabular}{l||cccccc}
		\textbf{Characteristic} & \textbf{Solid} & \textbf{mlrose} & \textbf{pyTSP} & \textbf{LocalSolver} & \textbf{jMetalPy} & \textbf{jMetal} \\
		\hline
		\hline
		Set of heuristics & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{yellow!25}Average & \cellcolor{green!25}Good \\
		
		Exposed hyper-parameters & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good \\
		
		Provided OPs & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average \\
		
		Warm-startup support & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{red!25}Poor \\
		
		Termination criteria & \cellcolor{yellow!25}Average & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{yellow!25}Average \\
	\end{tabular}
	\caption{LLH frameworks characteristics.}
	\label{iml: table llh selection}
\end{table}



\subsection{Scope of work analysis}
\paragraph{opened PR}

\section{Conclusion}
