\chapter{Implementation Details}\label{impl}
In this Chapter we dive into the development description of listed in \cref{Concept description} requirements.
 
The best practice in software engineering is to minimize an implementation effort reusing the already existing and well-tested code. With this idea in mind we select one of reviewed in \cref{bg: parameter tuning} open-source parameter tuning frameworks as the code basis for desired hyper-heuristic. We are also planning to reuse the existing low level heuristics (LLH) implementation from other frameworks. While the LLH-basis may be used almost out-of-box, the HLH-basis requires changes to be applicable in our case.

In \cref{impl:hlh code basis section} we analyze the parameter tuning frameworks from a perspective of required effort to implement listed in \cref{concept:prediction} HLH characteristics. In \cref{impl:hlh code basis conclusion} we conclude the analysis selecting the most suited HLH code basis. Afterwards, we split the HLH implementation description into two logical parts, as we have done during the concept description. In \cref{impl: search space} we discuss the search space development, while \cref{impl: prediction logic} is dedicated to the prediction process. Finally, in \cref{impl: LLH} we perform a similar code basis selection for LLH, present a set of reused meta-heuristics, their adaptations and the process of importing them in our system.


\section{Hyper-Heuristics Code Base Selection}\label{impl:hlh code basis section}
Before starting with the framework analysis, let us firstly outline the important characteristics from the implementation perspective.

The first two crucial criteria are the framework \emph{variability} and \emph{extensibility}. The desired HLH should be easily variable in terms of changing such functionality as the learning models or the termination criteria to use a new one. We are also planning to use a possibly different models for the LLH selection and parameters control therefore, the code basis should be variable in terms of different model usage for each prediction level (see \cref{concept:pict:Level-wise prediction process}).

The next is support for the \emph{on-line optimization}. It is a bit complex system characteristic, which we are willing to distinguish. As it turns out, many parameter tuning systems require full evaluation of target system,  while others are not, but they run evaluations isolated. In case of the desired hyper-heuristic, we treat the configuration evaluation as a trial to improve the problem solution using a particular LLH and tuning its parameters. It implies an important system ability to tackle the optimization problem (OP) using reinforcement learning approaches (\cref{concept:parameter control}).

The final characteristic is the \emph{conditional parameters} support. Since, it is a complex feature laying not only in the search space but also in the prediction process, we pay a close attention to both of them.

\subsection{Parameter Tuning Frameworks Analysis}\label{impl: Parameter Tuning Frameworks Analysis}
\paragraph{SMACv3}
We begin our review with the implementation of Sequential Model-based Algorithm Configuration framework, distributed under the BSD-3 license.

The idea of SMACv3 lays in an enhancement of ROAR mechanism with the model-based sampling algorithm.

The ROAR mechanism is a derivative from the FocusedILS algorithm (solver in the parameter tuning framework ParamILS~\cite{hutter2009paramils}) where each evaluation of a new candidate solution on problem instance performed sequentially and in isolation. Since the ROAR evaluation strategy is also used in SMACv3, we expect it to require much effort to enable system solving the problem on-line.

As we mention in \cref{bg: smac}, the underlying surrogate model in SMACv3 is selected statically among random forest or Gaussian process kernel density estimator. Both models could naturally fit complex dependencies among parameters. To sample next configuration, the \emph{one-exchange} neighborhood of the best found so far configuration is traversed using the surrogate model and the expected improvement estimations.
An ability of both surrogates to fit a sparse search space is promising, and the usage of expected improvement guarantees to converge the search process to the global optimum given enough time. However, the major drawback in this system is a lack of abilities to include the conditional dependencies between parameters into the sampling process. In fact, the used here search space representation framework ConfigSpace~\cite{configspace} is able to specify the dependencies among hyper-parameters. But, to the best of our knowledge, the one-exchange neighborhood used as sample mechanism in SMACv3, is unaware of the dependencies therefore, violates them during the parameters sampling resulting in illegal combinations. Those cases are controlled and rejected by the ConfigSpace, but we believe that in case of sparse search spaces it could lead to ineffective sampling and struggling in system predictive abilities. 

Unfortunately, we did not find any officially published empirical studies of such cases and can only make guesses based on our own intuition, but SMACv3 developers advises for operation in such cases~\footnote{Visit SMACv3 repository~\url{https://github.com/automl/SMAC3/issues/403}} may serve as an evidence to our assumptions correctness. One of the possible solutions here could be the usage of a conditional-aware one-exchange neighborhood definition for sampling process.


\paragraph{IRACE} This framework implements the iterated racing algorithm to evaluate a set of configurations during the parameter tuning session (\cref{bg: irace}). The software is distributed under the GNU General Public License with an open source code.

The framework uses a \emph{Friedman test}~\cite{conover1980practical}, or as an alternative \emph{paired t-test} for statistical analysis of racing in parallel configuration. As the surrogate models, IRACE uses the probability distributions of those parameter values, which shown to be good during the racing step. The prediction process is defined as the step-wise sampling in previously derived distributions. It elegantly handles the conditions among parameters and illuminates the possibility of invalid configuration appearance. However, we found it to be static in terms of variability and extensibility on the learning mechanisms.

In terms of parallel evaluations, the algorithm utilizes all available resources at the beginning of each racing step, but as the process continues fewer evaluations are executed simultaneously therefore, the available resources are idling and not utilized completely at all steps of IRACE execution.

For the on-line problem solving, let us discuss the racing algorithm. As we described in \cref{bg: irace}, this step is executed with (1) a set of TS configurations sampled for evaluation and (2) a benchmark set of optimization problems. Afterwards, the TSs are starting to solve the problem set under each configuration, while the racing algorithm terminates worst-performing configurations. In case of hyper-heuristic, it is possible to use a single problem instance and the divided into parts TS running time, which at each pause synchronize the current solutions to proceed with the best found. Doing so, it will be possible to adapt the system for on-line problem solving however, the granularity of parameter control will be reduced. The reason for such reduction is the amount of information obtained after each race: only the best configurations are reported, leaving the performance evidences of the others behind. We believe, this information may be used to create a more precise surrogate models (for instance, TPE).

\paragraph{HpBandSter} As we discussed in \cref{bg: bohb}, HpBandSter is an implementation of BOHB algorithm, which turns to be a hybridization of Hyperband and Bayesian Optimization approaches.

A role of Hyperband in this duet is the configuration evaluation and comparison, while the Bayesian Tree Parzen Estimator (TPE) suggests which configuration to evaluate next. The idea behind this combination lays in elimination of weak sides in each algorithm with strengths of the other. For instance, in original Hyperband the configuration sampling is made uniformly at random, which results in a slow converge of an optimization process. As for the BO TPE, a drawback here lays in a configuration evaluation. Na\"ive Bayesian optimization approaches do not take into account the early evidences about the TS performance. Thus, even when the proposed configuration results in a poor starting and intermediate TS performance (which may be an evidence of a weak final performance), BO still continues TS execution. These facts motivated authors to merge those two algorithms to create one for parameter tuning with strong anytime (HB) and final (BO) performance. The resulting hybrid effectively uses available computational resources in parallel (HB) in combination with scalable and robust learning mechanisms (BO).

Let us discuss the process of handling the conditions between hyper-parameters. SMACv3 and HpBandSter as well use ConfigSpace framework for search space representation. As we discussed in SMACv3 description above, ConfigSpace naturally allows to encode the dependencies and conditions among parameters. The TPE learning models are also able to fit somehow these dependencies by means of the \emph{impuration} mechanism\cite{levesque2017bayesian}. Shortly saying, when fitting the surrogate models, the disabled parameters (or their values) are replaced with their default values. Later, while building the surrogate models those default values are ignored therefore, the probability densities still represent a proper parameter values distributions. However, consider an appearance of two configurations families: $C_1$ and $C_2$, such that some parameter $P_i$ is forbidden in $C_1$ but is required in $C_2$. On contrary, the other parameter $P_j$ is required in $C_1$ but forbidden in $C_2$. If these configuration families are turn to be superior, the resulting probability densities will be biased towards $P_i$ and $P_j$ values. As a consequence, the proposed prediction mechanism will sample non-default parameter values for both $P_i$ and $P_j$, which results in the configurations with violated parameter dependencies. The sparser the search space, the more harming an effect will be in a prediction performance. One possible treatment here is to change the sampling process introducing an intermediate layer, which will perform a parameter prediction in level-wise approach suggested in \cref{concept:prediction}.

\paragraph{BRISEv2}
BRISEv2 is a software product line (SPL), created with an aim at solving the expensive optimization problems in general and for the parameter tuning in particular (\cref{bg: brise}).

The advantage of BRISEv2 over other systems comes from its \emph{main-node} modular design. It is a set of cooperating core entities (Experiment, Search Space and Configuration) with other non-core entities, exposed to user for variability. The prediction models, termination criteria, outliers detectors, repetition strategies, etc. are representatives of these non-core and variable components. A number of implementations are provided out-of-the-box for all variable entities, but we focus our attention only to the implemented sampling process. The reason of such a greedy review is that the underlying search space representation is carried out by the same ConfigSpace, while the provided surrogate models are ridge regression model with polynomial features and Bayesian Tree Parzen Estimator (TPE). We are not going to repeat ourselves reviewing the ConfigSpace + TPE combination, but we have to put a few words about the ridge regression.

Ridge is the machine learning linear regression model with regularization~\cite{hoerl1970ridge}. Being a linear model, its abilities to fit a sparse search spaces is poor therefore, the machine learning community are suggested to treat such cases with \emph{conditional linear regression models}~\cite{DBLP:journals/corr/abs-1806-02326}. The underlying idea is to split the search space into sub-search spaces and build a separate regression model there. But, to the best of our knowledge, this approach is not built-in into the used in BRISEv2 ridge regression model.

As for the on-line problem solving support, the routine of an optimization process, implemented in BRISEv2 is nothing else than a reinforcement learning. After each new obtained evidence (configuration), a new surrogate model is built to react on the learning process by predicting the next configuration. This makes the on-line parameter tuning approach, presented in \cref{concept:parameter control} embedding into BRISEv2 much easier.

\subsection{Conclusion on Code Base}\label{impl:hlh code basis conclusion}
The most among reviewed parameter tuning systems share the same SMBO approach for problem solving. They utilize a rather similar techniques for building the surrogate models and making the predictions however, the different system architectures are implemented.

To sum um our review, we use a term \emph{quality} to aggregate both (1) the provided  out-of-the-box desired characteristic support and (2) the required effort to adapt it, if necessary. For the visual representation, we collect the reviewed characteristic qualities in each software framework into \cref{iml: table code basis selection}.

The quality estimates are quantized into three ordinal values:
\begin{enumerate}
	\item \textbf{Poor} quality means the weak characteristic support and much effort required to improve it.
	\item \textbf{Average} quality means the weak characteristic support and requires less amount of effort to provide it.
	\item \textbf{Good} quality means a good out-of-the-box characteristic support, and requires minor or no changes at all.
\end{enumerate}

\begin{table}[h!]
	\centering
	\begin{tabular}{l||cccc}
		\textbf{Characteristic} & \textbf{SMACv3}& \textbf{IRACE} & \textbf{HpBandSter} & \textbf{BRISEv2} \\
		\hline
		\hline
		Variability \& extensibility & \cellcolor{yellow!25}Average & \cellcolor{red!25}Poor & \cellcolor{yellow!25}Average & \cellcolor{green!25}Good \\
	
		On-line optimization & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average \\
	
		Conditional parameters & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor \\
	\end{tabular}
	\caption{Code basis candidate systems analysis.}
	\label{iml: table code basis selection}
\end{table}

Among the reviewed software systems, the majority were created as an implementation of some concrete algorithm (or a combination of algorithms), which results in the reduction of the system flexibility. Every reviewed framework requires much adaptation effort and preparation steps, which we will discuss in upcoming sections. Between such features as a proper support of conditional parameters vs variability and extensibility, the former plays a settle role in our case. Thus, from our perspective, BRISEv2 is the most promising candidate for the hyper-heuristic with parameter control creation.

\section{Search Space}\label{impl: search space}
Previously, in \cref{concept:search space} we presented a set of structural requirements for the search space representation: parent-child relationship should be presented explicitly, supporting different parameter types in a values-specific approach. To support the prediction process, in \cref{concept:prediction} we listed the functional requirements in a form of mechanisms: data filtering, sampling propagation, parameter description and configuration verification.

In this section we (1) analyze the available ConfigSpace framework, how it fits to our requirements and (2) decide whether to use it or to set it aside to perform our own search space representation implementation.

\subsection{Base Version Description}
From the structural point of view, in ConfigSpace\footnote{ConfigSpace GitHub repository: \url{https://github.com/automl/ConfigSpace}} the parameter coupling is made implying parent-child relationship, which fit into our requirements. The set of parameter types suite the most of use-cases and the value-specific dependencies are supported as well. Thus, the structural requirements S.R.1, S.R.2 and S.R.3 are perfectly met.

When it comes to the functional requirements, ConfigSpace samples random configurations in a completion approach. In other words, there is no step-wise configuration construction, but only the final and valid ones are produced. To the best of our knowledge, there is no straightforward way to expose the underlying parent-child dependencies among parameters to distinguish a non-dependent, which is required for the prediction models, except of carrying them on a side and evaluating manually. As a consequence, the data filtering mechanism should be implemented on a side and the sampling propagation as well. The framework exposes an ability to validate a fully created configuration but not a set of parameters (flat validation). It worth mo mention that the configuration in this case turns to be a proprietary class. As for the parameter description, the amount of exposed knowledge is satisfying. Here we conclude that the functional requirements, except S.F.R.3, are not met.

As the conclusion, we decided to set aside the $3^{rd}$ party ConfigSpace framework. The reason for dong so is the amount of adaptation effort and the usage of foreign dependencies, more concretely: (1) it requires much adaptation effort and implies its usage as a core entity in BRISEv2, (2) it obligates us to replace the other core entity — Configuration, (3) it forces us to use a third proprietary entity — Hyperparameter. 

\subsection{Search Space Implementation}\label{impl: search space impl}
From the structural requirements we know that the parameters in search space should be treated uniformly. The desired feature tree shape is perfectly handled by the \emph{composite} design pattern. With this idea in mind, we construct the search space as a composite \emph{Hyperparameter} object with four possible hyper-parameter types: integer and float as numerical, nominal and ordinal as categorical. This fulfilling S.R.2, specified in \cref{concept:search space}. An implemented class diagram could be found in Appendix.
\todoy{add class diagram in appendix}

In the code snippets provided through the explanation we highlight signatures of implemented methods, which fulfills our requirements specified in \cref{Concept description}.

\paragraph{Search space construction.} The S.R.3 implementation is performed by adding a construction method \emph{add\_child\_hyperparameter} in the Hyperparameter class (\cref{impl: S.R.1 listing}). It should be called on a parent object, specifying the activation value(s) (\emph{activation\_categories} argument) of parent hyper-parameter which are exposing the child. 

\begin{lstlisting}[language=Python, caption=S.R.1 implementation., label=impl: S.R.1 listing]
class Hyperparameter:
	...
	def add_child_hyperparameter(self, other: Hyperparameter, activation_categories: Iterable[CATEGORY]) -> Hyperparameter:
		pass
	...
\end{lstlisting}

Note, currently we support a composite construct only by means of categorical parameter types therefore, requiring a list of activation categories. We postpone a composition on numerical ranges enhancement for the future work, while our current needs do not include such cases.

\paragraph{Search space role in prediction.}
Imagine a number of configurations were already evaluated. For making the prediction in tiered approach, the parameter values on a current level should be selected before moving to the nest one. For it, we firstly filter data, which fits to this level by means of S.F.R.1. The filter accepts already chosen parameter values and iterates over the available configurations. At each iteration it finds out, whether the already chosen parameter values (\emph{base\_values}) form a sub-feature-tree of the parameter values under comparison (\emph{target\_values}). It is implemented in form of hyper-parameter instance method \emph{are\_siblings}, presented in \cref{impl: S.F.R.1 listing}. As the outcome, this method decides, whether should this particular configuration be included into the dataset or not. For instance, if the selected LLH type in \emph{base\_values} is not the same as one in \emph{target\_values}, the result will be negative.

\begin{lstlisting}[language=Python, caption=S.F.R.1 implementation., label=impl: S.F.R.1 listing]
class Hyperparameter:
	...
    def are_siblings(self, base_values: MutableMapping, target_values: MutableMapping) -> bool:
    	pass
	...
\end{lstlisting}

After data filtering, the time comes to find out, which parameter values we must predict. For doing so, the search space, must expose the current level parameters by means of S.F.R.2 implementation in \emph{generate} method in \cref{impl: S.F.R.2 listing}. Since we always interact with a search space root object, the call to \emph{generate} is executed recursively. If a callee finds itself in \emph{values} argument (which depicts the current $parameter\ name \rightarrow random\ parameter\ value$ mapping), it redirects a call to all \textit{activated} children. If it does not, it adds itself a to the \emph{values} and terminates the recursion.

\begin{lstlisting}[language=Python, caption=S.F.R.2 implementation., label=impl: S.F.R.2 listing]
class Hyperparameter:
	...
    def generate(self, values: MutableMapping) -> None:
    	pass
	...
\end{lstlisting}

A randomly sampled for the current level values are then used for getting a description of current level and using it to (1) cut-off the data from levels above and below (simply selecting the required key-value pairs from the parameter mapping), (2) build the surrogate models and (3) make the prediction.

To build the surrogate models we require an available data (parameters) description. Thus, the S.F.R.3 implementation is performed in method \emph{describe} presented in \cref{impl: S.F.R.3 listing}. This is once again a recursive call, which terminates when the parameter object can not find the activated children or himself in the provided \emph{values}.

The resulting description contains a mapping from parameter name to its type and the range of possible values: either a set of categories for categorical, or lower and upper boundaries for numerical types.

\begin{lstlisting}[language=Python, caption=S.F.R.3 implementation., label=impl: S.F.R.3 listing]
class Hyperparameter:
	...
    def describe(self, values: MutableMapping) -> Description:
    	pass
	...
\end{lstlisting}

This description is then used by the prediction models for building surrogates and making the predictions, which will replace a randomly sampled parameter values obtained after \emph{generate} method call.

The described above process is controlled by S.F.R.4, implemented as method \emph{validate}, presented in \cref{impl: S.F.R.4 listing}. The control occurs in two places. Firstly, before starting a new loop of $filter \rightarrow propagate \rightarrow describe \rightarrow predict$, we check whether the construction process is finished (deep validation), meaning all parameter values were chosen, and we have a valid configuration. And secondly, after making the prediction by models (flat validation). In the later case, if the conditions are violated, the predicted values are discarded and sampled randomly. Since the sampling process implemented in hyper-parameters guarantees to provide valid parameter values, after maximally $N$ mentioned above loops, we derive a new and valid configuration, where $N$ is a maximal depth in the defined search space.

\begin{lstlisting}[language=Python, caption=S.F.R.4 implementation., label=impl: S.F.R.4 listing]
class Hyperparameter:
	...
	def validate(self, values: MutableMapping, recursive: bool) -> bool: pass
	...
\end{lstlisting}


\section{Prediction Process}\label{impl: prediction logic}
The next step is an investigation and planning of prediction logic adaptation.
In \cref{impl: Parameter Tuning Frameworks Analysis} we learned that BRISEv2 provides two learning models: Bayesian tree parzen estimator (TPE) and ridge linear regression. Both models could be used as surrogates within a tiered parameter values sampling however, this process should be generalized.

P.F.R.1 implies the addition of entity, which will encapsulate the prediction process, described in \cref{impl: search space impl}. We also make this entity responsible for the forgetting strategy therefore, fulfilling P.F.R.3. Both requirements are not yet fulfilled in BRISEv2, so we must implement them from scratch.

As for P.F.R.2, the current implementation of BRISEv2 already provides some level of model unification using a required interface, which, however, is too coarse-grained and implies binding of three logical steps: data preprocessing, surrogate models creation and optimization with surrogates to predict a next configuration.

The following parts of prediction logic description are dedicated to (1) P.F.R.1 + P.F.R.3 implementation in form of \emph{Predictor} entity and P.F.R.2 in form of decoupled data preprocessing mechanism and the prediction models. Note, the current implementation does not solve the problem of binding surrogate models creation and optimization over them. We postpone the solution for future work. Instead, we implement a simple random search over the surrogate models since as mentioned in \cref{bg: parameter tuning}, given enough evaluations, the random search results becomes comparable to model-based algorithms. We are allowed to do so, since the evaluations over surrogate models are cheap.

\subsection{Predictor Entity}
In addition to previously listed logic during the search space description, a role of predictor is also to decouple learning models from (1) feature tree shaped search space and (2) other core entities such as Configuration. Besides the static search space, the input to predictor is the available in a moment data (evaluated configurations), while the desired output is a configuration. \cref{impl: P.F.R.1 + P.F.R.3 implementation pseudocode} provides a pseudo-code of predictor implementation.

To implement the information forgetting mechanism, we refer the idea similar to mentioned in~\cite{ferreira2017multi} \emph{sliding window}. According to it, the predictor should use specified in a settings number of the latest configurations as information for surrogate models creation. We modify this logic, allowing user to specify not only a static number, but also a percentage of the latest configurations (line 3), which fulfills the P.F.R.3. Naturally, more exotic approaches may arise such as the statistical analysis to estimate a required configuration number or the other type of meta-learning, but we leave it out of this thesis scope.

The next question is a decoupling of prediction models from the search space structure by means off fulfilling P.F.R.1. As we discussed in \cref{impl: search space impl}, to predict a parameter values on each level, the models should be built on only related to this level information. For doing so, after filtering the data (line 6), predictor propagates the prediction from previous level to a current one (line 7), removes the parameters values from other levels and derive a description for the obtained parameters (line 9). Independently, we instantiate a specified in the predictor settings surrogate model for this level and fit it with the related information (lines 11-13). Afterwards, we make a prediction and check if the search space boundaries are not violated (lines 16-17). If either model can not properly fit the data, or the prediction is invalid, we keep the parameter values sampled randomly (lines 18-20).

\begin{code}[language=Python, caption=P.F.R.1 + P.F.R.3 implementation pseudo-code., label=impl: P.F.R.1 + P.F.R.3 implementation pseudocode]
class Predictor:
	def predict(measured_configurations):
		# P.F.R.3
		level_configurations = trim_in_window(measured_configurations)
		prediction = Mapping()
		
		# Deep validation
		while not search_space.validate(prediction, recursive=True):
		
			# Filtering the data
			level_configurations = filter(search_space.are_siblings(prediction, x), level_configurations)
			
			# Propagate the prediction
			randomly_generated = search_space.generate(prediction)
			
			# Derive the level description
			full_description = search_space.describe(randomly_generated)
			level_description = trim_previous_levels(description, prediction)
			
			# Cut-off data and build models
			data = trim_accodring_to_description(level_configurations, level_description)
			model = get_current_level_model()
			model.build(data, level_description)
			
			# Level prediction and flat validation
			if model.is_built():
				level_prediction = model.predict()
				if not search_space.validate(level_prediction, recursive=False):
					level_prediction = randomly_generated
			else:
				level_prediction = randomly_generated
		
		return Configuration(prediction)
\end{code}

For the sake of simplicity we omit some minor implementation details and provide the description of (1) data preprocessing and (2) available surrogate models in the Sections below.

\subsection{Data Preprocessing}\label{impl: preprocessing}
The data preprocessing concepts may be split into two complementary parts: an obligatory data encoding and optional data transformation. The first is required to make the underlying model compatible with the provided data. Imagine the parameters values to be a simple strings. Having a surrogate model, which is constructed as the probability densities of parameter values (TPE), one should first derive a numeric data encoding those string parameter values. The second concept is applied on a data, which is already suitable. This is usually done to improve an available surrogate model accuracy by reducing the bias, variance or both. An example of encoding could be a simple indexing of all possible string values. It is performed as a replacement of strings by their indexes during the data preprocessing. On a contrary, for the transformation one may try to add the polynomial features to already available data with an aim to disclose more complex dependencies.

The decision of which encoding to use is often defined by the learning model as a choice among several applicable variants. On contrary, the decision on data transformation is carried out by user and depends on the concrete use-case and experience.

In all reviewed parameter tuning systems, the data preprocessing is implemented only by means of an obligatory for underlying learning models encoding and omitting the possible data transformation. In most cases, it is implemented as a simple label enumeration and not encapsulated at all (as an example, check ConfigSpace's Configuration method \emph{get\_array}~\footnote{ConfigSpace documentation~\url{https://automl.github.io/ConfigSpace/master/API-Doc.html}}). Being the most straightforward approach, this encoding may introduce a non-existing patterns in categorical data. For instance, having 3 possible LLH types: genetic algorithm, simulated annealing and evolution strategy, the label encoding will encode such parameter values to numbers 0, 1 and 2 respectively. When such data is passed to the surrogate for learning, some models may interpret it as the GA is closer to SA than to ES within a search space. To prevent this, the other preprocessing should be used for instance, binary encoding.

In any case, the intent of this discussion is to provide the reader with an insight of data preprocessing importance, but the discussion of possible cases and their influence are out of this thesis scope. Here instead we decided to gain a certain level of flexibility by providing a uniformed wrapper for the preprocessing routines implemented in Scikit-learn machine learning framework~\cite{scikit-learn}.

We omit the details of wrapper implementation since it is a single object decorator, instantiated with the provided scikit-learn preprocessing unit. The wrapper is executed each time before the actual surrogate performs learning and after making the prediction to inverse the transformation.

To make models and preprocessing units interfaces compatible we store the level information in form of DataFrames — tabular data representation in Pandas framework~\footnote{Pandas Github repository~\url{https://github.com/pandas-dev/pandas}}. In \cref{impl: P.F.R.1 + P.F.R.3 implementation pseudocode} line 21 denotes a step of configuration objects transformation to DataFrame, keeping only the current level features.


\subsection{Prediction Models}\label{impl: prediction models}
As a derivative from predictor implementation, the underlying prediction models should expose a unified interface and behavior. Due to per-level prediction process implementation, the surrogate models are acting on a search space levels without forbidding dependencies. This enables us to use in addition to previously discussed surrogates a vast range of other learning algorithms, for instance, linear regression models. In fact, the previously used in BRISEv2 ridge regression with polynomial features is nothing else then a combination of data preprocessing step from the Section above with the ridge regression model from Scikit-learn framework. Later in this Section, we discuss an implementation of unified wrapper for Scikit-learn linear models.

As a step further, we also add the implementation of multi-armed bandit (MAB) selection strategy proposed in~\cite{auer2002finite}. This is motivated by a great reported performance of the selective hyper-heuristics built on MAB as HLH. It is worth to mention that MAB is applicable only to categorical parameters types.

We also decouple the previously available in BRISEv2 Bayesian TPE from the data preprocessing logic however, no other major changes except refactoring are required. Thus, we do not find a reason for the detained presentation of TPE implementation here.

\subsubsection{Scikit-learn Linear Model Wrapper}\label{impl: sklearn wrapper}
Scikit-learn is one among the most popular open-source machine learning frameworks. As a consequence of flexible architecture ($H<=T$ framework design patter), Scikit-learn often plays a central role in other products providing implementations for numerous building blocks for machine learning pipelines. These advantages in combination with a comprehensive documentation result into a large and active framework community~\footnote{Scikit-learn GitHub repository~\url{https://github.com/scikit-learn/scikit-learn}}.

All available in Scikit-learn linear regressors are implementing the same interface and usage routines. For instance, before making a prediction, the regression model should be trained on a preprocessed data, providing separate sets of \emph{features} and \emph{labels}. Afterwards, one may use model to make a prediction for unforeseen features and the surrogate will produce a corresponding label according to the learned dependencies. This implies that to find the best parameter combination, one should still solve the optimization problem but with the reduced evaluation cost.

To reuse the available in framework surrogate models we create the wrapper as an object decorator, implementing the required in \emph{Predictor} \emph{Model} interface. The pseudo-code of this wrapper is presented in \cref{impl: sklearn model wrapper pseudo-code}.

During the model creation, we firstly instantiate features and labels preprocessors, and transform the input data (lines 4-6). The underlying process of model building includes also a verification step, which is performed by means of cross-validation: splitting the set of data to \textit{k} disjoint folds, training \textit{k} model each time excluding one fold for accuracy verification (line 9). If the potential model accuracy is less than predefined threshold — the model is considered to be not accurate enough therefore, we reject it (line 15), forcing the predictor to use the random parameter values. However, if the model is able to perform well, we train it on an entire dataset and store for further usage (lines 12-13).

Later, for making the prediction (if the model was built successfully) we firstly sample features values from this level uniformly at random (line 20). Afterwards, we transform them using the same preprocessing steps, as we applied during the train creation (line 21). Then, using the regression model, we make a prediction for sampled features and transform those predictions back into original labels (lines 23-24). Finally, we select the best feature by means of predicted labels, transform it and return to \emph{Predictor} (lines 26-27).

\begin{code}[language=Python, caption=Scikit-learn linear model wrapper pseudo-code., label=impl: sklearn model wrapper pseudo-code]
class SklearnModelWrapper(Model):
	def build_model(features, labels, features_description):
		# Data preprocessing
		features_preprocessors, labels_preprocessors = build_preprocessors()
		transformed_features = features_preprocessors.transform(features)
	 	transformed_labels = labels_preprocessors.transform(labels)
	 	
	 	# Model accuracy validation
	 	score = cross_validation(model, transformed_features, transformed_labels)
	 	if score > threshold:
	 		# Training on all available data
	 		model.fit(transformed_features, transformed_labels)
	 		model_is_built = True
	 	else:
	 		model_is_built = False
	 	return model_is_built
	 
	 def predict():
	 	# Solving a reduced optimization problem with help of surrogates
	 	features = random_sample(features_description)
	 	features_transformed = features_preprocessors.transform(features)
	 	
	 	labels_predicted_transfored = model.predict(features_transformed)
	 	labels_predicted = labels_preprocessors.inverse_transform(labels_predicted_transfored)
	 	
	 	prediction_transformed = select_by_labels(features_transformed, labels_predicted)
	 	prediction = features_preprocessors.inverse_transform(features_transformed_chosen)
	 	
		return prediction
\end{code}

\subsubsection{Multi-Armed Bandit}\label{impl: FRAMAB}
Originally, the multi-armed bandit (MAB) problem was introduced in~\cite{robbins1952some} and defined as follows: for a given set of choices $c_i$ with unknown stochastic reward values $r_i$, which are distributed normally with variance $v_i$, the goal is to maximize the accumulated reward, selecting several times among available choices $c_i$. The problem obtained its name as an analogy to one-hand slot machines in casino and tackles the well-known exploration vs exploitation dilemma.

In most of the times, MAB is solved by RL approaches, which analyze the already available evidences before performing each next step. In~\cite{auer2002finite} the authors proposed the Upper Confidence Bound algorithm as an intuitive solution: in iteration $k$, among available choices select one with a maximal UCB value. The UCB for each category is calculated according to \cref{impl: ucb formula}, where first component $Q$ is a quality of category under evaluation and represents the exploitation portion of UCB. The second component estimates the exploration portion balancing the number of time each category was selected. The multiplier $C$ is a balancing coefficient.

\begin{equation}
UCB = Q + C \cdot \sqrt{\frac{2 \log \sum_{1}^{i} n_k^i}{n_k}}
\label{impl: ucb formula}
\end{equation}

In this work we implement a proposed in~\cite{li2013adaptive} Fitness-Rate-Average based MAB (FRAMAB) with two reasons: (1) it is an intuitive and robust approach, (2) according to the benchmarks in~\cite{ferreira2017multi} it outperforms the other MAB algorithms. In FRAMAB, $n_k^i$ denotes the overall number of categories, while $n_k$ is a number of times the category under evaluation was selected. The quality estimation $Q$ in FRAMAB is the average improvement, obtained by the underlying category.

As for the balancing coefficient $C$, the authors in~\cite{ferreira2017multi} were evaluating a range of values between $10^{-4}...10^{-1}$. The dominance of $C$ values for various problem types were different, therefore we expose it to user for configuration. 

In addition to the statically defined $C$ value, we propose a mechanism for $C$ estimation by means of standard deviation among improvement. The motivation for this is as follows: if there exists an uncertainty in category domination the deviation will be high and therefore, encouraging the exploration portion of UCB values. We do not provide a pseudo-code for this model implementation since it straightly repeats the provided above algorithm description.


\section{Low Level Heuristics}\label{impl: LLH}
When our HLH is ready to solve an OP, the time comes to provide the tools for solving. A role of LLH in our hyper-heuristic (HH) may play everything from a single heuristic algorithm to the meta-heuristic (MH) or even other HH. As we discussed in \cref{bg: mh}, nowadays the MH research is referred as the framework growth time. Therefore, we are able not only to reuse a single heuristic but to instantiate a set of underlying heuristics among available in framework. Thus, in this Section we present a review of several toolboxes (meta-heuristic frameworks) with an intent to select the best suited one, implement a facade for framework usage and use the available algorithms as LLHs in our hyper-heuristic.

Before diving into description of available frameworks we briefly outline the LLHs characteristics with respect to which we analyze each framework:
\begin{enumerate}
	\item \textbf{Set of meta-heuristics}, which we will be able to use as LLHs in our HH.
	
	\item \textbf{Exposed hyper-parameters}, which are required for LLH tuning. We point it out explicitly, since it happens so that the parameters of an algorithm are exposed not fully.
	
	\item \textbf{Set of supported optimization problems}, which will define the applicability of our HH. The wider this set, the more use-cases developed HH is able to tackle. Among them we also separately distinguish the required in our case TSP.
	
	\item \textbf{Warm-startup}, which is required to continue the problem solving from previously reached solution. The underlying LLH should not only report finally found solution(s) but also to accept them as the starting points.
	
	\item \textbf{Termination criteria}, which is reviewed to control the intermediate results of optimization process by HH. In our system we use the wall-clock time or number of target system evaluations termination to stop the LLH and report the results.
\end{enumerate}


\subsection{Low Level Heuristics Code Base Selection}\label{implementation:llh code basis selection}
We distinguish the following frameworks as the LLH code base: Solid~\footnote{Solid GitHub repository~\url{https://github.com/100/Solid}}, mlrose~\footnote{mlrose GitHub repository~\url{https://github.com/gkhayes/mlrose}}, pyTSP~\footnote{pyTSP GitHub repository~\url{https://github.com/afourmy/pyTSP}}, LocalSolver~\footnote{LocalSolver website~\url{https://localsolver.com}}, jMetalPy~\footnote{jMetalPy GitHub repository~\url{https://github.com/jMetal/jMetalPy}} and jMetal~\footnote{jMetal GitHub repository~\url{https://github.com/jMetal/jMetal}}.

\paragraph{Solid.} A framework for a gradient-free optimization. It comprises wide range of MH skeletons with exposed hyper-parameters: genetic algorithm, evolution algorithm, simulated annealing, particle swarm optimization, tabu search, harmony search and stochastic hill climbing. The support of warm-startup is not provided and requires changes in each algorithm as a consequence of the base class absence. As for the termination criteria, algorithms in this framework support the termination after maximal number of TS evaluations and after reaching the desired quality but not a time based. Once again, to add a new termination criteria, one should modify the core of all algorithms. The framework does not provide the problem instances, nor domain-dependent parts of algorithms therefore, to use this framework one will need to carry out not only a domain-specific framework adaptation but also the problem description.

\paragraph{mlrose.} A framework with implementation of various well-known stochastic optimization algorithms such as: na\"ive and randomized hill climbing, simulated annealing, genetic and mutual-information-maximizing input clustering (MIMIC) algorithms. Each listed solver is implemented as a separate function with exposed set of hyper-parameters. It is possible to control an initial state, which is handy in our case. As for the implemented OPs, the framework comprises a large set of different types: one max, flip-flop, four and six peaks, continuous peaks, knapsack, traveling salesman, n-queens and max-k color optimization problems. The proposed termination criteria are represented only by one criterion controlling the number of TS evaluations. As in the previous framework, here the algorithms are not sharing the same code basis therefore, it may require much effort for their adaptation in general and to introduce a new termination criteria in particular.

\paragraph{pyTSP.} A system, specially designed to tackle a traveling salesman optimization problem. Together with visualization techniques, it also provides a wide bunch of different algorithms. Here they are divided into four groups: (1) construction heuristics with the nearest neighbor, the nearest insertion, the farthest insertion and the cheapest insertion algorithms, (2) a linear programming algorithm, (3) simple heuristics among which a pairwise exchange, also known as 2-opt, a node insertion and an edge insertion, and finally (4) a meta-heuristics, represented by the genetic algorithm. As one may expect, the only supported problem type here is the TSP, moreover the representation of problem does not follow a broadly used in research community manner. The other drawbacks of this framework is a partial hard-coding of hyper-parameters and an absence of exposed termination criteria. Also, the construction heuristics by their nature do not expose the possibility to feed them with the initial solutions. However, in some other algorithms the functionality to specify the initial solution is present.

\paragraph{LocalSolver.} A commercial optimization tool with free academic license. It is implemented in C++, but the API is exposed to such programming languages as Python, C++, Java and C\#. The software implements a local search programming paradigm~\cite{benoist2010toward,benoist2011localsolver} therefore, the algorithm itself and its parameters are not exposed. It is required from the user to provide a solver-specific problem description. Thanks to a detailed documentation, the desired TSP example could be found among the number of other problem instances. Two possible termination criteria are exposed: a wall-clock time and a number of solver iterations. Also, the framework supports a possibility to set an initial solution for the solver therefore, it looks like a good candidate for the LLH. However, our trial to use the tool showed up that the provided academic license could not be easily used within BRISEv2 containerized architecture. A possible work-around is to deploy a license server on a host machine and force workers to register themselves, but we found this to be an expensive implementation task from the perspective of required effort.

\paragraph{jMetalPy.} An open-source meta-heuristic framework for multi- and single-objective optimizations. Among the provided single-objective algorithms one will find genetic algorithm, evolution strategy, local search (hill climber) and simulated annealing. Even if the list of proposed heuristics is not the largest in comparison to other reviewed frameworks, every implemented algorithm exposes its hyper-parameters for tuning. We also found the code well-structured therefore, in case of required changes they could be made with less effort. A functionality to warm-up the solving process by already obtained solutions is available out-of-the-box. The various termination criteria are ready as well, among which based on the wall-clock time and the number of TS evaluations. The list of supported single-objective optimization problems consists of knapsack, traveling salesman and four synthetic problems: one max, sphere, Rastrigin and subset sum.

\paragraph{jMetal.} A meta-heuristic framework implemented in Java is an alternative to previously reviewed Python-based jMetalPy. This framework also provides meta-heuristics for multi- and single-objective OP. For later, jMetal developers implemented following algorithms:  na\"ive and covariance matrix adaptation evolution strategies (CMA-ES), genetic, particle swarm (PSO), differential evolution and coral reef optimization algorithms. It is worth to mention that among this list, not all algorithms are universally applicable to wide range of OPs. For instance, CMA-ES, PSO and differential evolution can be applied only to OPs with continuous numeric input such as synthetic mathematical problems. In contrast to implemented in Python brother, jMetal supports only one termination criterion based on number of TS evaluations, and do not support algorithm warm-startup at all. Also, the drawback here is an absence of a proper termination criteria abstraction therefore, the adaptation should affect a framework core.

\paragraph{} To sum up our discussion, we aggregate the described characteristics in \cref{iml: table llh selection}, which is similar to \cref{iml: table code basis selection}, presented during the HLH code basis selection. Once again, the characteristics qualities are scored into three ordinal values: poor, average and good with respect to provided functionality and required effort for adaptation.

\begin{table}[h!]
	\centering
	\begin{tabular}{l||cccccc}
		\textbf{Characteristic} & \textbf{Solid} & \textbf{mlrose} & \textbf{pyTSP} & \textbf{LocalSolver} & \textbf{jMetalPy} & \textbf{jMetal} \\
		\hline
		\hline
		Set of heuristics & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & N/A & \cellcolor{yellow!25}Average & \cellcolor{green!25}Good \\
		
		Exposed hyper-parameters & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good \\
		
		Provided OPs & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{yellow!25}Average & \cellcolor{yellow!25}Average \\
		
		Warm-startup support & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{yellow!25}Average \\
		
		Termination criteria & \cellcolor{yellow!25}Average & \cellcolor{red!25}Poor & \cellcolor{red!25}Poor & \cellcolor{green!25}Good & \cellcolor{green!25}Good & \cellcolor{yellow!25}Average \\
	\end{tabular}
	\caption{Meta-heuristic frameworks characteristics.}
	\label{iml: table llh selection}
\end{table}

Our ultimate goal is not to reach the best performance in provided solution, but to investigate, whether a proposed concept to solve the problem of algorithm selection and generic parameter control simultaneously using the reinforcement learning is able to outperform the baseline performance measures. Thus, while selecting LLH, the quality of provided heuristics and their hyper-parameters are playing a crucial role. There is no perfect candidate among the reviewed systems since every system requires some sort of adaptation. For our experiments we decided to use three LLH: two MHs from Python-based jMetalPy (simulated annealing and evolution strategy) and one from Java-based jMetal (evolution strategy).


\subsection{Scope of Low Level Heuristics Adaptation}\label{impl: LLH scope}
The selected frameworks propose many algorithm implementations. Since the same people are leading the development process, the overall architecture of both frameworks is somehow similar. Nevertheless, the proposed features are slightly different. For instance, jMetal does not provide time-based termination, nor warming-up the solver by initial solutions. Therefore, we split the adaptation  of frameworks on two parts, one is dedicated to jMetalPy and in the other we discuss jMetal.

\paragraph{jMetalPy.} During the analysis above, we found out that the provided features are greatly fit our requirements. Even if the lists of implemented MHs and supported OPs are not that wide, we could simply reuse the provided out-of-box implementations. To do so, we implement a framework wrapper (see \cref{impl: jMetalPy framework wrapper pseudo-code}), which according to the received task creates a desired instance of the optimization problem type and instantiate the MH solver with provided hyper-parameters (line 3). Later, this wrapper will be called to start a solver execution and report the results in a framework-independent way (line 6). To prevent an expensive problem instances loading during within one run, we cache it in memory (lines 10-11). Also, we cache an expensive I/O introspection calls, which are used to find framework components: algorithms, termination criteria or different algorithm operators such as mutation, selection, crossover, etc. (lines 13-17).

\begin{code}[language=Python, caption=jMetalPy framework wrapper pseudo-code., label=impl: jMetalPy framework wrapper pseudo-code]
class JMetalPyWrapper(ILLHWrapper):

	def construct(hyperparameters: Mapping, scenario: Mapping, warm_startup_info: Mapping) -> None:
		# Constructing meta-heuristics initialization arguments, attach initial solutions
		pass
	def run_and_report() -> Mapping:
		pass

	# Helper methods
	@lru_cache(maxsize=10, typed=True)
	def _get_problem(problem_name: str, init_params: HashableMapping) -> Problem:
		pass
	@lru_cache(maxsize=10, typed=True)
	def _get_algorithm_class(mh_name: str) -> jmetal.core.algorithm.Algorithm.__class__:
		pass
	@lru_cache(maxsize=10, typed=True)
	def _get_class_from_module(name: str, module: object) -> Type:
		pass
\end{code}

While experimenting with the framework, we found several implementation flaws in listed algorithms. The fixes for these bugs were submitted as contributions~\footnote{jMetalPy PR 1:~\url{https://github.com/jMetal/jMetalPy/pull/67}}~\footnote{jMetalPy PR 2:~\url{https://github.com/jMetal/jMetalPy/pull/80}} to implemented open-source framework.

\paragraph{jMetal.} On contrary to jMetalPy, this framework is implemented in Java therefore, we can not perform the software instantiating in s straightforward way. Note, BRISEv2 workers are based on Python. There are several libraries which allow to execute a Java code within Python: JPype~\footnote{JPype GitHub repository:~\url{https://github.com/jpype-project/jpype/}}, Py4J~\footnote{Py4J GitHub repository:~\url{https://github.com/bartdag/py4j}} or PyJNIus~\footnote{PyJNIus GitHub repository:~\url{https://github.com/kivy/pyjnius}}. The usage of one among listed modules enables us to build the same framework wrapper, as we did in previous case. Since currently we are planning to use only one meta-heuristics, the implementation of such wrapper will be unreasonable. Thus, to use a provided in jMetal ES, we pack it into an executable file with exposed parameters and call it from worker script, providing a hyper-parameters settings and warming-up solutions.

\subsection{Low Level Heuristic Runner}
When the MH wrappers are ready, we use them as different execution strategies of low level heuristic with unified \emph{ILLHWrapper} interface. To operate these wrappers we implement a separate entity — LLH runner, which forwards the construction and execution commands to the wrapper, tracks the state, make general information logging and pass the results after execution. This enables us to easily scale workers horizontally since they are homogeneous and state-less (not taking into account the caching mechanisms). The resulting process of LLH execution from the worker perspective is represented as a sequence diagram in \cref{impl:pict:llh sequence diagram}.

\svgpath{{graphics/Impl/}}
\begin{figure}
	\centering
	\includesvg[width=\textwidth]{LLH}
	\caption{The low-level heuristic execution process.}
	\label{impl:pict:llh sequence diagram}
\end{figure}

\section{Conclusion}
The performed implementation of proposed in \cref{Concept description} concept was done reusing the existing frameworks. The hyper-heuristic is mostly based on the modular BRISEv2 framework for parameter tuning. We utilize BRISEv2 prediction models in form of reinforcement learning as a HLH, while several homogeneous workers are carrying out the optimization process using LLH. For the LLH implementation we reuse the existing meta-heuristic frameworks jMetalPy and jMetal. Despite the selected code basis, the proposed approach could be implemented in the majority of parameter tuning systems following SMBO approach however, requiring the adaptation according to our review in \cref{impl:hlh code basis section}.

In this particular case, we were forced to set aside the previously used search space representation and add our own to handle the configuration sparseness issue. The intermediate entity \emph{predictor} was added to decouple the search space shape from the learning-prediction process. It allows us to extend the previously available models with the several others: fitness-rate-average based multi-armed bandits for categorical parameter selection and linear regressors from Scikit-learn framework as surrogates models. We also decoupled data preprocessing step and reused the respective tools from Scikit-learn framework.

We believe the proposed implementation will serve well not only as a hyper-heuristic, but also as old good parameter tuning framework.